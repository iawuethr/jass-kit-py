{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example for a simple neural network\n",
    "\n",
    "## Trump by maximum color (2 colors)\n",
    "\n",
    "The inputs to the network are the number of cards of each color. The network should learn to select the color with the largest number of cards of that color.\n",
    "\n",
    "For a simple example, let us assume that there are 5 cards in total for a player and only 2 colors.\n",
    "\n",
    "### Libraries\n",
    "\n",
    "We use the keras library for building, training and evaluating the network. A tutorial for keras can be found on (https://keras.io/) or https://www.tensorflow.org/guide/keras. There are different implementations of keras, here I will use the one build on tensorflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output function\n",
    "\n",
    "We have to encode the output somehow, for two classes, the simplest solution is a single variable that should be 0 if there are more cards of color 0 and 1 if there are more cards of color 1.\n",
    "\n",
    "### Training and label data.\n",
    "\n",
    "So we can prepare some training data. In this simple case, all the possible configurations are actually known.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 5.]\n",
      " [1. 4.]\n",
      " [2. 3.]\n",
      " [3. 2.]\n",
      " [4. 1.]\n",
      " [5. 0.]]\n",
      "[1. 1. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Input x: All the combinations of how two colours can be distributed on 5 cards.\n",
    "x_train = np.array([\n",
    "    [0, 5],\n",
    "    [1, 4],\n",
    "    [2, 3],\n",
    "    [3, 2],\n",
    "    [4, 1],\n",
    "    [5, 0],\n",
    "], dtype=np.float32)\n",
    "# Outputresult: 1 is for first colour to be chosen as trump, 0 is for second colour to be chosen as trump.\n",
    "y_train = np.array([1, 1, 1, 0, 0, 0,], dtype=np.float32)\n",
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalisation\n",
    "\n",
    "Input data can have different ranges. It is always a good idea (in other words absolutely essential) to normalize the input data. This is usually done into the range 0..1 or -1..1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  1. ]\n",
      " [0.2 0.8]\n",
      " [0.4 0.6]\n",
      " [0.6 0.4]\n",
      " [0.8 0.2]\n",
      " [1.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train / 5.0\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first network.\n",
    "\n",
    "We will start with a very simple network, where we connect the inputs directly to the output. So there will be 2 variables, the weights for the connection and the bias. The output function is a sigmoid, which takes values between 0 and 1.\n",
    "\n",
    "With keras, we first have to create the type of model we want (Sequential), and can then add layers. In the tensorflow implementation, we have to add the input_shape parameter in the first layer to tell it the format of the input. This does not include the batch dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A model which takes a linear stack of layers.\n",
    "model = keras.Sequential()\n",
    "# A dense layer: every input node is connected with every outputnode. The input is a 2D-vector, eg. [0,5]\n",
    "# The direct way to add the input layer is depreceated --> better to use model.Input(shape=[2])\n",
    "model.add(keras.Input(shape=[2]))\n",
    "# The output is a number (1-dimension) which gives the probability of hitting the y-output.\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "# model.add(keras.layers.Dense(1, activation='sigmoid', input_shape=[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to compile the model and tell it what loss function and optimizer we want to have. We will take a mean squared error for loss function first. (This is actually not optimal and will be corrected in an exercise).\n",
    "\n",
    "Besides the loss, we usually want to look at some metrics. Here we choose accurary, that measures how often the network makes the correct decision (see last lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Defines the criteria on how the model is optimised.\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print some details about the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 3\n",
      "Trainable params: 3\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[array([[-0.48070568],\n",
      "       [-0.733546  ]], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "print(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either train one batch, or we can use fit to train repeatedly. The result from the training is the loss function and the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28931334614753723, 0.5]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train_on_batch(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try to fit the data in minibatches multiple times. This will calculate the weights, so as to minimize the loss. We might not always get a good result in the first try and even this very simple network seems to need a large number of training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2892 - accuracy: 0.5000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2890 - accuracy: 0.5000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2888 - accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2887 - accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2885 - accuracy: 0.5000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2884 - accuracy: 0.5000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2882 - accuracy: 0.5000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2881 - accuracy: 0.5000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2879 - accuracy: 0.5000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2877 - accuracy: 0.5000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2876 - accuracy: 0.5000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2874 - accuracy: 0.5000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2873 - accuracy: 0.5000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2871 - accuracy: 0.5000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2870 - accuracy: 0.5000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2868 - accuracy: 0.5000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2866 - accuracy: 0.5000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2865 - accuracy: 0.5000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2863 - accuracy: 0.5000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2862 - accuracy: 0.5000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 974us/step - loss: 0.2860 - accuracy: 0.5000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2859 - accuracy: 0.5000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2857 - accuracy: 0.5000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2855 - accuracy: 0.5000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2854 - accuracy: 0.5000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.5000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 989us/step - loss: 0.2851 - accuracy: 0.5000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2849 - accuracy: 0.5000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2848 - accuracy: 0.5000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 995us/step - loss: 0.2846 - accuracy: 0.5000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2845 - accuracy: 0.5000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2843 - accuracy: 0.5000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2841 - accuracy: 0.5000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 961us/step - loss: 0.2840 - accuracy: 0.5000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2838 - accuracy: 0.5000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2837 - accuracy: 0.5000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2835 - accuracy: 0.5000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2834 - accuracy: 0.5000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2832 - accuracy: 0.5000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2831 - accuracy: 0.5000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2829 - accuracy: 0.5000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2827 - accuracy: 0.5000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2826 - accuracy: 0.5000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2824 - accuracy: 0.5000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2823 - accuracy: 0.5000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2821 - accuracy: 0.5000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 989us/step - loss: 0.2820 - accuracy: 0.5000\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2818 - accuracy: 0.5000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2817 - accuracy: 0.5000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2815 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2000ca8d3c8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The small input dataset will be split up into batches. Here each batch has the size of 6. \n",
    "# This is basically exactly the size of our training data. --> we have one batch of size 6.\n",
    "# Therefore, we train the model here over the same data over and over again (each epoch).\n",
    "model.fit(x_train, y_train, epochs=50, batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict the values from the training value. Why are the results floating point number and not 0 or 1? Does the result seem likely?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.34279215],\n",
       "       [0.35096502],\n",
       "       [0.35922623],\n",
       "       [0.36757177],\n",
       "       [0.3759974 ],\n",
       "       [0.38449875]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The result should be [1,1,1,0,0,0].\n",
    "# Each epoch the model is trained with the same data. If we are already in a local\n",
    "# minimum, which is not optimal, the same data will not change the weights...\n",
    "\n",
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print the found weights for each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense\n",
      "[array([[-0.50145626],\n",
      "       [-0.68183583]], dtype=float32), array([0.03095977], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(layer.name)\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we find the actual predictions? We use a threshold on the output of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train) > 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A larger network\n",
    "\n",
    "Lets try a more complicated network with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=[2]))\n",
    "# However, relu does not change really anything, since all input is >0\n",
    "# so I guess the input is the same for the sigmoid.The relu-layers give some \n",
    "# additional weights, which seems, with many epochs, to help to refine the output.\n",
    "model.add(keras.layers.Dense(2, activation='relu'))\n",
    "model.add(keras.layers.Dense(2, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train it again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 986us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 993us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 965us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 965us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 983us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 993us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 999us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 947us/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2500 - accuracy: 0.5000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.2500 - accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2000b953208>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=100, batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are not necessarly better, how does the prediction look now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5],\n",
       "       [0.5],\n",
       "       [0.5],\n",
       "       [0.5],\n",
       "       [0.5],\n",
       "       [0.5]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.layers.core.Dense object at 0x00000200033482C8>\n",
      "[array([[-1.1297853, -0.5235271],\n",
      "       [-1.1350611, -0.3731423]], dtype=float32), array([0., 0.], dtype=float32)]\n",
      "<tensorflow.python.keras.layers.core.Dense object at 0x00000200051DF508>\n",
      "[array([[ 1.2035047 ,  0.05580592],\n",
      "       [-0.48381686, -0.66784024]], dtype=float32), array([0., 0.], dtype=float32)]\n",
      "<tensorflow.python.keras.layers.core.Dense object at 0x00000200051DFB08>\n",
      "[array([[-0.95569664],\n",
      "       [ 0.8953012 ]], dtype=float32), array([0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(layer)\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The larger network, does not seem to work better as the simpler one. Or is it maybe not large enough?\n",
    "\n",
    "The problem is not the network, but the data, we just do not have enough data. So lets try to make up some more data artificially.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07041532 0.92958468]\n",
      " [0.01534069 0.98465931]\n",
      " [0.67938608 0.32061392]\n",
      " ...\n",
      " [0.85991049 0.14008951]\n",
      " [0.46611902 0.53388098]\n",
      " [0.4182461  0.5817539 ]]\n"
     ]
    }
   ],
   "source": [
    "x_0 = np.random.random(size=(10000))\n",
    "x_1 = np.ones(shape=10000) - x_0\n",
    "# basically we random create tuples with [x, 1-x]\n",
    "# some of these value-combinations can practically only be achieved, if we had much more than 5 cards\n",
    "# e.g. [0.9,0.1] for the colour distribution [9,1] in case of 10 cards.\n",
    "x_new = np.stack([x_0, x_1], axis=1)\n",
    "print(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = np.zeros(10000, dtype=np.float32)\n",
    "condition = (x_new[:,1] > x_new[:,0])\n",
    "# If color 2 has more cards than color 1 set 1.0, else it will remain 0\n",
    "y_new[condition] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07041532 0.92958468]\n",
      " [0.01534069 0.98465931]\n",
      " [0.67938608 0.32061392]\n",
      " ...\n",
      " [0.85991049 0.14008951]\n",
      " [0.46611902 0.53388098]\n",
      " [0.4182461  0.5817539 ]] [1. 1. 0. ... 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(x_new, y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 0s 970us/step - loss: 0.2500 - accuracy: 0.4955\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 0s 948us/step - loss: 0.2500 - accuracy: 0.5023\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 0s 977us/step - loss: 0.2500 - accuracy: 0.5023\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 0s 978us/step - loss: 0.2500 - accuracy: 0.5023\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 0s 987us/step - loss: 0.2500 - accuracy: 0.5023\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 0s 985us/step - loss: 0.2500 - accuracy: 0.5023\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 0s 978us/step - loss: 0.2500 - accuracy: 0.5023\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 0s 985us/step - loss: 0.2500 - accuracy: 0.5023\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 0s 997us/step - loss: 0.2500 - accuracy: 0.5023\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.5023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2000dbd5bc8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_new, y_new, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That seems better. Lets look how it performs on our original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50161004],\n",
       "       [0.50161004],\n",
       "       [0.50161004],\n",
       "       [0.50161004],\n",
       "       [0.50161004],\n",
       "       [0.50161004]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We might want to check how the network performs on any data. For this, keras provides the evaluate function that will \n",
    "evaluate the loss and the metrics. So of course label (y) data is needed for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 540us/step - loss: 0.2500 - accuracy: 0.5023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24999524652957916, 0.5023000240325928]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_new, y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we would normally do that on validation or test data not used during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 0s 752us/step - loss: 0.2500 - accuracy: 0.5022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.24999548494815826, 0.5022000074386597]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0 = np.random.random(size=(5000))\n",
    "x_1 = np.ones(shape=5000) - x_0\n",
    "x_val_new = np.stack([x_0, x_1], axis=1)\n",
    "y_val_new = np.zeros(5000, dtype=np.float32)\n",
    "y_val_new[x_val_new[:,1] > x_val_new[:,0]] = 1.0\n",
    "model.evaluate(x_val_new, y_val_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation\n",
    "\n",
    "It is essential to visualise the training process to see what is going on. In Keras, an easy method to do this is to use the history object that is returned from fit. It contains the metrics and the loss.\n",
    "\n",
    "We will also split our data into training and validation for this test. We rebuild the model, so that it is initialized again. Otherwise we would just continue with the weights from the previous fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2486 - accuracy: 0.5032 - val_loss: 0.2486 - val_accuracy: 0.4812\n",
      "Epoch 2/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2484 - accuracy: 0.5032 - val_loss: 0.2484 - val_accuracy: 0.4812\n",
      "Epoch 3/300\n",
      "75/75 [==============================] - 0s 960us/step - loss: 0.2481 - accuracy: 0.5032 - val_loss: 0.2482 - val_accuracy: 0.4812\n",
      "Epoch 4/300\n",
      "75/75 [==============================] - 0s 920us/step - loss: 0.2479 - accuracy: 0.5032 - val_loss: 0.2480 - val_accuracy: 0.4812\n",
      "Epoch 5/300\n",
      "75/75 [==============================] - 0s 934us/step - loss: 0.2477 - accuracy: 0.5035 - val_loss: 0.2477 - val_accuracy: 0.4812\n",
      "Epoch 6/300\n",
      "75/75 [==============================] - 0s 956us/step - loss: 0.2474 - accuracy: 0.5076 - val_loss: 0.2475 - val_accuracy: 0.4872\n",
      "Epoch 7/300\n",
      "75/75 [==============================] - 0s 925us/step - loss: 0.2471 - accuracy: 0.5493 - val_loss: 0.2472 - val_accuracy: 0.5064\n",
      "Epoch 8/300\n",
      "75/75 [==============================] - 0s 887us/step - loss: 0.2468 - accuracy: 0.5257 - val_loss: 0.2469 - val_accuracy: 0.5336\n",
      "Epoch 9/300\n",
      "75/75 [==============================] - 0s 961us/step - loss: 0.2465 - accuracy: 0.5731 - val_loss: 0.2466 - val_accuracy: 0.5504\n",
      "Epoch 10/300\n",
      "75/75 [==============================] - 0s 879us/step - loss: 0.2461 - accuracy: 0.5949 - val_loss: 0.2462 - val_accuracy: 0.5708\n",
      "Epoch 11/300\n",
      "75/75 [==============================] - 0s 983us/step - loss: 0.2457 - accuracy: 0.6041 - val_loss: 0.2458 - val_accuracy: 0.5912\n",
      "Epoch 12/300\n",
      "75/75 [==============================] - 0s 954us/step - loss: 0.2453 - accuracy: 0.6285 - val_loss: 0.2454 - val_accuracy: 0.6088\n",
      "Epoch 13/300\n",
      "75/75 [==============================] - 0s 974us/step - loss: 0.2449 - accuracy: 0.6453 - val_loss: 0.2450 - val_accuracy: 0.6180\n",
      "Epoch 14/300\n",
      "75/75 [==============================] - 0s 956us/step - loss: 0.2444 - accuracy: 0.6595 - val_loss: 0.2445 - val_accuracy: 0.6300\n",
      "Epoch 15/300\n",
      "75/75 [==============================] - 0s 906us/step - loss: 0.2438 - accuracy: 0.6607 - val_loss: 0.2439 - val_accuracy: 0.6412\n",
      "Epoch 16/300\n",
      "75/75 [==============================] - 0s 943us/step - loss: 0.2432 - accuracy: 0.6693 - val_loss: 0.2433 - val_accuracy: 0.6560\n",
      "Epoch 17/300\n",
      "75/75 [==============================] - 0s 946us/step - loss: 0.2426 - accuracy: 0.6763 - val_loss: 0.2427 - val_accuracy: 0.6676\n",
      "Epoch 18/300\n",
      "75/75 [==============================] - 0s 982us/step - loss: 0.2419 - accuracy: 0.6904 - val_loss: 0.2419 - val_accuracy: 0.6736\n",
      "Epoch 19/300\n",
      "75/75 [==============================] - 0s 946us/step - loss: 0.2411 - accuracy: 0.7083 - val_loss: 0.2411 - val_accuracy: 0.6904\n",
      "Epoch 20/300\n",
      "75/75 [==============================] - 0s 960us/step - loss: 0.2402 - accuracy: 0.7184 - val_loss: 0.2403 - val_accuracy: 0.7004\n",
      "Epoch 21/300\n",
      "75/75 [==============================] - 0s 953us/step - loss: 0.2393 - accuracy: 0.7277 - val_loss: 0.2393 - val_accuracy: 0.7124\n",
      "Epoch 22/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2382 - accuracy: 0.7377 - val_loss: 0.2382 - val_accuracy: 0.7192\n",
      "Epoch 23/300\n",
      "75/75 [==============================] - 0s 950us/step - loss: 0.2370 - accuracy: 0.7447 - val_loss: 0.2370 - val_accuracy: 0.7284\n",
      "Epoch 24/300\n",
      "75/75 [==============================] - 0s 973us/step - loss: 0.2357 - accuracy: 0.7528 - val_loss: 0.2357 - val_accuracy: 0.7356\n",
      "Epoch 25/300\n",
      "75/75 [==============================] - 0s 973us/step - loss: 0.2343 - accuracy: 0.7579 - val_loss: 0.2342 - val_accuracy: 0.7472\n",
      "Epoch 26/300\n",
      "75/75 [==============================] - 0s 987us/step - loss: 0.2327 - accuracy: 0.7648 - val_loss: 0.2325 - val_accuracy: 0.7548\n",
      "Epoch 27/300\n",
      "75/75 [==============================] - 0s 946us/step - loss: 0.2309 - accuracy: 0.7721 - val_loss: 0.2307 - val_accuracy: 0.7612\n",
      "Epoch 28/300\n",
      "75/75 [==============================] - 0s 991us/step - loss: 0.2289 - accuracy: 0.7825 - val_loss: 0.2287 - val_accuracy: 0.7640\n",
      "Epoch 29/300\n",
      "75/75 [==============================] - 0s 954us/step - loss: 0.2267 - accuracy: 0.7828 - val_loss: 0.2264 - val_accuracy: 0.7700\n",
      "Epoch 30/300\n",
      "75/75 [==============================] - 0s 945us/step - loss: 0.2242 - accuracy: 0.7909 - val_loss: 0.2239 - val_accuracy: 0.7768\n",
      "Epoch 31/300\n",
      "75/75 [==============================] - 0s 960us/step - loss: 0.2214 - accuracy: 0.7955 - val_loss: 0.2211 - val_accuracy: 0.7836\n",
      "Epoch 32/300\n",
      "75/75 [==============================] - 0s 938us/step - loss: 0.2184 - accuracy: 0.7992 - val_loss: 0.2180 - val_accuracy: 0.7872\n",
      "Epoch 33/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2151 - accuracy: 0.8041 - val_loss: 0.2146 - val_accuracy: 0.7908\n",
      "Epoch 34/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2114 - accuracy: 0.8075 - val_loss: 0.2109 - val_accuracy: 0.7968\n",
      "Epoch 35/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.8151 - val_loss: 0.2068 - val_accuracy: 0.8040\n",
      "Epoch 36/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 0.8185 - val_loss: 0.2025 - val_accuracy: 0.8112\n",
      "Epoch 37/300\n",
      "75/75 [==============================] - 0s 973us/step - loss: 0.1985 - accuracy: 0.8264 - val_loss: 0.1978 - val_accuracy: 0.8136\n",
      "Epoch 38/300\n",
      "75/75 [==============================] - 0s 866us/step - loss: 0.1936 - accuracy: 0.8340 - val_loss: 0.1929 - val_accuracy: 0.8212\n",
      "Epoch 39/300\n",
      "75/75 [==============================] - 0s 944us/step - loss: 0.1885 - accuracy: 0.8377 - val_loss: 0.1877 - val_accuracy: 0.8312\n",
      "Epoch 40/300\n",
      "75/75 [==============================] - 0s 933us/step - loss: 0.1831 - accuracy: 0.8465 - val_loss: 0.1823 - val_accuracy: 0.8408\n",
      "Epoch 41/300\n",
      "75/75 [==============================] - 0s 949us/step - loss: 0.1776 - accuracy: 0.8545 - val_loss: 0.1768 - val_accuracy: 0.8492\n",
      "Epoch 42/300\n",
      "75/75 [==============================] - 0s 947us/step - loss: 0.1720 - accuracy: 0.8625 - val_loss: 0.1711 - val_accuracy: 0.8576\n",
      "Epoch 43/300\n",
      "75/75 [==============================] - 0s 914us/step - loss: 0.1663 - accuracy: 0.8705 - val_loss: 0.1655 - val_accuracy: 0.8636\n",
      "Epoch 44/300\n",
      "75/75 [==============================] - 0s 918us/step - loss: 0.1606 - accuracy: 0.8777 - val_loss: 0.1598 - val_accuracy: 0.8756\n",
      "Epoch 45/300\n",
      "75/75 [==============================] - 0s 905us/step - loss: 0.1550 - accuracy: 0.8852 - val_loss: 0.1542 - val_accuracy: 0.8840\n",
      "Epoch 46/300\n",
      "75/75 [==============================] - 0s 947us/step - loss: 0.1494 - accuracy: 0.8911 - val_loss: 0.1486 - val_accuracy: 0.8944\n",
      "Epoch 47/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.8995 - val_loss: 0.1432 - val_accuracy: 0.9056\n",
      "Epoch 48/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9061 - val_loss: 0.1379 - val_accuracy: 0.9132\n",
      "Epoch 49/300\n",
      "75/75 [==============================] - 0s 960us/step - loss: 0.1336 - accuracy: 0.9137 - val_loss: 0.1328 - val_accuracy: 0.9224\n",
      "Epoch 50/300\n",
      "75/75 [==============================] - 0s 933us/step - loss: 0.1287 - accuracy: 0.9211 - val_loss: 0.1278 - val_accuracy: 0.9280\n",
      "Epoch 51/300\n",
      "75/75 [==============================] - 0s 924us/step - loss: 0.1240 - accuracy: 0.9285 - val_loss: 0.1231 - val_accuracy: 0.9328\n",
      "Epoch 52/300\n",
      "75/75 [==============================] - 0s 874us/step - loss: 0.1194 - accuracy: 0.9325 - val_loss: 0.1186 - val_accuracy: 0.9384\n",
      "Epoch 53/300\n",
      "75/75 [==============================] - 0s 952us/step - loss: 0.1151 - accuracy: 0.9396 - val_loss: 0.1143 - val_accuracy: 0.9420\n",
      "Epoch 54/300\n",
      "75/75 [==============================] - 0s 974us/step - loss: 0.1110 - accuracy: 0.9441 - val_loss: 0.1102 - val_accuracy: 0.9464\n",
      "Epoch 55/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9477 - val_loss: 0.1063 - val_accuracy: 0.9536\n",
      "Epoch 56/300\n",
      "75/75 [==============================] - 0s 887us/step - loss: 0.1035 - accuracy: 0.9516 - val_loss: 0.1026 - val_accuracy: 0.9564\n",
      "Epoch 57/300\n",
      "75/75 [==============================] - 0s 922us/step - loss: 0.1000 - accuracy: 0.9549 - val_loss: 0.0991 - val_accuracy: 0.9584\n",
      "Epoch 58/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 866us/step - loss: 0.0967 - accuracy: 0.9589 - val_loss: 0.0958 - val_accuracy: 0.9620\n",
      "Epoch 59/300\n",
      "75/75 [==============================] - 0s 905us/step - loss: 0.0935 - accuracy: 0.9632 - val_loss: 0.0927 - val_accuracy: 0.9632\n",
      "Epoch 60/300\n",
      "75/75 [==============================] - 0s 882us/step - loss: 0.0906 - accuracy: 0.9643 - val_loss: 0.0897 - val_accuracy: 0.9656\n",
      "Epoch 61/300\n",
      "75/75 [==============================] - 0s 859us/step - loss: 0.0878 - accuracy: 0.9667 - val_loss: 0.0870 - val_accuracy: 0.9676\n",
      "Epoch 62/300\n",
      "75/75 [==============================] - 0s 883us/step - loss: 0.0851 - accuracy: 0.9676 - val_loss: 0.0843 - val_accuracy: 0.9688\n",
      "Epoch 63/300\n",
      "75/75 [==============================] - 0s 910us/step - loss: 0.0826 - accuracy: 0.9697 - val_loss: 0.0818 - val_accuracy: 0.9700\n",
      "Epoch 64/300\n",
      "75/75 [==============================] - 0s 853us/step - loss: 0.0802 - accuracy: 0.9711 - val_loss: 0.0794 - val_accuracy: 0.9736\n",
      "Epoch 65/300\n",
      "75/75 [==============================] - 0s 840us/step - loss: 0.0780 - accuracy: 0.9724 - val_loss: 0.0772 - val_accuracy: 0.9740\n",
      "Epoch 66/300\n",
      "75/75 [==============================] - 0s 914us/step - loss: 0.0758 - accuracy: 0.9743 - val_loss: 0.0750 - val_accuracy: 0.9744\n",
      "Epoch 67/300\n",
      "75/75 [==============================] - 0s 868us/step - loss: 0.0738 - accuracy: 0.9745 - val_loss: 0.0730 - val_accuracy: 0.9764\n",
      "Epoch 68/300\n",
      "75/75 [==============================] - 0s 849us/step - loss: 0.0719 - accuracy: 0.9771 - val_loss: 0.0710 - val_accuracy: 0.9776\n",
      "Epoch 69/300\n",
      "75/75 [==============================] - 0s 920us/step - loss: 0.0700 - accuracy: 0.9775 - val_loss: 0.0692 - val_accuracy: 0.9780\n",
      "Epoch 70/300\n",
      "75/75 [==============================] - 0s 830us/step - loss: 0.0683 - accuracy: 0.9809 - val_loss: 0.0674 - val_accuracy: 0.9784\n",
      "Epoch 71/300\n",
      "75/75 [==============================] - 0s 890us/step - loss: 0.0666 - accuracy: 0.9821 - val_loss: 0.0658 - val_accuracy: 0.9788\n",
      "Epoch 72/300\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.0650 - accuracy: 0.9816 - val_loss: 0.0642 - val_accuracy: 0.9812\n",
      "Epoch 73/300\n",
      "75/75 [==============================] - 0s 867us/step - loss: 0.0635 - accuracy: 0.9828 - val_loss: 0.0626 - val_accuracy: 0.9816\n",
      "Epoch 74/300\n",
      "75/75 [==============================] - 0s 933us/step - loss: 0.0621 - accuracy: 0.9840 - val_loss: 0.0612 - val_accuracy: 0.9824\n",
      "Epoch 75/300\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.0607 - accuracy: 0.9845 - val_loss: 0.0598 - val_accuracy: 0.9836\n",
      "Epoch 76/300\n",
      "75/75 [==============================] - 0s 907us/step - loss: 0.0594 - accuracy: 0.9855 - val_loss: 0.0585 - val_accuracy: 0.9848\n",
      "Epoch 77/300\n",
      "75/75 [==============================] - 0s 913us/step - loss: 0.0581 - accuracy: 0.9863 - val_loss: 0.0572 - val_accuracy: 0.9848\n",
      "Epoch 78/300\n",
      "75/75 [==============================] - 0s 864us/step - loss: 0.0569 - accuracy: 0.9875 - val_loss: 0.0560 - val_accuracy: 0.9852\n",
      "Epoch 79/300\n",
      "75/75 [==============================] - 0s 837us/step - loss: 0.0557 - accuracy: 0.9868 - val_loss: 0.0548 - val_accuracy: 0.9872\n",
      "Epoch 80/300\n",
      "75/75 [==============================] - 0s 867us/step - loss: 0.0546 - accuracy: 0.9880 - val_loss: 0.0537 - val_accuracy: 0.9880\n",
      "Epoch 81/300\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.0535 - accuracy: 0.9888 - val_loss: 0.0526 - val_accuracy: 0.9876\n",
      "Epoch 82/300\n",
      "75/75 [==============================] - 0s 839us/step - loss: 0.0525 - accuracy: 0.9884 - val_loss: 0.0516 - val_accuracy: 0.9884\n",
      "Epoch 83/300\n",
      "75/75 [==============================] - 0s 906us/step - loss: 0.0515 - accuracy: 0.9885 - val_loss: 0.0506 - val_accuracy: 0.9900\n",
      "Epoch 84/300\n",
      "75/75 [==============================] - 0s 871us/step - loss: 0.0506 - accuracy: 0.9891 - val_loss: 0.0497 - val_accuracy: 0.9908\n",
      "Epoch 85/300\n",
      "75/75 [==============================] - 0s 879us/step - loss: 0.0497 - accuracy: 0.9893 - val_loss: 0.0487 - val_accuracy: 0.9920\n",
      "Epoch 86/300\n",
      "75/75 [==============================] - 0s 926us/step - loss: 0.0488 - accuracy: 0.9896 - val_loss: 0.0478 - val_accuracy: 0.9920\n",
      "Epoch 87/300\n",
      "75/75 [==============================] - 0s 872us/step - loss: 0.0480 - accuracy: 0.9912 - val_loss: 0.0470 - val_accuracy: 0.9920\n",
      "Epoch 88/300\n",
      "75/75 [==============================] - 0s 908us/step - loss: 0.0471 - accuracy: 0.9903 - val_loss: 0.0462 - val_accuracy: 0.9920\n",
      "Epoch 89/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0463 - accuracy: 0.9895 - val_loss: 0.0454 - val_accuracy: 0.9936\n",
      "Epoch 90/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0456 - accuracy: 0.9912 - val_loss: 0.0446 - val_accuracy: 0.9936\n",
      "Epoch 91/300\n",
      "75/75 [==============================] - 0s 837us/step - loss: 0.0449 - accuracy: 0.9915 - val_loss: 0.0439 - val_accuracy: 0.9936\n",
      "Epoch 92/300\n",
      "75/75 [==============================] - 0s 896us/step - loss: 0.0442 - accuracy: 0.9925 - val_loss: 0.0432 - val_accuracy: 0.9932\n",
      "Epoch 93/300\n",
      "75/75 [==============================] - 0s 888us/step - loss: 0.0435 - accuracy: 0.9911 - val_loss: 0.0425 - val_accuracy: 0.9940\n",
      "Epoch 94/300\n",
      "75/75 [==============================] - 0s 853us/step - loss: 0.0428 - accuracy: 0.9916 - val_loss: 0.0418 - val_accuracy: 0.9940\n",
      "Epoch 95/300\n",
      "75/75 [==============================] - 0s 896us/step - loss: 0.0422 - accuracy: 0.9925 - val_loss: 0.0412 - val_accuracy: 0.9940\n",
      "Epoch 96/300\n",
      "75/75 [==============================] - 0s 867us/step - loss: 0.0416 - accuracy: 0.9924 - val_loss: 0.0406 - val_accuracy: 0.9940\n",
      "Epoch 97/300\n",
      "75/75 [==============================] - 0s 947us/step - loss: 0.0410 - accuracy: 0.9925 - val_loss: 0.0400 - val_accuracy: 0.9940\n",
      "Epoch 98/300\n",
      "75/75 [==============================] - 0s 867us/step - loss: 0.0404 - accuracy: 0.9927 - val_loss: 0.0394 - val_accuracy: 0.9940\n",
      "Epoch 99/300\n",
      "75/75 [==============================] - 0s 893us/step - loss: 0.0398 - accuracy: 0.9927 - val_loss: 0.0388 - val_accuracy: 0.9948\n",
      "Epoch 100/300\n",
      "75/75 [==============================] - 0s 1000us/step - loss: 0.0393 - accuracy: 0.9928 - val_loss: 0.0383 - val_accuracy: 0.9956\n",
      "Epoch 101/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0387 - accuracy: 0.9935 - val_loss: 0.0378 - val_accuracy: 0.9952\n",
      "Epoch 102/300\n",
      "75/75 [==============================] - 0s 840us/step - loss: 0.0382 - accuracy: 0.9936 - val_loss: 0.0372 - val_accuracy: 0.9948\n",
      "Epoch 103/300\n",
      "75/75 [==============================] - 0s 854us/step - loss: 0.0377 - accuracy: 0.9936 - val_loss: 0.0367 - val_accuracy: 0.9956\n",
      "Epoch 104/300\n",
      "75/75 [==============================] - 0s 853us/step - loss: 0.0372 - accuracy: 0.9945 - val_loss: 0.0363 - val_accuracy: 0.9956\n",
      "Epoch 105/300\n",
      "75/75 [==============================] - 0s 871us/step - loss: 0.0368 - accuracy: 0.9939 - val_loss: 0.0358 - val_accuracy: 0.9956\n",
      "Epoch 106/300\n",
      "75/75 [==============================] - 0s 870us/step - loss: 0.0363 - accuracy: 0.9936 - val_loss: 0.0353 - val_accuracy: 0.9956\n",
      "Epoch 107/300\n",
      "75/75 [==============================] - 0s 859us/step - loss: 0.0358 - accuracy: 0.9952 - val_loss: 0.0349 - val_accuracy: 0.9956\n",
      "Epoch 108/300\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.0354 - accuracy: 0.9936 - val_loss: 0.0344 - val_accuracy: 0.9968\n",
      "Epoch 109/300\n",
      "75/75 [==============================] - 0s 852us/step - loss: 0.0350 - accuracy: 0.9949 - val_loss: 0.0340 - val_accuracy: 0.9972\n",
      "Epoch 110/300\n",
      "75/75 [==============================] - 0s 865us/step - loss: 0.0346 - accuracy: 0.9960 - val_loss: 0.0336 - val_accuracy: 0.9960\n",
      "Epoch 111/300\n",
      "75/75 [==============================] - 0s 848us/step - loss: 0.0342 - accuracy: 0.9963 - val_loss: 0.0332 - val_accuracy: 0.9956\n",
      "Epoch 112/300\n",
      "75/75 [==============================] - 0s 893us/step - loss: 0.0338 - accuracy: 0.9960 - val_loss: 0.0328 - val_accuracy: 0.9956\n",
      "Epoch 113/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0334 - accuracy: 0.9951 - val_loss: 0.0324 - val_accuracy: 0.9960\n",
      "Epoch 114/300\n",
      "75/75 [==============================] - 0s 839us/step - loss: 0.0330 - accuracy: 0.9953 - val_loss: 0.0320 - val_accuracy: 0.9968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/300\n",
      "75/75 [==============================] - 0s 830us/step - loss: 0.0327 - accuracy: 0.9961 - val_loss: 0.0317 - val_accuracy: 0.9972\n",
      "Epoch 116/300\n",
      "75/75 [==============================] - 0s 857us/step - loss: 0.0323 - accuracy: 0.9955 - val_loss: 0.0313 - val_accuracy: 0.9972\n",
      "Epoch 117/300\n",
      "75/75 [==============================] - 0s 856us/step - loss: 0.0320 - accuracy: 0.9969 - val_loss: 0.0310 - val_accuracy: 0.9972\n",
      "Epoch 118/300\n",
      "75/75 [==============================] - 0s 861us/step - loss: 0.0316 - accuracy: 0.9963 - val_loss: 0.0306 - val_accuracy: 0.9972\n",
      "Epoch 119/300\n",
      "75/75 [==============================] - 0s 882us/step - loss: 0.0313 - accuracy: 0.9971 - val_loss: 0.0303 - val_accuracy: 0.9972\n",
      "Epoch 120/300\n",
      "75/75 [==============================] - 0s 883us/step - loss: 0.0310 - accuracy: 0.9975 - val_loss: 0.0300 - val_accuracy: 0.9972\n",
      "Epoch 121/300\n",
      "75/75 [==============================] - 0s 883us/step - loss: 0.0307 - accuracy: 0.9972 - val_loss: 0.0297 - val_accuracy: 0.9972\n",
      "Epoch 122/300\n",
      "75/75 [==============================] - 0s 878us/step - loss: 0.0304 - accuracy: 0.9967 - val_loss: 0.0294 - val_accuracy: 0.9972\n",
      "Epoch 123/300\n",
      "75/75 [==============================] - 0s 866us/step - loss: 0.0300 - accuracy: 0.9975 - val_loss: 0.0291 - val_accuracy: 0.9972\n",
      "Epoch 124/300\n",
      "75/75 [==============================] - 0s 960us/step - loss: 0.0298 - accuracy: 0.9969 - val_loss: 0.0288 - val_accuracy: 0.9976\n",
      "Epoch 125/300\n",
      "75/75 [==============================] - 0s 960us/step - loss: 0.0295 - accuracy: 0.9977 - val_loss: 0.0285 - val_accuracy: 0.9972\n",
      "Epoch 126/300\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.0292 - accuracy: 0.9969 - val_loss: 0.0282 - val_accuracy: 0.9976\n",
      "Epoch 127/300\n",
      "75/75 [==============================] - 0s 827us/step - loss: 0.0289 - accuracy: 0.9973 - val_loss: 0.0279 - val_accuracy: 0.9976\n",
      "Epoch 128/300\n",
      "75/75 [==============================] - 0s 878us/step - loss: 0.0286 - accuracy: 0.9977 - val_loss: 0.0277 - val_accuracy: 0.9972\n",
      "Epoch 129/300\n",
      "75/75 [==============================] - 0s 845us/step - loss: 0.0284 - accuracy: 0.9965 - val_loss: 0.0274 - val_accuracy: 0.9976\n",
      "Epoch 130/300\n",
      "75/75 [==============================] - 0s 865us/step - loss: 0.0281 - accuracy: 0.9975 - val_loss: 0.0271 - val_accuracy: 0.9976\n",
      "Epoch 131/300\n",
      "75/75 [==============================] - 0s 935us/step - loss: 0.0279 - accuracy: 0.9977 - val_loss: 0.0269 - val_accuracy: 0.9976\n",
      "Epoch 132/300\n",
      "75/75 [==============================] - 0s 933us/step - loss: 0.0276 - accuracy: 0.9980 - val_loss: 0.0267 - val_accuracy: 0.9976\n",
      "Epoch 133/300\n",
      "75/75 [==============================] - 0s 867us/step - loss: 0.0274 - accuracy: 0.9975 - val_loss: 0.0264 - val_accuracy: 0.9976\n",
      "Epoch 134/300\n",
      "75/75 [==============================] - 0s 893us/step - loss: 0.0271 - accuracy: 0.9977 - val_loss: 0.0262 - val_accuracy: 0.9976\n",
      "Epoch 135/300\n",
      "75/75 [==============================] - 0s 849us/step - loss: 0.0269 - accuracy: 0.9979 - val_loss: 0.0259 - val_accuracy: 0.9976\n",
      "Epoch 136/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0267 - accuracy: 0.9972 - val_loss: 0.0257 - val_accuracy: 0.9976\n",
      "Epoch 137/300\n",
      "75/75 [==============================] - 0s 930us/step - loss: 0.0264 - accuracy: 0.9977 - val_loss: 0.0255 - val_accuracy: 0.9976\n",
      "Epoch 138/300\n",
      "75/75 [==============================] - 0s 911us/step - loss: 0.0262 - accuracy: 0.9979 - val_loss: 0.0253 - val_accuracy: 0.9976\n",
      "Epoch 139/300\n",
      "75/75 [==============================] - 0s 829us/step - loss: 0.0260 - accuracy: 0.9977 - val_loss: 0.0251 - val_accuracy: 0.9976\n",
      "Epoch 140/300\n",
      "75/75 [==============================] - 0s 868us/step - loss: 0.0258 - accuracy: 0.9981 - val_loss: 0.0248 - val_accuracy: 0.9980\n",
      "Epoch 141/300\n",
      "75/75 [==============================] - 0s 826us/step - loss: 0.0256 - accuracy: 0.9980 - val_loss: 0.0246 - val_accuracy: 0.9980\n",
      "Epoch 142/300\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.0254 - accuracy: 0.9985 - val_loss: 0.0244 - val_accuracy: 0.9980\n",
      "Epoch 143/300\n",
      "75/75 [==============================] - 0s 867us/step - loss: 0.0252 - accuracy: 0.9980 - val_loss: 0.0242 - val_accuracy: 0.9976\n",
      "Epoch 144/300\n",
      "75/75 [==============================] - 0s 876us/step - loss: 0.0250 - accuracy: 0.9984 - val_loss: 0.0241 - val_accuracy: 0.9976\n",
      "Epoch 145/300\n",
      "75/75 [==============================] - 0s 987us/step - loss: 0.0248 - accuracy: 0.9981 - val_loss: 0.0239 - val_accuracy: 0.9976\n",
      "Epoch 146/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0246 - accuracy: 0.9980 - val_loss: 0.0237 - val_accuracy: 0.9976\n",
      "Epoch 147/300\n",
      "75/75 [==============================] - 0s 933us/step - loss: 0.0244 - accuracy: 0.9991 - val_loss: 0.0235 - val_accuracy: 0.9976\n",
      "Epoch 148/300\n",
      "75/75 [==============================] - 0s 834us/step - loss: 0.0242 - accuracy: 0.9981 - val_loss: 0.0233 - val_accuracy: 0.9976\n",
      "Epoch 149/300\n",
      "75/75 [==============================] - 0s 861us/step - loss: 0.0241 - accuracy: 0.9979 - val_loss: 0.0231 - val_accuracy: 0.9976\n",
      "Epoch 150/300\n",
      "75/75 [==============================] - 0s 869us/step - loss: 0.0239 - accuracy: 0.9979 - val_loss: 0.0230 - val_accuracy: 0.9976\n",
      "Epoch 151/300\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.0237 - accuracy: 0.9983 - val_loss: 0.0228 - val_accuracy: 0.9976\n",
      "Epoch 152/300\n",
      "75/75 [==============================] - 0s 827us/step - loss: 0.0235 - accuracy: 0.9981 - val_loss: 0.0226 - val_accuracy: 0.9980\n",
      "Epoch 153/300\n",
      "75/75 [==============================] - 0s 876us/step - loss: 0.0234 - accuracy: 0.9983 - val_loss: 0.0224 - val_accuracy: 0.9980\n",
      "Epoch 154/300\n",
      "75/75 [==============================] - 0s 859us/step - loss: 0.0232 - accuracy: 0.9985 - val_loss: 0.0223 - val_accuracy: 0.9980\n",
      "Epoch 155/300\n",
      "75/75 [==============================] - 0s 879us/step - loss: 0.0230 - accuracy: 0.9988 - val_loss: 0.0221 - val_accuracy: 0.9976\n",
      "Epoch 156/300\n",
      "75/75 [==============================] - 0s 934us/step - loss: 0.0229 - accuracy: 0.9991 - val_loss: 0.0220 - val_accuracy: 0.9976\n",
      "Epoch 157/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0227 - accuracy: 0.9980 - val_loss: 0.0218 - val_accuracy: 0.9980\n",
      "Epoch 158/300\n",
      "75/75 [==============================] - 0s 945us/step - loss: 0.0226 - accuracy: 0.9983 - val_loss: 0.0217 - val_accuracy: 0.9980\n",
      "Epoch 159/300\n",
      "75/75 [==============================] - 0s 920us/step - loss: 0.0224 - accuracy: 0.9980 - val_loss: 0.0215 - val_accuracy: 0.9988\n",
      "Epoch 160/300\n",
      "75/75 [==============================] - 0s 893us/step - loss: 0.0223 - accuracy: 0.9996 - val_loss: 0.0214 - val_accuracy: 0.9980\n",
      "Epoch 161/300\n",
      "75/75 [==============================] - 0s 840us/step - loss: 0.0221 - accuracy: 0.9989 - val_loss: 0.0212 - val_accuracy: 0.9976\n",
      "Epoch 162/300\n",
      "75/75 [==============================] - 0s 852us/step - loss: 0.0220 - accuracy: 0.9983 - val_loss: 0.0211 - val_accuracy: 0.9980\n",
      "Epoch 163/300\n",
      "75/75 [==============================] - 0s 826us/step - loss: 0.0218 - accuracy: 0.9985 - val_loss: 0.0209 - val_accuracy: 0.9980\n",
      "Epoch 164/300\n",
      "75/75 [==============================] - 0s 840us/step - loss: 0.0217 - accuracy: 0.9988 - val_loss: 0.0208 - val_accuracy: 0.9980\n",
      "Epoch 165/300\n",
      "75/75 [==============================] - 0s 842us/step - loss: 0.0215 - accuracy: 0.9987 - val_loss: 0.0207 - val_accuracy: 0.9984\n",
      "Epoch 166/300\n",
      "75/75 [==============================] - 0s 947us/step - loss: 0.0214 - accuracy: 0.9985 - val_loss: 0.0205 - val_accuracy: 0.9984\n",
      "Epoch 167/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0213 - accuracy: 0.9995 - val_loss: 0.0204 - val_accuracy: 0.9980\n",
      "Epoch 168/300\n",
      "75/75 [==============================] - 0s 907us/step - loss: 0.0211 - accuracy: 0.9985 - val_loss: 0.0203 - val_accuracy: 0.9980\n",
      "Epoch 169/300\n",
      "75/75 [==============================] - 0s 844us/step - loss: 0.0210 - accuracy: 0.9983 - val_loss: 0.0201 - val_accuracy: 0.9988\n",
      "Epoch 170/300\n",
      "75/75 [==============================] - 0s 860us/step - loss: 0.0209 - accuracy: 0.9989 - val_loss: 0.0200 - val_accuracy: 0.9988\n",
      "Epoch 171/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 854us/step - loss: 0.0207 - accuracy: 0.9992 - val_loss: 0.0199 - val_accuracy: 0.9988\n",
      "Epoch 172/300\n",
      "75/75 [==============================] - 0s 853us/step - loss: 0.0206 - accuracy: 0.9993 - val_loss: 0.0198 - val_accuracy: 0.9976\n",
      "Epoch 173/300\n",
      "75/75 [==============================] - 0s 875us/step - loss: 0.0205 - accuracy: 0.9985 - val_loss: 0.0196 - val_accuracy: 0.9980\n",
      "Epoch 174/300\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.0204 - accuracy: 0.9989 - val_loss: 0.0195 - val_accuracy: 0.9980\n",
      "Epoch 175/300\n",
      "75/75 [==============================] - 0s 909us/step - loss: 0.0203 - accuracy: 0.9992 - val_loss: 0.0194 - val_accuracy: 0.9980\n",
      "Epoch 176/300\n",
      "75/75 [==============================] - 0s 893us/step - loss: 0.0201 - accuracy: 0.9985 - val_loss: 0.0193 - val_accuracy: 0.9984\n",
      "Epoch 177/300\n",
      "75/75 [==============================] - 0s 987us/step - loss: 0.0200 - accuracy: 0.9989 - val_loss: 0.0192 - val_accuracy: 0.9988\n",
      "Epoch 178/300\n",
      "75/75 [==============================] - 0s 920us/step - loss: 0.0199 - accuracy: 0.9988 - val_loss: 0.0190 - val_accuracy: 0.9988\n",
      "Epoch 179/300\n",
      "75/75 [==============================] - 0s 853us/step - loss: 0.0198 - accuracy: 0.9993 - val_loss: 0.0189 - val_accuracy: 0.9980\n",
      "Epoch 180/300\n",
      "75/75 [==============================] - 0s 838us/step - loss: 0.0197 - accuracy: 0.9991 - val_loss: 0.0188 - val_accuracy: 0.9980\n",
      "Epoch 181/300\n",
      "75/75 [==============================] - 0s 853us/step - loss: 0.0196 - accuracy: 0.9989 - val_loss: 0.0187 - val_accuracy: 0.9984\n",
      "Epoch 182/300\n",
      "75/75 [==============================] - 0s 862us/step - loss: 0.0195 - accuracy: 0.9988 - val_loss: 0.0186 - val_accuracy: 0.9988\n",
      "Epoch 183/300\n",
      "75/75 [==============================] - 0s 849us/step - loss: 0.0193 - accuracy: 0.9991 - val_loss: 0.0185 - val_accuracy: 0.9988\n",
      "Epoch 184/300\n",
      "75/75 [==============================] - 0s 839us/step - loss: 0.0192 - accuracy: 0.9997 - val_loss: 0.0184 - val_accuracy: 0.9980\n",
      "Epoch 185/300\n",
      "75/75 [==============================] - 0s 843us/step - loss: 0.0191 - accuracy: 0.9984 - val_loss: 0.0183 - val_accuracy: 0.9988\n",
      "Epoch 186/300\n",
      "75/75 [==============================] - 0s 848us/step - loss: 0.0190 - accuracy: 0.9996 - val_loss: 0.0182 - val_accuracy: 0.9988\n",
      "Epoch 187/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0189 - accuracy: 0.9996 - val_loss: 0.0181 - val_accuracy: 0.9988\n",
      "Epoch 188/300\n",
      "75/75 [==============================] - 0s 889us/step - loss: 0.0188 - accuracy: 0.9995 - val_loss: 0.0180 - val_accuracy: 0.9980\n",
      "Epoch 189/300\n",
      "75/75 [==============================] - 0s 876us/step - loss: 0.0187 - accuracy: 0.9992 - val_loss: 0.0179 - val_accuracy: 0.9980\n",
      "Epoch 190/300\n",
      "75/75 [==============================] - 0s 859us/step - loss: 0.0186 - accuracy: 0.9988 - val_loss: 0.0178 - val_accuracy: 0.9988\n",
      "Epoch 191/300\n",
      "75/75 [==============================] - 0s 862us/step - loss: 0.0185 - accuracy: 0.9996 - val_loss: 0.0177 - val_accuracy: 0.9980\n",
      "Epoch 192/300\n",
      "75/75 [==============================] - 0s 868us/step - loss: 0.0184 - accuracy: 0.9989 - val_loss: 0.0176 - val_accuracy: 0.9988\n",
      "Epoch 193/300\n",
      "75/75 [==============================] - 0s 845us/step - loss: 0.0183 - accuracy: 0.9996 - val_loss: 0.0175 - val_accuracy: 0.9980\n",
      "Epoch 194/300\n",
      "75/75 [==============================] - 0s 867us/step - loss: 0.0182 - accuracy: 0.9993 - val_loss: 0.0174 - val_accuracy: 0.9980\n",
      "Epoch 195/300\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.0181 - accuracy: 0.9995 - val_loss: 0.0174 - val_accuracy: 0.9980\n",
      "Epoch 196/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0181 - accuracy: 0.9988 - val_loss: 0.0173 - val_accuracy: 0.9984\n",
      "Epoch 197/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9989 - val_loss: 0.0172 - val_accuracy: 0.9988\n",
      "Epoch 198/300\n",
      "75/75 [==============================] - 0s 920us/step - loss: 0.0179 - accuracy: 0.9992 - val_loss: 0.0171 - val_accuracy: 0.9988\n",
      "Epoch 199/300\n",
      "75/75 [==============================] - 0s 867us/step - loss: 0.0178 - accuracy: 0.9996 - val_loss: 0.0170 - val_accuracy: 0.9988\n",
      "Epoch 200/300\n",
      "75/75 [==============================] - 0s 859us/step - loss: 0.0177 - accuracy: 0.9989 - val_loss: 0.0169 - val_accuracy: 0.9988\n",
      "Epoch 201/300\n",
      "75/75 [==============================] - 0s 854us/step - loss: 0.0176 - accuracy: 0.9991 - val_loss: 0.0168 - val_accuracy: 0.9996\n",
      "Epoch 202/300\n",
      "75/75 [==============================] - 0s 863us/step - loss: 0.0175 - accuracy: 0.9997 - val_loss: 0.0167 - val_accuracy: 0.9988\n",
      "Epoch 203/300\n",
      "75/75 [==============================] - 0s 875us/step - loss: 0.0174 - accuracy: 0.9989 - val_loss: 0.0167 - val_accuracy: 0.9988\n",
      "Epoch 204/300\n",
      "75/75 [==============================] - 0s 933us/step - loss: 0.0174 - accuracy: 0.9996 - val_loss: 0.0166 - val_accuracy: 0.9984\n",
      "Epoch 205/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0173 - accuracy: 0.9995 - val_loss: 0.0165 - val_accuracy: 0.9988\n",
      "Epoch 206/300\n",
      "75/75 [==============================] - 0s 907us/step - loss: 0.0172 - accuracy: 0.9984 - val_loss: 0.0164 - val_accuracy: 0.9996\n",
      "Epoch 207/300\n",
      "75/75 [==============================] - 0s 867us/step - loss: 0.0171 - accuracy: 0.9995 - val_loss: 0.0163 - val_accuracy: 0.9992\n",
      "Epoch 208/300\n",
      "75/75 [==============================] - 0s 846us/step - loss: 0.0170 - accuracy: 0.9993 - val_loss: 0.0163 - val_accuracy: 0.9992\n",
      "Epoch 209/300\n",
      "75/75 [==============================] - 0s 888us/step - loss: 0.0170 - accuracy: 0.9997 - val_loss: 0.0162 - val_accuracy: 0.9988\n",
      "Epoch 210/300\n",
      "75/75 [==============================] - 0s 871us/step - loss: 0.0169 - accuracy: 0.9992 - val_loss: 0.0161 - val_accuracy: 0.9988\n",
      "Epoch 211/300\n",
      "75/75 [==============================] - 0s 872us/step - loss: 0.0168 - accuracy: 0.9993 - val_loss: 0.0160 - val_accuracy: 0.9988\n",
      "Epoch 212/300\n",
      "75/75 [==============================] - 0s 920us/step - loss: 0.0167 - accuracy: 0.9992 - val_loss: 0.0160 - val_accuracy: 0.9988\n",
      "Epoch 213/300\n",
      "75/75 [==============================] - 0s 907us/step - loss: 0.0167 - accuracy: 0.9992 - val_loss: 0.0159 - val_accuracy: 0.9988\n",
      "Epoch 214/300\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.0166 - accuracy: 0.9995 - val_loss: 0.0158 - val_accuracy: 0.9988\n",
      "Epoch 215/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0165 - accuracy: 0.9993 - val_loss: 0.0157 - val_accuracy: 0.9988\n",
      "Epoch 216/300\n",
      "75/75 [==============================] - 0s 920us/step - loss: 0.0164 - accuracy: 0.9992 - val_loss: 0.0157 - val_accuracy: 0.9988\n",
      "Epoch 217/300\n",
      "75/75 [==============================] - 0s 902us/step - loss: 0.0164 - accuracy: 0.9992 - val_loss: 0.0156 - val_accuracy: 0.9988\n",
      "Epoch 218/300\n",
      "75/75 [==============================] - 0s 865us/step - loss: 0.0163 - accuracy: 0.9999 - val_loss: 0.0155 - val_accuracy: 0.9984\n",
      "Epoch 219/300\n",
      "75/75 [==============================] - 0s 833us/step - loss: 0.0162 - accuracy: 0.9995 - val_loss: 0.0155 - val_accuracy: 0.9988\n",
      "Epoch 220/300\n",
      "75/75 [==============================] - 0s 842us/step - loss: 0.0161 - accuracy: 0.9995 - val_loss: 0.0154 - val_accuracy: 0.9988\n",
      "Epoch 221/300\n",
      "75/75 [==============================] - 0s 841us/step - loss: 0.0161 - accuracy: 0.9996 - val_loss: 0.0153 - val_accuracy: 0.9984\n",
      "Epoch 222/300\n",
      "75/75 [==============================] - 0s 867us/step - loss: 0.0160 - accuracy: 0.9995 - val_loss: 0.0153 - val_accuracy: 0.9980\n",
      "Epoch 223/300\n",
      "75/75 [==============================] - 0s 879us/step - loss: 0.0159 - accuracy: 0.9996 - val_loss: 0.0152 - val_accuracy: 0.9980\n",
      "Epoch 224/300\n",
      "75/75 [==============================] - 0s 901us/step - loss: 0.0159 - accuracy: 0.9995 - val_loss: 0.0152 - val_accuracy: 0.9980\n",
      "Epoch 225/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0158 - accuracy: 0.9989 - val_loss: 0.0151 - val_accuracy: 0.9988\n",
      "Epoch 226/300\n",
      "75/75 [==============================] - 0s 813us/step - loss: 0.0157 - accuracy: 0.9995 - val_loss: 0.0150 - val_accuracy: 0.9992\n",
      "Epoch 227/300\n",
      "75/75 [==============================] - 0s 977us/step - loss: 0.0157 - accuracy: 0.9997 - val_loss: 0.0149 - val_accuracy: 0.9988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 228/300\n",
      "75/75 [==============================] - 0s 872us/step - loss: 0.0156 - accuracy: 0.9995 - val_loss: 0.0149 - val_accuracy: 0.9984\n",
      "Epoch 229/300\n",
      "75/75 [==============================] - 0s 873us/step - loss: 0.0155 - accuracy: 0.9985 - val_loss: 0.0148 - val_accuracy: 0.9996\n",
      "Epoch 230/300\n",
      "75/75 [==============================] - 0s 896us/step - loss: 0.0155 - accuracy: 0.9996 - val_loss: 0.0147 - val_accuracy: 0.9996\n",
      "Epoch 231/300\n",
      "75/75 [==============================] - 0s 888us/step - loss: 0.0154 - accuracy: 0.9997 - val_loss: 0.0147 - val_accuracy: 0.9992\n",
      "Epoch 232/300\n",
      "75/75 [==============================] - 0s 864us/step - loss: 0.0154 - accuracy: 0.9997 - val_loss: 0.0146 - val_accuracy: 0.9988\n",
      "Epoch 233/300\n",
      "75/75 [==============================] - 0s 876us/step - loss: 0.0153 - accuracy: 0.9996 - val_loss: 0.0146 - val_accuracy: 0.9988\n",
      "Epoch 234/300\n",
      "75/75 [==============================] - 0s 873us/step - loss: 0.0152 - accuracy: 0.9995 - val_loss: 0.0145 - val_accuracy: 0.9988\n",
      "Epoch 235/300\n",
      "75/75 [==============================] - 0s 846us/step - loss: 0.0152 - accuracy: 0.9997 - val_loss: 0.0145 - val_accuracy: 0.9988\n",
      "Epoch 236/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0151 - accuracy: 0.9996 - val_loss: 0.0144 - val_accuracy: 0.9984\n",
      "Epoch 237/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0151 - accuracy: 0.9995 - val_loss: 0.0143 - val_accuracy: 0.9988\n",
      "Epoch 238/300\n",
      "75/75 [==============================] - 0s 869us/step - loss: 0.0150 - accuracy: 0.9993 - val_loss: 0.0143 - val_accuracy: 0.9988\n",
      "Epoch 239/300\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.0149 - accuracy: 0.9993 - val_loss: 0.0142 - val_accuracy: 0.9996\n",
      "Epoch 240/300\n",
      "75/75 [==============================] - 0s 863us/step - loss: 0.0149 - accuracy: 0.9997 - val_loss: 0.0142 - val_accuracy: 0.9996\n",
      "Epoch 241/300\n",
      "75/75 [==============================] - 0s 893us/step - loss: 0.0148 - accuracy: 0.9997 - val_loss: 0.0141 - val_accuracy: 0.9988\n",
      "Epoch 242/300\n",
      "75/75 [==============================] - 0s 892us/step - loss: 0.0148 - accuracy: 0.9997 - val_loss: 0.0141 - val_accuracy: 0.9984\n",
      "Epoch 243/300\n",
      "75/75 [==============================] - 0s 910us/step - loss: 0.0147 - accuracy: 0.9997 - val_loss: 0.0140 - val_accuracy: 0.9988\n",
      "Epoch 244/300\n",
      "75/75 [==============================] - 0s 849us/step - loss: 0.0147 - accuracy: 0.9997 - val_loss: 0.0140 - val_accuracy: 0.9988\n",
      "Epoch 245/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0146 - accuracy: 0.9997 - val_loss: 0.0139 - val_accuracy: 0.9988\n",
      "Epoch 246/300\n",
      "75/75 [==============================] - 0s 933us/step - loss: 0.0146 - accuracy: 0.9992 - val_loss: 0.0138 - val_accuracy: 0.9992\n",
      "Epoch 247/300\n",
      "75/75 [==============================] - 0s 865us/step - loss: 0.0145 - accuracy: 0.9997 - val_loss: 0.0138 - val_accuracy: 0.9988\n",
      "Epoch 248/300\n",
      "75/75 [==============================] - 0s 831us/step - loss: 0.0144 - accuracy: 0.9997 - val_loss: 0.0138 - val_accuracy: 0.9988\n",
      "Epoch 249/300\n",
      "75/75 [==============================] - 0s 849us/step - loss: 0.0144 - accuracy: 0.9996 - val_loss: 0.0137 - val_accuracy: 0.9988\n",
      "Epoch 250/300\n",
      "75/75 [==============================] - 0s 827us/step - loss: 0.0143 - accuracy: 0.9996 - val_loss: 0.0136 - val_accuracy: 0.9992\n",
      "Epoch 251/300\n",
      "75/75 [==============================] - 0s 896us/step - loss: 0.0143 - accuracy: 0.9997 - val_loss: 0.0136 - val_accuracy: 0.9988\n",
      "Epoch 252/300\n",
      "75/75 [==============================] - 0s 875us/step - loss: 0.0142 - accuracy: 0.9995 - val_loss: 0.0136 - val_accuracy: 0.9988\n",
      "Epoch 253/300\n",
      "75/75 [==============================] - 0s 831us/step - loss: 0.0142 - accuracy: 0.9993 - val_loss: 0.0135 - val_accuracy: 0.9988\n",
      "Epoch 254/300\n",
      "75/75 [==============================] - 0s 898us/step - loss: 0.0141 - accuracy: 0.9996 - val_loss: 0.0134 - val_accuracy: 0.9992\n",
      "Epoch 255/300\n",
      "75/75 [==============================] - 0s 827us/step - loss: 0.0141 - accuracy: 0.9996 - val_loss: 0.0134 - val_accuracy: 0.9992\n",
      "Epoch 256/300\n",
      "75/75 [==============================] - 0s 804us/step - loss: 0.0140 - accuracy: 0.9996 - val_loss: 0.0133 - val_accuracy: 0.9996\n",
      "Epoch 257/300\n",
      "75/75 [==============================] - 0s 893us/step - loss: 0.0140 - accuracy: 0.9996 - val_loss: 0.0133 - val_accuracy: 0.9988\n",
      "Epoch 258/300\n",
      "75/75 [==============================] - 0s 1000us/step - loss: 0.0139 - accuracy: 0.9993 - val_loss: 0.0133 - val_accuracy: 0.9992\n",
      "Epoch 259/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0139 - accuracy: 0.9996 - val_loss: 0.0132 - val_accuracy: 0.9996\n",
      "Epoch 260/300\n",
      "75/75 [==============================] - 0s 841us/step - loss: 0.0138 - accuracy: 0.9995 - val_loss: 0.0132 - val_accuracy: 0.9996\n",
      "Epoch 261/300\n",
      "75/75 [==============================] - 0s 847us/step - loss: 0.0138 - accuracy: 0.9997 - val_loss: 0.0131 - val_accuracy: 0.9996\n",
      "Epoch 262/300\n",
      "75/75 [==============================] - 0s 813us/step - loss: 0.0137 - accuracy: 0.9997 - val_loss: 0.0131 - val_accuracy: 0.9996\n",
      "Epoch 263/300\n",
      "75/75 [==============================] - 0s 878us/step - loss: 0.0137 - accuracy: 0.9996 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
      "Epoch 264/300\n",
      "75/75 [==============================] - 0s 841us/step - loss: 0.0137 - accuracy: 0.9997 - val_loss: 0.0130 - val_accuracy: 0.9992\n",
      "Epoch 265/300\n",
      "75/75 [==============================] - 0s 857us/step - loss: 0.0136 - accuracy: 0.9999 - val_loss: 0.0129 - val_accuracy: 0.9992\n",
      "Epoch 266/300\n",
      "75/75 [==============================] - 0s 849us/step - loss: 0.0136 - accuracy: 0.9996 - val_loss: 0.0129 - val_accuracy: 0.9988\n",
      "Epoch 267/300\n",
      "75/75 [==============================] - 0s 880us/step - loss: 0.0135 - accuracy: 0.9997 - val_loss: 0.0128 - val_accuracy: 0.9988\n",
      "Epoch 268/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0135 - accuracy: 0.9996 - val_loss: 0.0128 - val_accuracy: 0.9988\n",
      "Epoch 269/300\n",
      "75/75 [==============================] - 0s 903us/step - loss: 0.0134 - accuracy: 0.9997 - val_loss: 0.0128 - val_accuracy: 0.9992\n",
      "Epoch 270/300\n",
      "75/75 [==============================] - 0s 862us/step - loss: 0.0134 - accuracy: 0.9997 - val_loss: 0.0127 - val_accuracy: 0.9988\n",
      "Epoch 271/300\n",
      "75/75 [==============================] - 0s 860us/step - loss: 0.0133 - accuracy: 0.9995 - val_loss: 0.0127 - val_accuracy: 0.9992\n",
      "Epoch 272/300\n",
      "75/75 [==============================] - 0s 847us/step - loss: 0.0133 - accuracy: 0.9997 - val_loss: 0.0126 - val_accuracy: 0.9992\n",
      "Epoch 273/300\n",
      "75/75 [==============================] - 0s 911us/step - loss: 0.0133 - accuracy: 0.9999 - val_loss: 0.0126 - val_accuracy: 0.9988\n",
      "Epoch 274/300\n",
      "75/75 [==============================] - 0s 836us/step - loss: 0.0132 - accuracy: 0.9997 - val_loss: 0.0125 - val_accuracy: 0.9992\n",
      "Epoch 275/300\n",
      "75/75 [==============================] - 0s 882us/step - loss: 0.0132 - accuracy: 0.9997 - val_loss: 0.0125 - val_accuracy: 0.9992\n",
      "Epoch 276/300\n",
      "75/75 [==============================] - 0s 871us/step - loss: 0.0131 - accuracy: 0.9997 - val_loss: 0.0125 - val_accuracy: 0.9992\n",
      "Epoch 277/300\n",
      "75/75 [==============================] - 0s 854us/step - loss: 0.0131 - accuracy: 0.9997 - val_loss: 0.0124 - val_accuracy: 0.9992\n",
      "Epoch 278/300\n",
      "75/75 [==============================] - 0s 871us/step - loss: 0.0130 - accuracy: 0.9997 - val_loss: 0.0124 - val_accuracy: 0.9988\n",
      "Epoch 279/300\n",
      "75/75 [==============================] - 0s 827us/step - loss: 0.0130 - accuracy: 0.9996 - val_loss: 0.0123 - val_accuracy: 0.9996\n",
      "Epoch 280/300\n",
      "75/75 [==============================] - 0s 908us/step - loss: 0.0130 - accuracy: 0.9997 - val_loss: 0.0123 - val_accuracy: 0.9988\n",
      "Epoch 281/300\n",
      "75/75 [==============================] - 0s 874us/step - loss: 0.0129 - accuracy: 0.9997 - val_loss: 0.0123 - val_accuracy: 0.9992\n",
      "Epoch 282/300\n",
      "75/75 [==============================] - 0s 867us/step - loss: 0.0129 - accuracy: 0.9997 - val_loss: 0.0122 - val_accuracy: 0.9992\n",
      "Epoch 283/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0128 - accuracy: 0.9996 - val_loss: 0.0122 - val_accuracy: 0.9996\n",
      "Epoch 284/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 877us/step - loss: 0.0128 - accuracy: 0.9997 - val_loss: 0.0121 - val_accuracy: 0.9996\n",
      "Epoch 285/300\n",
      "75/75 [==============================] - 0s 919us/step - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0121 - val_accuracy: 0.9992\n",
      "Epoch 286/300\n",
      "75/75 [==============================] - 0s 791us/step - loss: 0.0127 - accuracy: 0.9995 - val_loss: 0.0121 - val_accuracy: 0.9996\n",
      "Epoch 287/300\n",
      "75/75 [==============================] - 0s 852us/step - loss: 0.0127 - accuracy: 0.9997 - val_loss: 0.0120 - val_accuracy: 1.0000\n",
      "Epoch 288/300\n",
      "75/75 [==============================] - 0s 891us/step - loss: 0.0127 - accuracy: 0.9996 - val_loss: 0.0120 - val_accuracy: 0.9996\n",
      "Epoch 289/300\n",
      "75/75 [==============================] - 0s 886us/step - loss: 0.0126 - accuracy: 0.9997 - val_loss: 0.0120 - val_accuracy: 0.9996\n",
      "Epoch 290/300\n",
      "75/75 [==============================] - 0s 893us/step - loss: 0.0126 - accuracy: 0.9997 - val_loss: 0.0119 - val_accuracy: 0.9992\n",
      "Epoch 291/300\n",
      "75/75 [==============================] - 0s 883us/step - loss: 0.0125 - accuracy: 0.9997 - val_loss: 0.0119 - val_accuracy: 0.9988\n",
      "Epoch 292/300\n",
      "75/75 [==============================] - 0s 821us/step - loss: 0.0125 - accuracy: 0.9999 - val_loss: 0.0118 - val_accuracy: 0.9992\n",
      "Epoch 293/300\n",
      "75/75 [==============================] - 0s 871us/step - loss: 0.0125 - accuracy: 0.9992 - val_loss: 0.0118 - val_accuracy: 0.9996\n",
      "Epoch 294/300\n",
      "75/75 [==============================] - 0s 841us/step - loss: 0.0124 - accuracy: 0.9997 - val_loss: 0.0118 - val_accuracy: 0.9996\n",
      "Epoch 295/300\n",
      "75/75 [==============================] - 0s 854us/step - loss: 0.0124 - accuracy: 0.9997 - val_loss: 0.0117 - val_accuracy: 0.9996\n",
      "Epoch 296/300\n",
      "75/75 [==============================] - 0s 836us/step - loss: 0.0124 - accuracy: 0.9997 - val_loss: 0.0117 - val_accuracy: 0.9992\n",
      "Epoch 297/300\n",
      "75/75 [==============================] - 0s 820us/step - loss: 0.0123 - accuracy: 0.9997 - val_loss: 0.0117 - val_accuracy: 0.9996\n",
      "Epoch 298/300\n",
      "75/75 [==============================] - 0s 868us/step - loss: 0.0123 - accuracy: 0.9997 - val_loss: 0.0116 - val_accuracy: 0.9988\n",
      "Epoch 299/300\n",
      "75/75 [==============================] - 0s 872us/step - loss: 0.0122 - accuracy: 0.9991 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
      "Epoch 300/300\n",
      "75/75 [==============================] - 0s 841us/step - loss: 0.0122 - accuracy: 0.9999 - val_loss: 0.0116 - val_accuracy: 0.9988\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(2, activation='relu', input_shape=[2]))\n",
    "model.add(keras.layers.Dense(2, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_new, y_new, validation_split=0.25, epochs=300, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.3519330e-01],\n",
       "       [9.3519330e-01],\n",
       "       [9.3519330e-01],\n",
       "       [2.1820128e-02],\n",
       "       [1.2027707e-05],\n",
       "       [6.4852159e-09]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only with an increase of the epoch the results improved\n",
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20011928a08>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2380 - accuracy: 0.5045 - val_loss: 0.2380 - val_accuracy: 0.4984\n",
      "Epoch 2/300\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2337 - accuracy: 0.5045 - val_loss: 0.2338 - val_accuracy: 0.4984\n",
      "Epoch 3/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2297 - accuracy: 0.5045 - val_loss: 0.2298 - val_accuracy: 0.4984\n",
      "Epoch 4/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2258 - accuracy: 0.5045 - val_loss: 0.2261 - val_accuracy: 0.4984\n",
      "Epoch 5/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2222 - accuracy: 0.5045 - val_loss: 0.2225 - val_accuracy: 0.4984\n",
      "Epoch 6/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2187 - accuracy: 0.5051 - val_loss: 0.2191 - val_accuracy: 0.5064\n",
      "Epoch 7/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2154 - accuracy: 0.5437 - val_loss: 0.2159 - val_accuracy: 0.5552\n",
      "Epoch 8/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2123 - accuracy: 0.5984 - val_loss: 0.2129 - val_accuracy: 0.5984\n",
      "Epoch 9/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2094 - accuracy: 0.6456 - val_loss: 0.2100 - val_accuracy: 0.6440\n",
      "Epoch 10/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2066 - accuracy: 0.6893 - val_loss: 0.2073 - val_accuracy: 0.6848\n",
      "Epoch 11/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2039 - accuracy: 0.7245 - val_loss: 0.2046 - val_accuracy: 0.7192\n",
      "Epoch 12/300\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2013 - accuracy: 0.7565 - val_loss: 0.2021 - val_accuracy: 0.7512\n",
      "Epoch 13/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1988 - accuracy: 0.7821 - val_loss: 0.1997 - val_accuracy: 0.7808\n",
      "Epoch 14/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1965 - accuracy: 0.8037 - val_loss: 0.1974 - val_accuracy: 0.8056\n",
      "Epoch 15/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1942 - accuracy: 0.8232 - val_loss: 0.1952 - val_accuracy: 0.8256\n",
      "Epoch 16/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1920 - accuracy: 0.8405 - val_loss: 0.1930 - val_accuracy: 0.8456\n",
      "Epoch 17/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1899 - accuracy: 0.8613 - val_loss: 0.1910 - val_accuracy: 0.8688\n",
      "Epoch 18/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1878 - accuracy: 0.8760 - val_loss: 0.1890 - val_accuracy: 0.8832\n",
      "Epoch 19/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1859 - accuracy: 0.8891 - val_loss: 0.1870 - val_accuracy: 0.8896\n",
      "Epoch 20/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1840 - accuracy: 0.8968 - val_loss: 0.1852 - val_accuracy: 0.8992\n",
      "Epoch 21/300\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1821 - accuracy: 0.9069 - val_loss: 0.1834 - val_accuracy: 0.9136\n",
      "Epoch 22/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1803 - accuracy: 0.9160 - val_loss: 0.1816 - val_accuracy: 0.9176\n",
      "Epoch 23/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1785 - accuracy: 0.9237 - val_loss: 0.1799 - val_accuracy: 0.9240\n",
      "Epoch 24/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.9339 - val_loss: 0.1782 - val_accuracy: 0.9360\n",
      "Epoch 25/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1752 - accuracy: 0.9405 - val_loss: 0.1765 - val_accuracy: 0.9424\n",
      "Epoch 26/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1735 - accuracy: 0.9459 - val_loss: 0.1750 - val_accuracy: 0.9472\n",
      "Epoch 27/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1719 - accuracy: 0.9501 - val_loss: 0.1734 - val_accuracy: 0.9504\n",
      "Epoch 28/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1704 - accuracy: 0.9557 - val_loss: 0.1719 - val_accuracy: 0.9544\n",
      "Epoch 29/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1689 - accuracy: 0.9621 - val_loss: 0.1704 - val_accuracy: 0.9552\n",
      "Epoch 30/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.9648 - val_loss: 0.1689 - val_accuracy: 0.9600\n",
      "Epoch 31/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1659 - accuracy: 0.9691 - val_loss: 0.1675 - val_accuracy: 0.9656\n",
      "Epoch 32/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.9717 - val_loss: 0.1661 - val_accuracy: 0.9680\n",
      "Epoch 33/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1631 - accuracy: 0.9744 - val_loss: 0.1648 - val_accuracy: 0.9704\n",
      "Epoch 34/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1617 - accuracy: 0.9768 - val_loss: 0.1634 - val_accuracy: 0.9720\n",
      "Epoch 35/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9784 - val_loss: 0.1621 - val_accuracy: 0.9752\n",
      "Epoch 36/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1591 - accuracy: 0.9803 - val_loss: 0.1608 - val_accuracy: 0.9784\n",
      "Epoch 37/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.9819 - val_loss: 0.1596 - val_accuracy: 0.9800\n",
      "Epoch 38/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1565 - accuracy: 0.9821 - val_loss: 0.1583 - val_accuracy: 0.9800\n",
      "Epoch 39/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9853 - val_loss: 0.1571 - val_accuracy: 0.9816\n",
      "Epoch 40/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9867 - val_loss: 0.1559 - val_accuracy: 0.9832\n",
      "Epoch 41/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9872 - val_loss: 0.1548 - val_accuracy: 0.9848\n",
      "Epoch 42/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9883 - val_loss: 0.1536 - val_accuracy: 0.9880\n",
      "Epoch 43/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.9888 - val_loss: 0.1525 - val_accuracy: 0.9888\n",
      "Epoch 44/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9901 - val_loss: 0.1514 - val_accuracy: 0.9920\n",
      "Epoch 45/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9909 - val_loss: 0.1503 - val_accuracy: 0.9920\n",
      "Epoch 46/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1473 - accuracy: 0.9912 - val_loss: 0.1492 - val_accuracy: 0.9928\n",
      "Epoch 47/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9912 - val_loss: 0.1482 - val_accuracy: 0.9928\n",
      "Epoch 48/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9915 - val_loss: 0.1472 - val_accuracy: 0.9928\n",
      "Epoch 49/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.9920 - val_loss: 0.1462 - val_accuracy: 0.9928\n",
      "Epoch 50/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9920 - val_loss: 0.1452 - val_accuracy: 0.9936\n",
      "Epoch 51/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9923 - val_loss: 0.1442 - val_accuracy: 0.9944\n",
      "Epoch 52/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9928 - val_loss: 0.1432 - val_accuracy: 0.9944\n",
      "Epoch 53/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1402 - accuracy: 0.9928 - val_loss: 0.1423 - val_accuracy: 0.9952\n",
      "Epoch 54/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9936 - val_loss: 0.1414 - val_accuracy: 0.9952\n",
      "Epoch 55/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9936 - val_loss: 0.1404 - val_accuracy: 0.9952\n",
      "Epoch 56/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9939 - val_loss: 0.1395 - val_accuracy: 0.9968\n",
      "Epoch 57/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1365 - accuracy: 0.9936 - val_loss: 0.1387 - val_accuracy: 0.9976\n",
      "Epoch 58/300\n",
      "38/38 [==============================] - 0s 998us/step - loss: 0.1356 - accuracy: 0.9944 - val_loss: 0.1378 - val_accuracy: 0.9976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9947 - val_loss: 0.1369 - val_accuracy: 0.9976\n",
      "Epoch 60/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9952 - val_loss: 0.1361 - val_accuracy: 0.9976\n",
      "Epoch 61/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.9963 - val_loss: 0.1353 - val_accuracy: 0.9976\n",
      "Epoch 62/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9960 - val_loss: 0.1345 - val_accuracy: 0.9976\n",
      "Epoch 63/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1314 - accuracy: 0.9960 - val_loss: 0.1337 - val_accuracy: 0.9976\n",
      "Epoch 64/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9960 - val_loss: 0.1329 - val_accuracy: 0.9976\n",
      "Epoch 65/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9965 - val_loss: 0.1321 - val_accuracy: 0.9976\n",
      "Epoch 66/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9965 - val_loss: 0.1314 - val_accuracy: 0.9976\n",
      "Epoch 67/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9968 - val_loss: 0.1306 - val_accuracy: 0.9976\n",
      "Epoch 68/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9968 - val_loss: 0.1299 - val_accuracy: 0.9976\n",
      "Epoch 69/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9965 - val_loss: 0.1291 - val_accuracy: 0.9976\n",
      "Epoch 70/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9968 - val_loss: 0.1284 - val_accuracy: 0.9976\n",
      "Epoch 71/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9971 - val_loss: 0.1277 - val_accuracy: 0.9976\n",
      "Epoch 72/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9971 - val_loss: 0.1270 - val_accuracy: 0.9976\n",
      "Epoch 73/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9971 - val_loss: 0.1263 - val_accuracy: 0.9976\n",
      "Epoch 74/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1233 - accuracy: 0.9968 - val_loss: 0.1257 - val_accuracy: 0.9976\n",
      "Epoch 75/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9973 - val_loss: 0.1250 - val_accuracy: 0.9976\n",
      "Epoch 76/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9973 - val_loss: 0.1244 - val_accuracy: 0.9976\n",
      "Epoch 77/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9976 - val_loss: 0.1237 - val_accuracy: 0.9976\n",
      "Epoch 78/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9971 - val_loss: 0.1231 - val_accuracy: 0.9976\n",
      "Epoch 79/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1201 - accuracy: 0.9976 - val_loss: 0.1224 - val_accuracy: 0.9976\n",
      "Epoch 80/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9973 - val_loss: 0.1218 - val_accuracy: 0.9976\n",
      "Epoch 81/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9979 - val_loss: 0.1212 - val_accuracy: 0.9976\n",
      "Epoch 82/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.9976 - val_loss: 0.1206 - val_accuracy: 0.9976\n",
      "Epoch 83/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9976 - val_loss: 0.1200 - val_accuracy: 0.9976\n",
      "Epoch 84/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9976 - val_loss: 0.1194 - val_accuracy: 0.9984\n",
      "Epoch 85/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9979 - val_loss: 0.1189 - val_accuracy: 0.9984\n",
      "Epoch 86/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9979 - val_loss: 0.1183 - val_accuracy: 0.9984\n",
      "Epoch 87/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9981 - val_loss: 0.1177 - val_accuracy: 0.9984\n",
      "Epoch 88/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9979 - val_loss: 0.1172 - val_accuracy: 0.9984\n",
      "Epoch 89/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9979 - val_loss: 0.1166 - val_accuracy: 0.9984\n",
      "Epoch 90/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.9979 - val_loss: 0.1161 - val_accuracy: 0.9984\n",
      "Epoch 91/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.9981 - val_loss: 0.1156 - val_accuracy: 0.9984\n",
      "Epoch 92/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9981 - val_loss: 0.1151 - val_accuracy: 0.9984\n",
      "Epoch 93/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9981 - val_loss: 0.1145 - val_accuracy: 0.9984\n",
      "Epoch 94/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9981 - val_loss: 0.1140 - val_accuracy: 0.9984\n",
      "Epoch 95/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1111 - accuracy: 0.9981 - val_loss: 0.1135 - val_accuracy: 0.9984\n",
      "Epoch 96/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9981 - val_loss: 0.1130 - val_accuracy: 0.9984\n",
      "Epoch 97/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9981 - val_loss: 0.1125 - val_accuracy: 0.9984\n",
      "Epoch 98/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9987 - val_loss: 0.1121 - val_accuracy: 0.9984\n",
      "Epoch 99/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9989 - val_loss: 0.1116 - val_accuracy: 0.9984\n",
      "Epoch 100/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9989 - val_loss: 0.1111 - val_accuracy: 0.9984\n",
      "Epoch 101/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.9987 - val_loss: 0.1106 - val_accuracy: 0.9984\n",
      "Epoch 102/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9989 - val_loss: 0.1102 - val_accuracy: 0.9984\n",
      "Epoch 103/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9989 - val_loss: 0.1097 - val_accuracy: 0.9984\n",
      "Epoch 104/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9987 - val_loss: 0.1093 - val_accuracy: 0.9984\n",
      "Epoch 105/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1064 - accuracy: 0.9989 - val_loss: 0.1088 - val_accuracy: 0.9984\n",
      "Epoch 106/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1059 - accuracy: 0.9989 - val_loss: 0.1084 - val_accuracy: 0.9984\n",
      "Epoch 107/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9989 - val_loss: 0.1080 - val_accuracy: 0.9984\n",
      "Epoch 108/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9989 - val_loss: 0.1075 - val_accuracy: 0.9984\n",
      "Epoch 109/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9989 - val_loss: 0.1071 - val_accuracy: 0.9984\n",
      "Epoch 110/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9989 - val_loss: 0.1067 - val_accuracy: 0.9984\n",
      "Epoch 111/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9989 - val_loss: 0.1063 - val_accuracy: 0.9984\n",
      "Epoch 112/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9989 - val_loss: 0.1059 - val_accuracy: 0.9984\n",
      "Epoch 113/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9989 - val_loss: 0.1055 - val_accuracy: 0.9984\n",
      "Epoch 114/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9989 - val_loss: 0.1051 - val_accuracy: 0.9984\n",
      "Epoch 115/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9989 - val_loss: 0.1047 - val_accuracy: 0.9984\n",
      "Epoch 116/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1018 - accuracy: 0.9987 - val_loss: 0.1043 - val_accuracy: 0.9984\n",
      "Epoch 117/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1015 - accuracy: 0.9989 - val_loss: 0.1039 - val_accuracy: 0.9984\n",
      "Epoch 118/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9989 - val_loss: 0.1035 - val_accuracy: 0.9984\n",
      "Epoch 119/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.9989 - val_loss: 0.1032 - val_accuracy: 0.9984\n",
      "Epoch 120/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9992 - val_loss: 0.1028 - val_accuracy: 0.9984\n",
      "Epoch 121/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9989 - val_loss: 0.1024 - val_accuracy: 0.9984\n",
      "Epoch 122/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9992 - val_loss: 0.1021 - val_accuracy: 0.9984\n",
      "Epoch 123/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9989 - val_loss: 0.1017 - val_accuracy: 0.9984\n",
      "Epoch 124/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9989 - val_loss: 0.1013 - val_accuracy: 0.9984\n",
      "Epoch 125/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9992 - val_loss: 0.1010 - val_accuracy: 0.9984\n",
      "Epoch 126/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9992 - val_loss: 0.1007 - val_accuracy: 0.9984\n",
      "Epoch 127/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.9989 - val_loss: 0.1003 - val_accuracy: 0.9984\n",
      "Epoch 128/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9989 - val_loss: 0.1000 - val_accuracy: 0.9984\n",
      "Epoch 129/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.9992 - val_loss: 0.0996 - val_accuracy: 0.9984\n",
      "Epoch 130/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9992 - val_loss: 0.0993 - val_accuracy: 0.9984\n",
      "Epoch 131/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9992 - val_loss: 0.0990 - val_accuracy: 0.9984\n",
      "Epoch 132/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9992 - val_loss: 0.0986 - val_accuracy: 0.9984\n",
      "Epoch 133/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9992 - val_loss: 0.0983 - val_accuracy: 0.9984\n",
      "Epoch 134/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0956 - accuracy: 0.9992 - val_loss: 0.0980 - val_accuracy: 0.9984\n",
      "Epoch 135/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9992 - val_loss: 0.0977 - val_accuracy: 0.9984\n",
      "Epoch 136/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9992 - val_loss: 0.0974 - val_accuracy: 0.9984\n",
      "Epoch 137/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9992 - val_loss: 0.0971 - val_accuracy: 0.9984\n",
      "Epoch 138/300\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.0943 - accuracy: 0.9992 - val_loss: 0.0968 - val_accuracy: 0.9984\n",
      "Epoch 139/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9992 - val_loss: 0.0965 - val_accuracy: 0.9984\n",
      "Epoch 140/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.9992 - val_loss: 0.0962 - val_accuracy: 0.9984\n",
      "Epoch 141/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9992 - val_loss: 0.0959 - val_accuracy: 0.9984\n",
      "Epoch 142/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9992 - val_loss: 0.0956 - val_accuracy: 0.9984\n",
      "Epoch 143/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9992 - val_loss: 0.0953 - val_accuracy: 0.9984\n",
      "Epoch 144/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9992 - val_loss: 0.0950 - val_accuracy: 0.9984\n",
      "Epoch 145/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9992 - val_loss: 0.0947 - val_accuracy: 0.9984\n",
      "Epoch 146/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0920 - accuracy: 0.9992 - val_loss: 0.0944 - val_accuracy: 0.9984\n",
      "Epoch 147/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0917 - accuracy: 0.9992 - val_loss: 0.0941 - val_accuracy: 0.9984\n",
      "Epoch 148/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9992 - val_loss: 0.0939 - val_accuracy: 0.9984\n",
      "Epoch 149/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9992 - val_loss: 0.0936 - val_accuracy: 0.9984\n",
      "Epoch 150/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9995 - val_loss: 0.0933 - val_accuracy: 0.9984\n",
      "Epoch 151/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9995 - val_loss: 0.0930 - val_accuracy: 0.9984\n",
      "Epoch 152/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9992 - val_loss: 0.0928 - val_accuracy: 0.9984\n",
      "Epoch 153/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9992 - val_loss: 0.0925 - val_accuracy: 0.9984\n",
      "Epoch 154/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.9989 - val_loss: 0.0922 - val_accuracy: 0.9984\n",
      "Epoch 155/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.9992 - val_loss: 0.0920 - val_accuracy: 0.9984\n",
      "Epoch 156/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.9992 - val_loss: 0.0917 - val_accuracy: 0.9984\n",
      "Epoch 157/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9992 - val_loss: 0.0915 - val_accuracy: 0.9984\n",
      "Epoch 158/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9992 - val_loss: 0.0912 - val_accuracy: 0.9984\n",
      "Epoch 159/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0886 - accuracy: 0.9995 - val_loss: 0.0910 - val_accuracy: 0.9984\n",
      "Epoch 160/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0883 - accuracy: 0.9992 - val_loss: 0.0907 - val_accuracy: 0.9984\n",
      "Epoch 161/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9992 - val_loss: 0.0905 - val_accuracy: 0.9984\n",
      "Epoch 162/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9995 - val_loss: 0.0902 - val_accuracy: 0.9984\n",
      "Epoch 163/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.9995 - val_loss: 0.0900 - val_accuracy: 0.9984\n",
      "Epoch 164/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9992 - val_loss: 0.0898 - val_accuracy: 0.9984\n",
      "Epoch 165/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9992 - val_loss: 0.0895 - val_accuracy: 0.9984\n",
      "Epoch 166/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.9992 - val_loss: 0.0893 - val_accuracy: 0.9984\n",
      "Epoch 167/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9992 - val_loss: 0.0890 - val_accuracy: 0.9984\n",
      "Epoch 168/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.9992 - val_loss: 0.0888 - val_accuracy: 0.9984\n",
      "Epoch 169/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9992 - val_loss: 0.0886 - val_accuracy: 0.9984\n",
      "Epoch 170/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9995 - val_loss: 0.0884 - val_accuracy: 0.9984\n",
      "Epoch 171/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9997 - val_loss: 0.0881 - val_accuracy: 0.9984\n",
      "Epoch 172/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.9992 - val_loss: 0.0879 - val_accuracy: 0.9984\n",
      "Epoch 173/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.9995 - val_loss: 0.0877 - val_accuracy: 0.9984\n",
      "Epoch 174/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.9997 - val_loss: 0.0875 - val_accuracy: 0.9984\n",
      "Epoch 175/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.9995 - val_loss: 0.0873 - val_accuracy: 0.9984\n",
      "Epoch 176/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0847 - accuracy: 0.9992 - val_loss: 0.0870 - val_accuracy: 0.9984\n",
      "Epoch 177/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9995 - val_loss: 0.0868 - val_accuracy: 0.9984\n",
      "Epoch 178/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9995 - val_loss: 0.0866 - val_accuracy: 0.9984\n",
      "Epoch 179/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9997 - val_loss: 0.0864 - val_accuracy: 0.9984\n",
      "Epoch 180/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9992 - val_loss: 0.0862 - val_accuracy: 0.9984\n",
      "Epoch 181/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9995 - val_loss: 0.0860 - val_accuracy: 0.9984\n",
      "Epoch 182/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.9992 - val_loss: 0.0858 - val_accuracy: 0.9984\n",
      "Epoch 183/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9997 - val_loss: 0.0856 - val_accuracy: 0.9984\n",
      "Epoch 184/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.9995 - val_loss: 0.0854 - val_accuracy: 0.9984\n",
      "Epoch 185/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9995 - val_loss: 0.0852 - val_accuracy: 0.9984\n",
      "Epoch 186/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9995 - val_loss: 0.0850 - val_accuracy: 0.9984\n",
      "Epoch 187/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9992 - val_loss: 0.0848 - val_accuracy: 0.9984\n",
      "Epoch 188/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9992 - val_loss: 0.0846 - val_accuracy: 0.9984\n",
      "Epoch 189/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9995 - val_loss: 0.0844 - val_accuracy: 0.9984\n",
      "Epoch 190/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.0819 - accuracy: 0.9997 - val_loss: 0.0842 - val_accuracy: 0.9984\n",
      "Epoch 191/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9997 - val_loss: 0.0840 - val_accuracy: 0.9984\n",
      "Epoch 192/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9992 - val_loss: 0.0838 - val_accuracy: 0.9984\n",
      "Epoch 193/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.9995 - val_loss: 0.0836 - val_accuracy: 0.9984\n",
      "Epoch 194/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9995 - val_loss: 0.0834 - val_accuracy: 0.9984\n",
      "Epoch 195/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9997 - val_loss: 0.0832 - val_accuracy: 0.9984\n",
      "Epoch 196/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9995 - val_loss: 0.0831 - val_accuracy: 0.9984\n",
      "Epoch 197/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.9997 - val_loss: 0.0829 - val_accuracy: 0.9984\n",
      "Epoch 198/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.9997 - val_loss: 0.0827 - val_accuracy: 0.9984\n",
      "Epoch 199/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9997 - val_loss: 0.0825 - val_accuracy: 0.9992\n",
      "Epoch 200/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9997 - val_loss: 0.0823 - val_accuracy: 0.9992\n",
      "Epoch 201/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0799 - accuracy: 0.9997 - val_loss: 0.0822 - val_accuracy: 0.9992\n",
      "Epoch 202/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.9997 - val_loss: 0.0820 - val_accuracy: 0.9992\n",
      "Epoch 203/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9997 - val_loss: 0.0818 - val_accuracy: 0.9984\n",
      "Epoch 204/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9997 - val_loss: 0.0816 - val_accuracy: 0.9984\n",
      "Epoch 205/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0792 - accuracy: 0.9997 - val_loss: 0.0815 - val_accuracy: 0.9984\n",
      "Epoch 206/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.9997 - val_loss: 0.0813 - val_accuracy: 0.9984\n",
      "Epoch 207/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9997 - val_loss: 0.0811 - val_accuracy: 0.9984\n",
      "Epoch 208/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.9997 - val_loss: 0.0810 - val_accuracy: 0.9984\n",
      "Epoch 209/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.9995 - val_loss: 0.0808 - val_accuracy: 0.9984\n",
      "Epoch 210/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9995 - val_loss: 0.0806 - val_accuracy: 0.9984\n",
      "Epoch 211/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9997 - val_loss: 0.0805 - val_accuracy: 0.9984\n",
      "Epoch 212/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.9995 - val_loss: 0.0803 - val_accuracy: 0.9984\n",
      "Epoch 213/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.9995 - val_loss: 0.0801 - val_accuracy: 0.9984\n",
      "Epoch 214/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.9997 - val_loss: 0.0800 - val_accuracy: 0.9984\n",
      "Epoch 215/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.9997 - val_loss: 0.0798 - val_accuracy: 0.9984\n",
      "Epoch 216/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0774 - accuracy: 0.9995 - val_loss: 0.0797 - val_accuracy: 0.9984\n",
      "Epoch 217/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9997 - val_loss: 0.0795 - val_accuracy: 0.9984\n",
      "Epoch 218/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9997 - val_loss: 0.0793 - val_accuracy: 0.9984\n",
      "Epoch 219/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9997 - val_loss: 0.0792 - val_accuracy: 0.9984\n",
      "Epoch 220/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.9997 - val_loss: 0.0790 - val_accuracy: 0.9984\n",
      "Epoch 221/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9995 - val_loss: 0.0789 - val_accuracy: 0.9984\n",
      "Epoch 222/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0765 - accuracy: 0.9997 - val_loss: 0.0787 - val_accuracy: 0.9984\n",
      "Epoch 223/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9997 - val_loss: 0.0786 - val_accuracy: 0.9984\n",
      "Epoch 224/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.9997 - val_loss: 0.0784 - val_accuracy: 0.9984\n",
      "Epoch 225/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.9997 - val_loss: 0.0783 - val_accuracy: 0.9984\n",
      "Epoch 226/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9997 - val_loss: 0.0781 - val_accuracy: 0.9984\n",
      "Epoch 227/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.9997 - val_loss: 0.0780 - val_accuracy: 0.9984\n",
      "Epoch 228/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.9997 - val_loss: 0.0778 - val_accuracy: 0.9984\n",
      "Epoch 229/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.9997 - val_loss: 0.0777 - val_accuracy: 0.9984\n",
      "Epoch 230/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9995 - val_loss: 0.0775 - val_accuracy: 0.9992\n",
      "Epoch 231/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9995 - val_loss: 0.0774 - val_accuracy: 0.9984\n",
      "Epoch 232/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9997 - val_loss: 0.0772 - val_accuracy: 0.9992\n",
      "Epoch 233/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.9997 - val_loss: 0.0771 - val_accuracy: 0.9992\n",
      "Epoch 234/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.9997 - val_loss: 0.0770 - val_accuracy: 0.9992\n",
      "Epoch 235/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9997 - val_loss: 0.0768 - val_accuracy: 0.9992\n",
      "Epoch 236/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9997 - val_loss: 0.0767 - val_accuracy: 0.9992\n",
      "Epoch 237/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.9997 - val_loss: 0.0765 - val_accuracy: 0.9992\n",
      "Epoch 238/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0743 - accuracy: 0.9997 - val_loss: 0.0764 - val_accuracy: 0.9992\n",
      "Epoch 239/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.9997 - val_loss: 0.0763 - val_accuracy: 0.9992\n",
      "Epoch 240/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.9997 - val_loss: 0.0761 - val_accuracy: 0.9992\n",
      "Epoch 241/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.9997 - val_loss: 0.0760 - val_accuracy: 0.9992\n",
      "Epoch 242/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9997 - val_loss: 0.0759 - val_accuracy: 0.9992\n",
      "Epoch 243/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.9997 - val_loss: 0.0757 - val_accuracy: 0.9992\n",
      "Epoch 244/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9997 - val_loss: 0.0756 - val_accuracy: 0.9992\n",
      "Epoch 245/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.9997 - val_loss: 0.0755 - val_accuracy: 0.9992\n",
      "Epoch 246/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9997 - val_loss: 0.0753 - val_accuracy: 0.9992\n",
      "Epoch 247/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.9997 - val_loss: 0.0752 - val_accuracy: 0.9992\n",
      "Epoch 248/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.9997 - val_loss: 0.0751 - val_accuracy: 0.9992\n",
      "Epoch 249/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.9997 - val_loss: 0.0749 - val_accuracy: 0.9992\n",
      "Epoch 250/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.9997 - val_loss: 0.0748 - val_accuracy: 0.9992\n",
      "Epoch 251/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.9997 - val_loss: 0.0747 - val_accuracy: 0.9992\n",
      "Epoch 252/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.9997 - val_loss: 0.0746 - val_accuracy: 0.9992\n",
      "Epoch 253/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.9997 - val_loss: 0.0744 - val_accuracy: 0.9992\n",
      "Epoch 254/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.9997 - val_loss: 0.0743 - val_accuracy: 0.9992\n",
      "Epoch 255/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9997 - val_loss: 0.0742 - val_accuracy: 0.9992\n",
      "Epoch 256/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.9997 - val_loss: 0.0741 - val_accuracy: 0.9992\n",
      "Epoch 257/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9997 - val_loss: 0.0739 - val_accuracy: 0.9992\n",
      "Epoch 258/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9997 - val_loss: 0.0738 - val_accuracy: 0.9992\n",
      "Epoch 259/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9997 - val_loss: 0.0737 - val_accuracy: 0.9992\n",
      "Epoch 260/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.9997 - val_loss: 0.0736 - val_accuracy: 0.9992\n",
      "Epoch 261/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.9997 - val_loss: 0.0735 - val_accuracy: 0.9992\n",
      "Epoch 262/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.9997 - val_loss: 0.0733 - val_accuracy: 0.9992\n",
      "Epoch 263/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.9997 - val_loss: 0.0732 - val_accuracy: 0.9992\n",
      "Epoch 264/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.9997 - val_loss: 0.0731 - val_accuracy: 0.9992\n",
      "Epoch 265/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.9997 - val_loss: 0.0730 - val_accuracy: 0.9992\n",
      "Epoch 266/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9997 - val_loss: 0.0729 - val_accuracy: 0.9992\n",
      "Epoch 267/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.9997 - val_loss: 0.0728 - val_accuracy: 0.9992\n",
      "Epoch 268/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.9997 - val_loss: 0.0726 - val_accuracy: 0.9992\n",
      "Epoch 269/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0705 - accuracy: 0.9997 - val_loss: 0.0725 - val_accuracy: 0.9992\n",
      "Epoch 270/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.9997 - val_loss: 0.0724 - val_accuracy: 0.9992\n",
      "Epoch 271/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.9997 - val_loss: 0.0723 - val_accuracy: 0.9992\n",
      "Epoch 272/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.9995 - val_loss: 0.0722 - val_accuracy: 0.9992\n",
      "Epoch 273/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.9997 - val_loss: 0.0721 - val_accuracy: 0.9992\n",
      "Epoch 274/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9997 - val_loss: 0.0720 - val_accuracy: 0.9992\n",
      "Epoch 275/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 0.9997 - val_loss: 0.0719 - val_accuracy: 0.9992\n",
      "Epoch 276/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.9997 - val_loss: 0.0718 - val_accuracy: 0.9992\n",
      "Epoch 277/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.9997 - val_loss: 0.0716 - val_accuracy: 0.9992\n",
      "Epoch 278/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.9997 - val_loss: 0.0715 - val_accuracy: 0.9992\n",
      "Epoch 279/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.9997 - val_loss: 0.0714 - val_accuracy: 0.9992\n",
      "Epoch 280/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.9997 - val_loss: 0.0713 - val_accuracy: 0.9992\n",
      "Epoch 281/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.9997 - val_loss: 0.0712 - val_accuracy: 0.9992\n",
      "Epoch 282/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.9995 - val_loss: 0.0711 - val_accuracy: 0.9992\n",
      "Epoch 283/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.9997 - val_loss: 0.0710 - val_accuracy: 0.9992\n",
      "Epoch 284/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9997 - val_loss: 0.0709 - val_accuracy: 0.9992\n",
      "Epoch 285/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.9997 - val_loss: 0.0708 - val_accuracy: 0.9992\n",
      "Epoch 286/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.9997 - val_loss: 0.0707 - val_accuracy: 0.9992\n",
      "Epoch 287/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9997 - val_loss: 0.0706 - val_accuracy: 0.9992\n",
      "Epoch 288/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9997 - val_loss: 0.0705 - val_accuracy: 0.9992\n",
      "Epoch 289/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.9997 - val_loss: 0.0704 - val_accuracy: 0.9992\n",
      "Epoch 290/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.9997 - val_loss: 0.0703 - val_accuracy: 0.9984\n",
      "Epoch 291/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.9997 - val_loss: 0.0702 - val_accuracy: 0.9984\n",
      "Epoch 292/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.9997 - val_loss: 0.0701 - val_accuracy: 0.9984\n",
      "Epoch 293/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.9995 - val_loss: 0.0700 - val_accuracy: 0.9992\n",
      "Epoch 294/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.9995 - val_loss: 0.0699 - val_accuracy: 0.9992\n",
      "Epoch 295/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.9997 - val_loss: 0.0698 - val_accuracy: 0.9992\n",
      "Epoch 296/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0677 - accuracy: 0.9997 - val_loss: 0.0697 - val_accuracy: 0.9992\n",
      "Epoch 297/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.9997 - val_loss: 0.0696 - val_accuracy: 0.9992\n",
      "Epoch 298/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.9997 - val_loss: 0.0695 - val_accuracy: 0.9992\n",
      "Epoch 299/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.9997 - val_loss: 0.0694 - val_accuracy: 0.9992\n",
      "Epoch 300/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.9997 - val_loss: 0.0693 - val_accuracy: 0.9992\n"
     ]
    }
   ],
   "source": [
    "# How does the model perform with many data and the original layers?\n",
    "x_0 = np.random.random(size=(5000))\n",
    "x_1 = np.ones(shape=5000) - x_0\n",
    "x_val_new = np.stack([x_0, x_1], axis=1)\n",
    "y_val_new = np.zeros(5000, dtype=np.float32)\n",
    "y_val_new[x_val_new[:,1] > x_val_new[:,0]] = 1.0\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=[2]))\n",
    "# However, relu does not change really anything, since all input is >0\n",
    "# so I guess the input is the same for the sigmoid.\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='mse',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_val_new, y_val_new, validation_split=0.25, epochs=300, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9466989 ],\n",
       "       [0.84926796],\n",
       "       [0.64123255],\n",
       "       [0.36182874],\n",
       "       [0.15244016],\n",
       "       [0.05397525]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The additional ReLu-Layers seem to make a difference in the quality of the result.\n",
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Correct loss function\n",
    "\n",
    "The loss function used above (mse) is not optimal. A better loss function would be the crossentropy. Change the network to use that loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6932 - val_accuracy: 0.4808\n",
      "Epoch 2/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6932 - accuracy: 0.4925 - val_loss: 0.6932 - val_accuracy: 0.4808\n",
      "Epoch 3/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6932 - val_accuracy: 0.4808\n",
      "Epoch 4/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6933 - val_accuracy: 0.4808\n",
      "Epoch 5/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6933 - val_accuracy: 0.4808\n",
      "Epoch 6/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6933 - val_accuracy: 0.4808\n",
      "Epoch 7/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6933 - val_accuracy: 0.4808\n",
      "Epoch 8/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6933 - val_accuracy: 0.4808\n",
      "Epoch 9/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4808\n",
      "Epoch 10/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4808\n",
      "Epoch 11/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4808\n",
      "Epoch 12/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4808\n",
      "Epoch 13/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4808\n",
      "Epoch 14/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4808\n",
      "Epoch 15/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4808\n",
      "Epoch 16/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4808\n",
      "Epoch 17/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4808\n",
      "Epoch 18/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4808\n",
      "Epoch 19/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4808\n",
      "Epoch 20/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4808\n",
      "Epoch 21/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6934 - val_accuracy: 0.4808\n",
      "Epoch 22/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 23/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 24/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 25/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 26/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 27/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 28/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 29/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 30/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 31/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 32/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 33/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 34/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 35/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 36/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 37/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 38/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 39/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 40/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 41/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 42/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 43/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 44/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 45/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 46/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 47/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 48/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 49/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 50/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 51/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 52/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 53/300\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 54/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 55/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 56/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 57/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 58/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 60/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 61/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 62/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 63/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 64/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 65/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 66/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 67/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 68/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 69/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 70/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 71/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 72/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 73/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 74/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 75/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 76/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 77/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 78/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 79/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 80/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 81/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 82/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 83/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 84/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 85/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 86/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 87/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 88/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 89/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 90/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 91/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 92/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6936 - val_accuracy: 0.4808\n",
      "Epoch 93/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6936 - val_accuracy: 0.4808\n",
      "Epoch 94/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6936 - val_accuracy: 0.4808\n",
      "Epoch 95/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6936 - val_accuracy: 0.4808\n",
      "Epoch 96/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6936 - val_accuracy: 0.4808\n",
      "Epoch 97/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6936 - val_accuracy: 0.4808\n",
      "Epoch 98/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6936 - val_accuracy: 0.4808\n",
      "Epoch 99/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 100/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 101/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 102/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 103/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 104/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 105/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 106/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 107/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 108/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 109/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 110/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 111/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 112/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 113/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 114/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 115/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 116/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 117/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 118/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 119/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 120/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 121/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 122/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 123/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 124/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 125/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 126/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 127/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 128/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 129/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 130/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 131/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 132/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 133/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 134/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 135/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 136/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 137/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 138/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 139/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 140/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 141/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 142/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 143/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 144/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 145/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 146/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 147/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 148/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 149/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 150/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 151/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 152/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 153/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 154/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 155/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 156/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 157/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 158/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 159/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 160/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 161/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 162/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 163/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 164/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 165/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 166/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 167/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 168/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 169/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 170/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 171/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 172/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 173/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 174/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 175/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 176/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 177/300\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 178/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 179/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 180/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 181/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 182/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 183/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 184/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 185/300\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 186/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 187/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 188/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 189/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 190/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 191/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 192/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 193/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 194/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 195/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 196/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 197/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 198/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 199/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 200/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 201/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 202/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 203/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 204/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 205/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 206/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 207/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 208/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 209/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 210/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 211/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 212/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 213/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 214/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 215/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 216/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 217/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 218/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 219/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 220/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 221/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 222/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 223/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 224/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 225/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 226/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 227/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 228/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 229/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 230/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 231/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 232/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 233/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 234/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 235/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 236/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 237/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 238/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 239/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 240/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 241/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 242/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 243/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 244/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 245/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 246/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 247/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 248/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 249/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 250/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 251/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 252/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 253/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 254/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 255/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 256/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 257/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 258/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 259/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6936 - val_accuracy: 0.4808\n",
      "Epoch 260/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 261/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 262/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 263/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 264/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 265/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 266/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6936 - val_accuracy: 0.4808\n",
      "Epoch 267/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 268/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 269/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 270/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 271/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 272/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 273/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 274/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 275/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 276/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 277/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 278/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 279/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 280/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 281/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 282/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 283/300\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 284/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 285/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 286/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 287/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 288/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 289/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 290/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 291/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 292/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 293/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 294/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 295/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 296/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 297/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 298/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 299/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n",
      "Epoch 300/300\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6931 - accuracy: 0.5043 - val_loss: 0.6935 - val_accuracy: 0.4808\n"
     ]
    }
   ],
   "source": [
    "# How does the model perform with many data and the original layers?\n",
    "x_0 = np.random.random(size=(5000))\n",
    "x_1 = np.ones(shape=5000) - x_0\n",
    "x_val_new = np.stack([x_0, x_1], axis=1)\n",
    "y_val_new = np.zeros(5000, dtype=np.float32)\n",
    "y_val_new[x_val_new[:,1] > x_val_new[:,0]] = 1.0\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=[2]))\n",
    "model.add(keras.layers.Dense(2, activation='relu'))\n",
    "model.add(keras.layers.Dense(2, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "# Binary crossentropy works only if we have two categories\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "model.compile(loss= loss_fn,\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_val_new, y_val_new, validation_split=0.25, epochs=300, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000020011938288> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.49589917],\n",
       "       [0.49589917],\n",
       "       [0.49589917],\n",
       "       [0.49589917],\n",
       "       [0.49589917],\n",
       "       [0.49589917]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only with an increase of the epoch the results improved\n",
    "model.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABYIklEQVR4nO2deZhcVZn/v29V3dp637JvhLDvECCsIsoiKLjgiI4obowjuIzLTx2XUcaZcZlRB1CYuIKCoiIDg4CyBAFJAgmErIQsJCGdTrrT+1b7+f1xzzl17q1bS3e6urpT7+d58qS66tatc7fz7u8hIQQYhmGY6sVX6QEwDMMwlYUFAcMwTJXDgoBhGKbKYUHAMAxT5bAgYBiGqXJYEDAMw1Q5LAgYhmGqHBYEDFMAItpFRG+u9DgYppywIGAYhqlyWBAwzBghohAR/ZCI9sl/PySikPyslYgeIqI+IuohomeIyCc/+yIRtRPRIBFtJaI3VfZIGMYmUOkBMMw05CsAlgE4FYAA8ACArwL4GoDPAdgLoE1uuwyAIKJjANwE4EwhxD4iWgTAP7nDZhhvpqVFQEQ/J6JOIto4QftLE9E6+e/BMX73Ivm9TUT01zzbXExELxLRRiK6k4gC8v2riWi9/P4aIjrf+M6jUqt86NCOTu+vhYhWENEQEd02EfusYv4ewM1CiE4hRBeAbwK4Tn6WBDAbwEIhRFII8YywG3qlAYQAHE9ElhBilxBiR0VGzzAupqUgAPBLAJdP4P5GhRCnyn9XeW1ARLs83msE8GMAVwkhTgDwbo9tfADuBHCtEOJEALsBfFB+/ASAU4QQpwL4MICfGl/9HrKTy0QQg62xfn4C91mtzIF9HRW75XuAfd22A/gLEe0koi8BgBBiO4DPAPgGgE4i+i0RzQHDTAGmpSAQQjwNoMd8j4iOlFr0WumXPXYShvI+AH8UQuyR4+r02KYFQEII8ar8+zEA75LbD4ls+9ca2G4GyM+eADDo3hkRnUFEf5XH+Wciml3KQIUQw0KIZ2ELBObQ2AdgofH3AvkehBCDQojPCSEWA7gKwGdVLEAIcY8Q4nz5XQHgO5M7bIbxZloKgjwsB/BJIcQZsLXeH4/hu2HpmllFRG8fw/eOBtBERE/JifkDHtscBBAgoqXy72sAzFcfEtE7iOgVAH+CbRXkhYgsALcCuEYe588B/NsYxsuMD4uIwuofgN8A+CoRtRFRK4CvA/g1ABDRW4loCRERgH7YLqEMER0jXYQh2MJ4FECmMofDME4Oi2AxEdUCOBfA7+3nD4DtjwURvRPAzR5faxdCXCZfLxRCtBPRYgBPEtEGIcQOIvoRgPPkNnOIaJ18/XshxL/BPn9nAHgTgAiAlUS0ytD+IYQQRHQtgB/ISeAvsCcH9fn9AO4nogsB/CuAQjnrxwA4EcBj8jj9ADrkcX4SwD94fOcFIcSHCuyTKc7Drr//C8AaAOvl378H8C35+igAt8EOFvcC+LEQYgURnQzg2wCOgx1HeA7ADWUeN8OUxGEhCGBbNn3S1+5ACPFHAH8s9GUhRLv8fycRPQXgNAA7hBA3qm2IaJfH/vcC6BZCDAMYJqKnAZwC4FVzIyHESgAXyP1cCtuScI/haSJaTEStQoiDeYZKADYJIc7x+P6tsK0FZgIRQiwq8PGnPLb/AYAfeLy/HsBZEzcyhpk4DgvXkBBiAMBrRPRuACCbU0r5LhE1mTngsC2AzSX+9AMAzieiABFFAZwNYIvHb8yQ/4cAfBHAHfJv5UIAEZ0O24rpLvB7WwG0EdE58jsWEZ1Q4lgZhmE8mZYWARH9BsBFAFqJaC+Af4Gd0nc7EX0VgAXgtwBeLmF3xwH4HyLKwBaM3xZClCQIhBBbiOhR2C6CDICfCiE2yjE+DOCjQoh9AL5ARG+V+79dCPGk3MW7AHyAiJKwfcbvUcFjInoGwLEAauUxfkQI8WciugbALUTUAPv6/RDAplLGKzOf6gEEZSzk0lKPlWGYwxfiNYsZhmGqm8PCNcQwDMOMn2nnGmptbRWLFi2q9DAYhmGmFWvXrj0ohGjz+mzaCYJFixZhzZo1lR4GwzDMtIKIduf7jF1DDMMwVQ4LAoZhmCqHBQHDMEyVw4KAYRimymFBwDAMU+WwIGAYhqlyypo+KlsaDMLutpkSQix1fX4R7H49r8m3/iiE8OoUyjAMw5SJyagjeGOBbpoA8IwQ4q3lHsTW/YP4v5f34cPnH4HmmmC5f45hGGbaUDWuodcODuG2Fduxv58X6GIYhjEptyAQsNduXUtE+RbhOIeIXiaiR/K1VCaiG+QKYmu6urrGNZC6sAUAGIwlx/V9hmGYw5Vyu4bOlyt/zYC9qtYrcr1hxYuwVwcbIqIrAPwv7BWeHAghlsNeihJLly4dV7vUeikIBmKp8XydYRjmsKWsFoGx8lcngPvhWqFJCDEghBiSrx+GvTZsaznGUhe2ZR5bBAzDME7KJgiIqIaI6tRrAJcC2OjaZpaxQtdZcjyFVugaN1lBwBYBwzCMSTldQzNhL8qufuceIcSjRPRxABBC3AHgGgD/SEQp2Ct0XSvKtFKOihEMjLJFwDAMY1I2QSCE2Al7IXf3+3cYr28DcFu5xmASDPgQtnwYjLNFwDAMY1I16aOAbRVwjIBhGMZJlQmCAAZG2SJgGIYxqSpBUB+2MMAWAcMwjIOqEgR14QDXETAMw7ioKkFQzzEChmGYHKpLEEQCXEfAMAzjoqoEQV3Y4joChmEYF9UlCEIBxFMZJFKZSg+FYRhmylBVgqA+wh1IGYZh3FSVIFD9hjhziGEYJktVCYKakC0IhrnNBMMwjKaqBEEwYB9uIs0xAoZhGEVVCYKQ3z7cJAeLGYZhNFUlCCy2CBiGYXKoLkGgLAIWBAzDMJqqEgRBKQi4joBhGCZLdQmCAAEAEumyLILGMAwzLSmrICCiXUS0gYjWEdEaj8+JiG4hou1EtJ6ITi/neIJ+PwC2CBiGYUzKuWax4o1CiIN5PnsLgKPkv7MB3C7/LwuWtAg4RsAwDJOl0q6hqwHcJWxWAWgkotnl+jGOETAMw+RSbkEgAPyFiNYS0Q0en88F8Lrx9175ngMiuoGI1hDRmq6urnEPRqWPskXAMAyTpdyC4HwhxOmwXUA3EtGF49mJEGK5EGKpEGJpW1vbuAejLII4WwQMwzCasgoCIUS7/L8TwP0AznJt0g5gvvH3PPleWQhyHQHDMEwOZRMERFRDRHXqNYBLAWx0bfYggA/I7KFlAPqFEB3lGpPPRwj4iGMEDMMwBuXMGpoJ4H4iUr9zjxDiUSL6OAAIIe4A8DCAKwBsBzAC4ENlHA8Au7qYLQKGYZgsZRMEQoidAE7xeP8O47UAcGO5xuBFMOBDkgvKGIZhNJVOH510LL+Pg8UMwzAGVScIQgF2DTEMw5hUnSCw/BwsZhiGMalCQcAWAcMwjEnVCYJgwMcWAcMwjEHVCQLL7+MVyhiGYQyqThCwRcAwDOOk+gQBxwgYhmEcVJ8gCLBriGEYxqTqBIHlJyRTXFnMMAyjqDpBEAz42SJgGIYxqDpBwAVlDMMwTqpOEIQ4RsAwDOOg6gQBVxYzDMM4qTpBEPT7kGTXEMMwjKbqBIHFriGGYRgH1ScI/PbCNPaaOAzDMEzZBQER+YnoJSJ6yOOz64moi4jWyX8fLfd4QgH7kNkqYBiGsSnnmsWKTwPYAqA+z+f3CiFumoRxALDTRwEgmRYITcbRMwzDTHHKahEQ0TwAVwL4aTl/ZywE/dIi4IAxwzAMgPK7hn4I4P8BKDTrvouI1hPRH4hovtcGRHQDEa0hojVdXV2HNCBLuoY4hZRhGMambIKAiN4KoFMIsbbAZv8HYJEQ4mQAjwG402sjIcRyIcRSIcTStra2QxoXWwQMwzBOymkRnAfgKiLaBeC3AC4mol+bGwghuoUQcfnnTwGcUcbxALC7jwIcLGYYhlGUTRAIIb4shJgnhFgE4FoATwoh3m9uQ0SzjT+vgh1ULitsETAMwziZ9LwZIroZwBohxIMAPkVEVwFIAegBcH25fz9k2YIglkyX+6cYhmGmBZMiCIQQTwF4Sr7+uvH+lwF8eTLGoGiMBgEAfSPJyfxZhmGYKUvVVRa31NiC4OBQvMiWDMMw1UHVCYJmKQh6hhMVHgnDMMzUoOoEQW0ogKDfx4KAYRhGUnWCgIjQXBNENwsChmEYAFUoCADbPcQWAcMwjE1VCoKWWrYIGIZhFNUpCGqC6BnmrCGGYRigSgVBc00IPUNsETAMwwBVKghaaoMYTqS5uphhGAZVKgi4loBhGCZLVQuCbnYPMQzDVKcgqJVrVI4kUhUeCcMwTOWpSkHAaxIwDMNkqUpBEJKCIJ5kQcAwDFOVgoAtAoZhmCzVKQh4lTKGYRhNVQqCkOUHAMRTXEfAMAxTdkFARH4ieomIHvL4LERE9xLRdiJaTUSLyj0egC0ChmEYk8mwCD6N/IvSfwRArxBiCYAfAPjOJIxHr1scZ0HAMAxTXkFARPMAXAngp3k2uRrAnfL1HwC8iYionGMCshYBCwKGYZjyWwQ/BPD/AOSbcecCeB0AhBApAP0AWtwbEdENRLSGiNZ0dXUd8qDYNcQwDJOlbIKAiN4KoFMIsfZQ9yWEWC6EWCqEWNrW1nbIY/P5CEG/jy0ChmEYlNciOA/AVUS0C8BvAVxMRL92bdMOYD4AEFEAQAOA7jKOSRMM+NgiYBiGQRkFgRDiy0KIeUKIRQCuBfCkEOL9rs0eBPBB+foauY0o15hMQgEfEmlOH2UYhglM9g8S0c0A1gghHgTwMwC/IqLtAHpgC4xJIRjwcYsJhmEYTJIgEEI8BeAp+frrxvsxAO+ejDG4CQZ83GKCYRgGVVpZDNiuIbYIGIZhqlgQsEXAMAxjU7WCIBTwc9YQwzAMqlgQ2HUEnDXEMAxTvYKA6wgYhmEAVLEgCAW4sphhGAaoYkHAFgHDMIxN1QqCUMDPFgHDMAxKFARE9GkiqiebnxHRi0R0abkHV06C7BpiGIYBULpF8GEhxACASwE0AbgOwLfLNqpJIBTwIcFZQwzDMCULArVYzBUAfiWE2GS8Ny0JcUEZwzAMgNIFwVoi+gtsQfBnIqpD/sVmpgXKNTRJzU4ZhmGmLKU2nfsIgFMB7BRCjBBRM4APlW1Uk0Ao4IMQQCojYPmntXHDMAxzSJRqEZwDYKsQoo+I3g/gq7CXlZy2BAO8XCXDMAxQuiC4HcAIEZ0C4HMAdgC4q2yjmgR4AXuGYRibUgVBSq4cdjWA24QQPwJQV75hlZ+Q5QdgWwRrd/di2b8/ge2dgxUeFcMwzORTqiAYJKIvw04b/RMR+QBY5RtW+VEWwXAihXfd/hz2D8SwpYMFAcMw1UepguA9AOKw6wn2A5gH4HuFvkBEYSJ6noheJqJNRPRNj22uJ6IuIlon/310zEcwTkKWfejPv9aj3+P8IYZhqpGSsoaEEPuJ6G4AZxLRWwE8L4QoFiOIA7hYCDFERBaAZ4noESHEKtd29wohbhr70A8NZRHs74/p9+JJLjBjGKb6KLXFxN8BeB72+sJ/B2A1EV1T6DvCZkj+acl/U0bpVllDXUNx/R4XmDEMU42UWkfwFQBnCiE6AYCI2gA8DuAPhb5ERH4AawEsAfAjIcRqj83eRUQXAngVwD8JIV732M8NAG4AgAULFpQ45MLUhuxDb+8d1e/xGsYMw1QjpcYIfEoISLpL+a4QIi2EOBV2TOEsIjrRtcn/AVgkhDgZwGMA7syzn+VCiKVCiKVtbW0lDrkwTTVBAMBrB4f1e2wRMAxTjZQqCB4loj/L4O71AP4E4OFSf0QI0QdgBYDLXe93CyGUb+anAM4odZ+HSosUBHt7R1AnrQO2CBiGqUZKEgRCiC8AWA7gZPlvuRDii4W+Q0RtRNQoX0cAXALgFdc2s40/rwKwpeSRHyL1YQsBHyEjbOsg4CNew5hhmKqk1BgBhBD3AbhvDPueDeBOGSfwAfidEOIhIroZwBohxIMAPkVEVwFIAegBcP0Y9n9I+HyEppogugbjaIhYODgU53YTDMNUJQUFARENwjvTh2AnBtXn+64QYj2A0zze/7rx+ssAvlzyaCeYFkMQ8BrGDMNUKwUFgRBiWreRKEazjBPYgsDPFgHDMFVJ1a5ZDBiCIGrJ9Qk4RsAwTPVR1YKgxWERsGuIYZjqpKoFQXNNCIAtCIIBH7uGGIapSqpbENSyRcAwDFPVgqCFg8UMwzDVLQgWtkQBAPObohwsZhimaim5oOxw5IQ5Dfjbly7G3MYIu4YYhqlaqtoiAIC5jREAmLBg8fOv9WB393DxDRmGYaYIVS8IFKGAf0Isgn+6dx1uf2rHBIyIYRhmcmBBIAlZpbuG7lq5y9G+2mQkkWIXE8Mw0woWBJKgv7RgcTKdwdcf2IT7X2r3/DyeyiCVmTILsTEMwxSFBYGkVItAbZNPaCRSGaQzbBEwDDN9YEEgCfntYLEQhbV5tcC9V2A5lbatgWSaLQKGYaYPLAgkIcsPoPhylepzL+tBfZZm1xDDMNMIFgSSUMA+FcVSSNVyll7LWqrvcoyAYZjpBAsCSVAKgmJxgkIxAvUZxwgYhplOlE0QEFGYiJ4nopeJaBMRfdNjmxAR3UtE24loNREtKtd4ihEqURAkUgVcQ8oi4BgBc5gRS6YxFE9V5LcTqQxuvPtFbO8crMjvF6N/JIkHX95X6WEcEuW0COIALhZCnALgVACXE9Ey1zYfAdArhFgC4AcAvlPG8RQkWKprSFoCXoJAfcauIeZw418f2oyP3vlCRX57f38Mf9rQgedf663I7xfjgZfb8anfvITe4USlhzJuyiYIhM2Q/NOS/9wz5NUA7pSv/wDgTURE5RpTIUIBO1hcrJZAWwTJ3O1iSY4RMIcnBwbi2N8fq8hvJ9IqU29qNoUcjNmW0nQuJC1rjICI/ES0DkAngMeEEKtdm8wF8DoACCFSAPoBtHjs5wYiWkNEa7q6usoyVuUa+uy9Lxe84dXF9souymYNTd8bgpneZDKiaAr0eEhnMhVr055I2cdTLKOvUsSkUpicouMrhbIKAiFEWghxKoB5AM4iohPHuZ/lQoilQoilbW1tEzpGhXINbe4YwK9W7cq7nQ4We2QNqfc4RsBUglgyjTO+9Rj+vOnAhO87lREVm4jV707V9UJGEywISkII0QdgBYDLXR+1A5gPAEQUANAAoHsyxuRGuYYAwFfAO5WNEeSaqeqGZdcQUwmG4in0jiTxes/IhO87lRYVc32oCTYxRRWs0eT0jw2WM2uojYga5esIgEsAvOLa7EEAH5SvrwHwpCiHXVsCZhHYvr7iriHPYLG8IbigjKkE2Qlz4ifsdKaCgiA1xS0Cdg0VZDaAFUS0HsALsGMEDxHRzUR0ldzmZwBaiGg7gM8C+FIZx1OQ4+fU48xFTagJ+rGvbzTvdoXSR+O6oKxyN0Q6I/DstoMV+32mciiXZDlckykZI6iEnhbP4xp6YVcP+keTkz4eNypGMJ1dwuXMGlovhDhNCHGyEOJEIcTN8v2vCyEelK9jQoh3CyGWCCHOEkLsLNd4itEQsfD7j5+Li4+biX39tiAYjCVzbvx4gawhdaOmK3hDPP1qF97/s9XYun9q5lwz5SOZLp8iotweleijpS2CdPaZ27SvH+++YyX+6y9bJ308blSMoJIK4KHClcUu5jSG0dEXw6Z9/TjpG3/JKRQpzSKonCDoHUk4/meqh3JO1krbrUTAWB2PaREsf9rWGaeCXz7rGqr8WMYLCwIXcxsjSKQz+I+H7XDG9s4hx+cqSJxI55rJiSlQUDYitZORRGWqQCcDIQRe2T9Q6WFMOdREWQ5ftYp7VcJPn60jsH87lkzjofUdAOx1RCqNtghYEBw+zGmw1zB+drvtZ68LBxyfK61fiFwNQFsEFQwaKQEwHJ+axTcTwXM7unH5D5/JEdLVjlJAynH/JTOF1+EoJ0lXHUE8mdGCabhCbS9MtEXArqHDhwUtUcffownnxTU1IvdDkW06V3mLYCo8IOXi4FAcANA5WJlK16mKEgDJMtx/lbUIVLA41z2l7vdKogXBFM1qKgUWBC6OnlmHn35gKdZ89c2w/KQvssKc/N1xgqnQhlqZqcNT4AEpF/oYD2OrJ55K494X9iAzhnspmS6fRZDy8NNPFgkdLM51fQ1PAReoUhanQrxivLAg8ODNx89Ea20IYcuvU8MUTovA+VBMhaZzOkZwGFsESjgPxSufOlgunn71IL543wZs2ld6LCSpJ8rypI8Clemno+sjUrn5+iNTQBngFhOHOREPQWA+CO4UUtM1VKG6OK0hHdYWgRYExY8xkxH4/mOv4sDA9HIjDcj8+LG0flaTdVmDxRWY7BKugrKpYBGMJFLYtK8fQohsZTEHiw9PSrEI1u7uRddgPOezSsUJRqsgaygmj3EoVvwY9/aO4pYntuHxLRPff6ecqOvnvv8KkSxrQZm9T68eW+XGXTGtjjPo9+n7fbK5Z/UevOPHz2EkkdbPOtcRHKZELD+G4il89M4XcPVtz2Lt7l6HRZBIZXD9L57HT5+1c5rNzyrlHlKuoWKapBACD6xrr9hiI4eC0sBKCYgPSvdRJSawQ0FZO+4YVSF0ZXE5CsoqWEeQcMUnlGBoiFoVswgODiWQSGUc9TpcR3CYErZ82N09gse3dOLlvf14eEOHI1g8kkhjMJZC37A92Uwpi6CI22Tljm58+rfr8J9/rnxl5ljJuoaKTwIqoDzdesUri2AsGm85m7Mp4VLRYLFLEDRGrIrFCJQS0jeSjVNVMm38UGFBUICw5depigCwsb0fiVRGt6zuH7W1Aa11GkKiUv7CbIyg8CS5vcvOwZ9uEySQzdIYLME1pALKU7VhWT6Gx2ER6BYTh1lBmTsIrtJIGyK2RVCJeJwSBGavI84aOkyJBP3olRJ/QXMUm/cNIJbMoD5sAYD+zGuFomLm+aMbO/C7F16f8DFnYwSFJ5D2Xruf0tzG8ISPoRg/f/Y1XPcz9xpFpTOaVEVzJbiG9LUpn+b4+OYDuGf1ngndpzq2scQIsgVl5YsRmP1+JgslCOJu11DEQkZURpkZ9BAE7Bo6TAkbaxScfUQzBuMp7OwaQr2sNlb+QS9BUMw19PFfv4j/d9/6iR5yyQVle2WH1bDlz7vNSCJV8Dju+OsO/EmW+o+Fl/f24eXX+8b8PcVoiXEQc5tyThYfvWsN/vn+DRO6z+FxuIayBWUTe6x2Fpz9eqIsggMDMXzyNy/lJDWo2JWZGZR1DTnTNBuitkJWieJJL4ugWLZW/2gSN97z4pRc25gFQQEiwewkuWyxvYLmcCKNuoh9Ayr/oNdkU6qZONFZDyMluoZUq+18wT8hBC763lO4e/XuvPu4e/VuPLCufcxjHBhNInYIE8pYYgRDk2ARKCbyAdf1IGOwCFRsYKLTR03rdqIE6gu7evB/L+/Djs5hx/sb2vvx6d+uw7PbDiKTEfjVyl3okQqXO2uoMRIEUJnqYk/XUJHzvmFvP/60vgPr9vaVc2jjggVBAcJW9vScvrAJAZ+9cpmyCPq0RZDrhy7VPN95cGL75YyUGCxWriHVx8VNPJVB52Ac7X2j+MTda/HQ+n0524wmMuN6CAdiKUdv+7tX78a1y1eW/P1RmQHkJQg27xtw+Iy1kJ6ErKFtE9j7SI17PBbBRLuGTKtwoiyCWDK3tTSQFdwDsSQe3bQfX3tgE57a2uX4bR0slhZBJQTBkEewuFhrD/WdqVAE54YFQQFMt0lzNIijZ9YBgI4RaIvAQ+ssNYVvR9dw8Y1KJJHKaEukkEUwmkijU9Y+5NMe1cOVSGXw+OZOrNnVm7NNLJkeV/qe0qKUdvmV+zdi1c6ekoN++eoItncO4YpbnsHKHdnVTpXbrpxpj1FpOW7rHP8aEB39o/j3h7fo6zGeOoJsG+qJPVbT9z1RFoE6roRLEYmlsu1DNrb3Oz7LCFvYuQVBOVJIOwdjePVA/uupgvljsQiUFTEV2mK4YUFQgIghCGpCfpwwpx5AtiOpEgTDsqgknswgJDOKTC3qxntexDce3OT5GxPZQVNpjw0RCzGjQ6MbtfAOYN/IS7/1OJ7a2unYZthwdyWMh88klkyPyz+rqmZjybTDR+w1yQghcibDfHUEPdI1c9Bw0QxPgkXQKF2F2w6M/1p+79GtWP70Tjz5in0dRg4ha2iig5blsQiy7dyd72cF4U4PJcm8FxvkeVfnKpZMT1ja9i1PbMMNd63J+7m6rwbGECxWAmAqtn8p55rF84loBRFtJqJNRPRpj20uIqJ+Ilon/329XOMZD8oiiFh+BPw+nDi3AUBWEJjFJEPxFOKpjNYOzRjB1v2DjgnfnNh2yDTOWDJ9yMvujchsmtZa5Tv1vuHM3z8wEMPBoXjOJOa+ad2CIJm2rY/heBo/WrEdv32+9KyZgZgSBBmsfq1Hv+8lCJ7Y0onT//Ux/R3AiBHI1MG+kQSu/8XzetH2mOEqyMZvymeOq/GY1/jRjR24+f82l7wPFfh87aA9+Y3PNVS86dzBofiYJ3PTui1mWe3vj+HeF/YUtWR0fx7XWMyGgls9NPJkSuhYiBIE6l499muP4p/uXVfwd0ulfzSVNz1ZCIEh+Zt9MoWcqLgXYEhbBOO7F1fv7C5bJXU5LYIUgM8JIY4HsAzAjUR0vMd2zwghTpX/bi7jeMaMsghq5cR/4lzbIogGAwj6feg2NM/BWBJD8SSaovYkbGomI/GU4+Ezb7Dd3faD/91Htx5SSiWQNVdba0P6b68FXGKGdqweIre/Xe1LVbi6Jw81+Y0kUrhv7V78edP+ksYYT6X178eSaazZ1eP4zM2u7mGMJNLoHsqeazXRC2G7sLZ0DOKprV14XgoVU4uejKwh5UYz17p+ZON+/H5N6enBsxvsNF51P6h9jibT6ByI4e0/+hv29xful6QtggJa8dJvPY4LvvukpxtuxSud+hwqVu/sxhNbstZiMcvqrpW78MX7NuCdP36uoHaejRG4LAJ5D+wfGNVC0SSeTmtBpy0CQ+Fxryg4XuLJdF6hN5JI6ywq5RWoDQWKxmaUFTGe9i89wwlc/4sX8K0/la5cjIVyrlncIYR4Ub4eBLAFwNxy/V45UMHiupAtCI6bXY/aUAAz6kNoiFraFQHYFyqWzGi/paNDYjKtF+AGssFlIPvAHxiIoXMgW7w2HpS20FpnC4LlT+/E5T98Bs9tdy5mb064ys/uvjlHtIBIyuNx+XINzW0glix5ojWFYDyVcf7tMcmMutJhhRAYSabRJM/zUDylz7USzGbw0Cu114ut+wfRbwT+Xtk/gM4SGtWlM0Lv29SCuwbjGE6k0DeSKGn9aGVB7u4egRBCC+hYMo1X9g9i3et92LSvv9AuSm5DfWAgrlf4Mvn2I6/gx09td7z3nuWr8OU/ZlNji9URqGdic8eA4/lwk69jpxIQmzu8z1kiZcYIbKVrOD5xLiFFPOXtDrV/L3vPKiu+LhQo7hqKl1bjI4TAfz++TSsFAHDnc7swmkzj+nMXlTL8MTMpMQIiWgTgNABeKu85RPQyET1CRCfk+f4NRLSGiNZ0dXWVc6gOwi6LIBoM4KkvXIT3LJ2v/cKKfX32pNFc42URpD0tgqaopSe/eCq/BlKMWDKNFVs79eQ9q97WLn/+t9cAAPtdE5o5KSqN2d3JczjutBTcYxs1fLy9I6ULAtP9FUumHaaul0WgzGj18CTTAumMQJsUdoOxrCBQrjovi8DLHSKEwKIv/Qm3PrEN7/vJKix/Zoce4zW3r8TH7lpTNIBt/lY8lUEmI5BMZ9A1GEdGAD98fBve+5NVBfdhjm939whGk1mNcySR1r9RzHWY7T7qPWbzWJ7ddjDn877RRFHXQzG3klmJb752ozR/932jBIRpXbl/P5s+aj+Do4n0hFc8x1PpvOdxyCUIwpYPwYDPITg6B2MOxcL8XrGsoe7hBH7w+Kt4ZKNtZSfTGdy5chcuOX4mjpIJKxNN2QUBEdUCuA/AZ4QQbj/FiwAWCiFOAXArgP/12ocQYrkQYqkQYmlbW1tZx2uiBEFNMLtcZWttCAG/T2v+CnXjKkGQMkryE+mMLoYBsoKgrS6kb/x4KjPuFY7+vGk/PvSLF/Cq9FFffOwMzGuK6M9NtwrgbJ+ttJTBWBKfvXcdNuztd7yv/s+nuQFKKy7NdzngFgTGWGIeFoG7LkJtP1MKuwMDMT02pYGamnmhOoID0gK746870D+a1Gb+r1ftxlA8hZf39ntqzo7xyYc7FPAhnsrghl+txVFfeURnZe3qHkbvSKKoQFET2b7+URwczF6v0WRaH89AEUGQLFJH4GiY6LFNfwn1HcUFQUJb0qorrxfqWufeV2m5H/u7yv2j1ia2nyX7O/URC0T2vXsofb5GE+kc12g8ZSdbeO3LXBBpMJbSMUQliJ/a2omz/u0JnP0fj2OdUThZatZQ1kq3f+elPX3oG0niXaeXz6FSVkFARBZsIXC3EOKP7s+FEANCiCH5+mEAFhG1lnNMY8EdIzBRZqlCCQJ3jEBpWAkP11BbXUg/nPFUxuE+GgtKsHTIMbTUBvHda07GDKk1u3vxq98M+n36AdjdPYI/vtSOp7fZFpeagNW+3Q9szsptRXzHg7EkugbjGDBcQbFUxjFpe1kVuudOIpsZAgBvOLoNtaEA7lu7VwcPu+XkYbq5zOyn1w4OOx5s5YOe1RBGKiMQS9q1DXet3IULjmrFkW01+OVzuwoel3pYm2uCiCXTut210t7398cgSmiDoCYyIaDTFgM+QiyRFQT9o4UnEN1rKM9EaF6j3GVW7diNe42NnHEWuUe7h+M4ZpYdSytkEWiLMo9FoOTm3EZboakJ+fX2yXQGAR/B7yM0RYPoGUkgbrisCgkgL6667Vmc+C9/drwXzyOogGxvMUXE8iPgIy2IX369H0S2B+HfH96ilQBtERSxutQzp5SMZ7Z1we8jnHNk+abGcmYNEYCfAdgihPh+nm1mye1ARGfJ8XR7bVsJVGWxihGYKLNU1pjplMwmaRGoG0hJfy/XUGtt1iJQN/h4GmipSUZpuDXBAM49shXPf+XNWNgS1dqp3l7e5LXhgB6fEmTuzIbhPK4Vtwuh2ET37UdewYd/+UJBi8BLa9cWgSuLpqU2iHecNhcPbejQvnwlZFRTukwmm93RPZTAG//zKXz2d+sAAJv29etAunIzxVNp7OgawoGBOK48aTbefupcrN3dWzBIqx7qpmjQcwJWbrliWTTmBNvRnxXoo4b7zMyc8kLFBtIZ4bnEZcxcZtUluPuNlF6FlzZcTOB3DyVw3CzbfeEWBCte6cQF333SzrDLEyNwKxhzm5QgsJ9B9ZxY0kJokrE68/4006NLQRUCmlaBOldegs+9RGo46Ifl9+nzv6NrCHMbI/jMm4/C86/1YL22skvrkaUEjaoqf3rbQZw6v1FbR+WgnBbBeQCuA3CxkR56BRF9nIg+Lre5BsBGInoZwC0ArhWVWtrLA2XielsE9kVpkRk67SpG4LIIRjwEgXqg22pDSGUEUukM4qkMhBhfB0M1garF3GsNwTWzLpxjEaibvC4c0JqXCrS6b9ZsLEDgpT29etLNXbmt8ESn0lTNySyeymA0kdaC1tMicMUI1Hgilh+XHD8TiVQGm13LOaqxjRi+dvWQP7BuH0YTaVx5y7P4pkzvVAWCsWQGK3faWTPLFrfgLSfNBmCnguZDNcBrqvF+SJW7qVg9gHl/dEjB01obsgWBnHyLxQjMbCGvfkPmNUukM9jY3q8VAOXPNt1zXq6oQhbBSCKFkUQaC1qiCAV8OZr5h375Al7vGcXu7uFs1lCORZD92/KTFtLqno4l7RiB5bc1sJaaUI4g6Ogb32p0L+3JFk1qi8DTSnVO5LZriPSzu71zCEe21eLyE2YBgM7EKjVYPGRYBIOxJNbv7cN5S8rrKCln1tCzQggSQpxspIc+LIS4Qwhxh9zmNiHECUKIU4QQy4QQz5VrPONBB4u9LAI54TdHg/D7KOsacsUIzApdhbIIlBCJpTJ6InVrSOmMKKoJqptWPXg1xnjb6kN49cAg/vHXa/Xn2iLwOC51E7q1nmQqg3f8+Dmc9e9P2GP2WKZzY3s/ntuRG4QE7PPgrpWIyUlO5dB7Zw15xwjCll9bbG7/rq4zkMdSE3Q21vvThg7P7eOpNFbt6MbshjAWtkSxZEYt5jdH8IJHVbVCnSflEmyKeguEYg+/lyBoqQ0hlszoc1AoRvDwhg5HcNIrldGcZOPJDN5667M499tPAjAsAkOgewmeQpafikW11obQWhvCQSM21WfU3IwY7q58riHAFtCqZkfd0+//2WrcvXq3bgXfVCMtAg+LqlSUwvfCaz2Ip+wiR3WcXoLPfb9FLD8snx0szmQEdh4cwpIZtZhRb99HL8gU6aESYgSPbOjQ1384kcb6vf0QAli6sGlMxzRWuLK4AO6sIRNlEUSCfjRFLT3JNkvNUFkEaqJwxghSqAn6td8znsxmPbgfjJvueREnf+MvBcepbtrOwTiCAZ9+SADbIugdSeKRjfvxJ9kvSAkdL0EwqP2YzpvVFFDdQ3HPGMGnfvsS3veT1fj+Y6/m7Hc4kUY8lcGA4eeOy0CoOpeeWUPuGEEiaxGoAKL7wRpxFfvMqHe22v6Nq/hNp5gmM3hxTy/OOqIZ0mOJ5ppQQUFsuoYAwCe/56ZYNk48ndE9rFQfqDapKPTJCVlNzIlUBj9/9jUda+ociOETd7+IZ400YW9B4O2GE0J4uobcgsDyU8FgsXIFtdYG0VYXclgEZn7/UCxluF5cacnG7zdELG0thox72rYI7L+ba0LoGXYGi4vVWwC2BdTeNwohhD5XL+zqxVW3/g3Hf/3PWcXMoxeXsghU77FIUFoEaYF9/aOIJTM4sq0WALB0YTPW7O51pAR7ZQ3t7R3BgYEY/vHuF3HXyl0A7HtGWSmnzG8sekyHAguCAqhsobpwrpanOh9Gg34d0CLKuhlSLtdQMp312w7GkqgLW/rmti0Cbw1EpZAlUhls7xzEe5evwjt//DdHrrh6eHqGEzmT+4z6kH5dH1ETbgY+yvbIMdEWgWviMsf10PqOXEGQSgPymXG3qwBszT6WtGsOlKCytd20Ppde2qY6fy/s6sHbbn0WXXKyiQYDej/u1FflSlFZRKpYS2G6AOzvZzOL+keTegIGgNqQv6BPN+saso8hn/umaIwgldH1HztlEFtlfqnjUDGQ257chpsf2oz/fandMX6TQq6hunDAca7b+0YNQZCNU7mPJRoMFBQEuRaBfa2EEPj1qt36fhuIJUtyDdVHLESDucoKAAT8SlBb6B1JOI4nX+XuQ+v36ZqaT9/7Es779pP4/O/X62d1W+egrmYuZBFs7xxCXTigffYqayiZEbp32JFtNQCAMxc1oWc4gdcODucUlPWPJvHWW5/BczsO4qLvPYU/rN0LANgjK+SHEym8tKcPS2bUljU+ALAgKMjM+hC+9fYTcaX0FZsoLTYa9GNeUxSAHVRWmkpKB4udflnAfhDqwgFtccTyWARmQclwPIWHN+zHyp3deHFPn27NCzgnUGVlKFpqgo59qN8LBfwOy0ExlCegZbo2XtrTm6PhZkR2ojFbb3z/sVfx2d+tw3A8jYywb341pnjKDhZr15DsSPrQ+n34/l+2Si3K3ueqnT3Y0N6PzR12PCASzI7fPVZlNagJdJYhCBa31SAjbG3u2+88Ca21QUc8ZDSZdgjImmAgx01moj5rjjoVALNzrdp3IRKpDOpCAYQCPhwcisNHwBy5aJA6nwOjSaQzAj95xq4PUVaLV9qtZ28oeZ/YvajM69nnmPTV/eQWBDVBv8N11DUYdxS5qYm/pTaEtrqg/nvN7l68emAIn7joSAC2Babun3zpo2qcIXke3ZFD0yJIZ4TD+vASuolUBjfd8xLe91O7lElljKn4zxGtNQ5XlrvTqSKTEVixtRNvOLpNK3KRoB9BPyGVzmBvrz2Jz2+O6v0CdhpxMi3gI1WZLLCjawgb2wfw5JZOpDJCFx6q7KOReBovvd6H08psDQAsCApCRHj/soW6NsBEawPBAOY125pbXdiCX5qLw4k0kumMo8GUesCG4inUhgMIBZRryIwRZO94s7R/KJ5yuEAcLhbj4axxaVCmNTOoNd8MwpZPP0wm+TIbzGrog0MJT+1duTDUGs4AsGFvH9bs6s0WRY3YQtBH0iJIpnUGVjyZxh9fbMdN97yEW57cjr6RZE6Drr09ttukIWJlXUNuoSW19F4Pi+BU+VDNbgzj2rMW4Mi2Wi38BkZTEMLOAlHUhgIF1z1QE1qTcY+cv6QVP/nAUs/t8qGWQFX3VXNNUGvDPfJ8Dowm8fLePn0ulUvLS8gUcg01RCzHMak8dYWK1fS5BMGM+rCjJuU9y1fiylue1ZauSjhoqQmirTaE7uEEUulsMP/qU+fqcat7Nl/rEjVO9Yy4M5jUtVdKRYfhDvISBGYrE8BYwEn+f8q8hpzvALmC4OW9fTg4lMCbj5upFRE7fdSOEfTI89Mi+32p//d02wKiVSaIJNIZLWgPSCHmjm0cGIyhZzihux6XExYE40Q9+FEraxHURyw9uX7tfzfiPx5+xWkRyJt+JJFGTTCgtZ1YymkRxJJpbU4qTC0KgKcGB+T6/S89fiaWX3cG/D7KFlclM7ZF4CEIsjEC58Nkap0Hh+IYTaThI9sdplDfGTTaPsSSGQzGktnFvkcTiFh+hC0/RmRFaKNhEaw13DZD8VTOwiyvS42rPhLQGpm7HbVKH1UT08z6XEEwr9G+ZsGAT49Nad5RR9dZO8V25Y5urPdYUEQds1lXcuysOlxwVJuje21RiyBtCwLlvmupCenvK4E2GE9hu9EcUAlnr9z/fN1iAXuCNe+fXd3DziB+yruAbV5TBP2jSe3aUN1Bd0nLtb1vFI1RC2HLj7lNEQhhv6fOUWttyL4P41nXUFGLQF7jltog3r9sgb7fdPqofA5VZpzlJ89z/bhUquY2RuyYyEjSce/m88G7BdVfNh+AjyAtAvv6hK1sjKB7OIE6Q8lrrrHdfXukAqNctSPxtA7uq7Hvc2U7KeHc5KGITjQsCMZJYyQbLFa+3PpwQFsEALCnZ9ih0SrX0GgijUjQr5fCHIqloBSeZDqDW5/chnf8+G8OLXxYpuYpHGmYxo1f4xIEPh/h0hNmoS6c1WxjqTRCeSyCbIzAWwv2kW0RjCbTiFh+x6SpzgFgrjmQxkAsm4XRN5JEWAoClUlSH7YrROOpDHYZwq9nOJHjEtjbO4qw5XO4ttxj1S6q4QQaIpa2kmqCfq1dqfz0oN+nz70ao+mXrgkFMBxP4b0/WYWrbvtbzvkYSaYQDPgcmUleSQalxAiCfp8+f801Qf19Mx9/3d4+BHyEunBAWwQxjyC7VxqyKQhMy7NrMJ5T3wHkuoaUwqO0byXAN0qNf0/3CBZKl8iSGfZ53nZgyMj08qE2ZI9bV9R7VKyr69oQsfTrZFrgW28/CSfOsTX3bPqo0yKoD1ue5/pvMjaQytiLKSXSGRwrC98A4OQ8FoEZI8hkBB54qR0XHt2Gpppg1iKQdQTJTAbdwwmHO7YxYsFHWb//jDpbKRlOpLIWgRQE7jRvRb5MtImEBcE4iQb9aK0NYU5jGPNVjCBs6UwCwPYfmxqt0i6UH1r5kd259Ts6h7G3d9TxIA5Ji0BpMQP5LAKPDCdAujgMiyAc8MMK5Ga4jCbt7o75AqRzmyLoGY5jOJ5CJOhH1CV45sjAuZrk3esi9I8kEQn6EQr4dJwjKv+OJ9PYdXBYTzBelak9cnIHoB9E95w3IttTdw8n0FwT1JZXXdjS2RxKeHvFSZyuIb9j0nQXao3E1bU0BYGsPzHOTSmuIctvWAS1QX0ezEl93Z4+LGiJoikaxGAsiTW7ehzBcnX7eQV1lRZuBh4DPrL74jgEgYwRuHrlzJcu0P1GnQMAbJILyOzuGcaCFtsnvmSGfZ63dQ4hJpUGIluA9Y4ks4voeKSPqom0PhLQbln1noqB5VgEckwNEUsnCygyGYHXpNUyMJrSLq9jZ2VdLrMaIphpJFYozGu/6rVu7OuP4Z2nzwMAl2tIWgRDcZ0WDtiKWFM0qFukq0SEkURaa/xKAOSrIXJ3MSgHLAjGCRHh8c9eiOvPPSJrEUQC8Puzk+tIMu20CAzXUMTya/PR9Pcn0xl0DcWRzgh09Md0mwgVI5gt3RwOQWDc+LV5sixMX3e8gEUASAGWJ0A6rzGKjLA1sLDlz3FFZQVB1iIwGYyntGtIbROW56J/NIl9/TG9AFC+VgEqM8vLtQXYgsFuhicFgTzPdeEA2upCuPW9p+F9Zy+w9+EhCNyuIZPXjAA+YF/LqOV3pDfqHlVG4N5UCDa29+PrD2x0CBXtGgor11DQMWErBWBzxwAWt9agPhLApn0DuOaOlXjQWDdauZMKWQT1xn4XNEdxcCjhSj7wtgjmuywCdW9v2jeAZDqDfX0xbRE0RCzMqg9jW+egtoABWxh3DWY139yFadLar94QsbB0YRP+892n4GtvszvYK+tOB4vlJNkxYLte6iKWYz0KwPa1J1IZzKoPYzSZxkF5X5mCoDFiYZEUYiamoHps8wGELR8uOW4mgOz9Fw3KrKG0QPdQIiem2FIb1EvSKmE6HE/p9GavYL8JWwRTnMaobR6GLT9OmdeA42bVw/JlT+lIPOUZI9CuIQ+LQHWuBGxzUk2sQ3HbNTRTBj6dMYL8riGFwzUkV1JzT6TK3B6MJzGcSOVkvgBZl8rrvSOIWH588JyFeKfRDEsFZns9KlUVETlxKqtBWQiq1P/42U5B4C4IUxNkwO9zuOKA7IQZS2TQPZRAUzRrwqsJ8G2nzNEmupcwdGQNuc7nJlcV82gyJcef6xoyA/fm5PSHtXtx18rdOk0UyA0Wt9SGdFotkO0oC9iZKHUhSy9qZPqWI/I3vVpRe1kE85ujSGcEtncO6QlHbbeja0jfE0D22u+XQU2lWW/a14/23lGkMwILW6J6+6Nm1mJ755B2IwL2fWgKeDNGIIRALJXR6dgz6sIgIlxzxjytcKjrocalssdUo76GiJXjKtstA7VqPRHlpjlGCoKAjxAN+vGBcxbhgqOcFbymoNrUPoAT5jRooabuq7Dlh+UnpKRrSC0MpWiuCWrL4nip5IwkSl+IqoktgunDAzedj49duNgxMY0knEsxJtJ22ljWNaQsguwNkUhlBcFIIq1TCIfjKZlzbyFs+ZzN20yLwJU+mn3faRHYN6/z8iuh83rPKDICmN0QydmPekj39o4iEvTj+vOOwDtOm5uzDxV49couCkmLQAmLiOVHyPLp9LkTpB9Y1QyoNgMKcyJzCzPdmjiZRu+I7a9V2nqdh9usuGvIJQhc6+ju6BzGzPqwQ2h6VaSrLqKv94xgi0yB3dDepz+Pp2zhXB+xv9NSG7TbG8vjW7a4RU8w85ujqAsHtEusezg7sUaCWZ/6vS/s0S4JwI4lWH5yBLEXSA1+MJbSK/DFkmls7xzEts4hR+p0TdB21XT0x2QFru3G6R1J4qXX7SD/QkOrXjKjFtsODGHEUCrqwwHPNE015nRG4MQ5Dbj3hmV4w9G5nYbdriF1nlUcoj4c0G64vb0jGIgldcaOOj4lCGY3RFAfDqAxaoGIcOXJs/HJi49y/J4SVJmMwOaOAW2tAtkiN5U1lEhl5D3nvF/V3621IRwlYyd7ekZyXG+O45T3oI+cFly5YEEwwThiBImUIwddrf+bzghEg9msF9Mi6B1JOrIeZtXbE+tgLIWRRArRUAD1YctxE5ViEdSGLaMlsz3pWIZpCwBz5MSvFmE3tTuF0goTqYye8ExtWFkEf1i7Fw9v6PDMaLFdQz6tESk3mRJUx7tcQ621zgfLfDDcE7nyGY8kUugZTqDJIQhyHygv91Ihi+DJVzp1zGN75xC2HhjEpcfPdJyDiHYNqYJEe6L6xoObcMF3V+hF2V9+PStUEqm0DBZnXUNEpGssakJ+3P3RZTh2Vh3OX9LqOAfmAjDqt3uGE/jifRvwO2OVtFgyjXDAr2MmQFYQAHAIgoc37AcRcNWpc/TnAT9hVn0Y+/uzvfZPl60PVKqzec8saI5iNJlGe19Ma9G1oYBDEzYri5UmHwn6cfbiFvh8uTEs7Royrrt5vcwaiff8zyr83R0rsWX/AAI+0q4glevfGLUwqyHsOJfuIku9TkTPCIbiKR2sBuAKFhNG5NrlbteQ+vuI1ijmNkZQGwpgS8dAQYtAZbo1RKwcq7ccsCCYYMybdySedmhDCdlkDYD2kwPO9sKqvYCiMWqhJujXrqGo5UdDxMoJMCvyB4v9OjVUFZSpYPHcxgguP2EWLjvB9n2qNshePtN5jVkrIaIFQfY2aqsLIeAjPP9aDz5x94ueFkEk6HO6UoJZH3tN0K+FiRIEBS0ClyBQPuOuwTiSaSEtAvu36j3OTcgzRpDdzrSwPnDOQmzrHMJ9sgJUFSNdfuJsx+SqrndNKICwZU/uo4kMVu3sBpDNXd9gWBc56aNS+JnVq8fMqsOjn7kQi9tqHdaNGdBU10SldJpCIpbMIGQ53VjzDUFwkhIEqQwe3tCBpQubdFwAsJWc2Q1h7OuPabfQGVIQ/HVrF2qCfh3TArKC9+Bg3HANOeMesUQaj20+gExGaPdZyPK2agHo5ART4TItr7qwhVgyg86BGNr7RvHK/kH88rldmNsU0UFX5SpqiFg4bnY9FrfWZvfvEgTq3KrCueMNi8ARLDZcaC0eriHAfp58UiBt6RjIqdMAsp2OVZrpZLiFABYEZUUVjaiLmUhldApo1Jj8TNeQe2Wm+nDAbhetBEHQj3pXHnihOgLzfbM3f8hwO0SDftxx3Rk4V3Y4VAvZm9qiYkZ9KOufVYLAcmpnKVcQ1I2yCMy/1bmYWR/W6ZsHXa4hNSmaE7pbo1cWgWpF3Bi1HFlDbrxdQ9n3TIvgvWctwLGz6rSWvWpnD06YU49ZDWHPYPEVJ83CR89fjEjQj1gy7RDSp85vxKZ9/dqXr2IER8+sRTTo10K40RAEJl7HYqLScJ2FYmmZeptrEfgNjfmVjgG8sn8QbzlxtkNo+H2E5pog+kYSurbh+Nn1CAV8GIynsGxxi652BrL3YtdQXMcuTAFWGwpg64FBfOyuNbjlyW3axRn2uCbZ78hguCH81DUKBnyIWH4k0hnd+vnKk2ZDCNtlpaytPT0jWhH77jUn40d/f1rOvhRqQaktHbZVYRZ3hRwxguyYc1xDUjAsklXGx82uxysdg45ro1gysxbNNUFd1+BeAKtcsCAoM/v6YnoiS6Qz2TbKMtMg4COHdt/uFgQRy869ljGCSDCQYxGYedPuymJFbcjSpqvtGsrevOphVw/utk47SKjiEyahgF/HDtTE7HSLeP++SdhypluaGVRtdXbRUdjy6fUVlJapyvVNU96t0bfoAqPsCldKWHjFCLyDxUYdgfG6KRrEaQsadaHfwaG4jokQkSN4CAAXHNWGz192DCKWHyOJFHYftDXRoN+Hy06YhVjSXuYzlc4gI4Cg348zFjZj882X5wg/t5bsZd0AQFoWXiiLwGz3EZOxIfOcNUbtDp+LWqJauNwvexhdfuIsh5C3/D7UhAJ26qNURJprglgsU3IvdPn01flOpDKIeAjjeuP1rU9u1xXhkWABiyCoWpbnukNDAZ9WMNbstmMW337XSfjCZcfgA8sW6vjL3t5RPcGGAk4LKZ9F0NEXw8z6sENxMBUphyAoYBEAtiAYjKc806Nn1IXw4tcuwVWn2C65ybIIij+1zCExFE/JDJX+HNcQYE8a/QUsgrqwLQj6R5JIpDO2RRAOYFun0yKIWH6MJtMFYgQBPZ6Y1AzVzatubrVNz3AC85sjjgdEEbb8+Pn1S7G9cwhnLmp2fB8o/BCb24RN4WFkUCnfaE0wgO7hBIiAdy+dDyHsSuF1r/eVFCPolIKgNhRAvSxMmtuYG/z2sghM7ds8n41RC0e01qB7OIH+kSQODiVw2oJG/Xko4JOxE+c+I5Yf7X2jGIyn8IXLjsGbjpuhA+MDsaQOgHqNRcUI3BZBfR6LQBlgu6T7o3ckiTuf24Vli1sQS2b0+rqKsOXHEa01OLKtVo+7oz+GU+Y1YE5jRGv+gG0RRIK2UFMxgsaohSPbarClYyAn48a0Ts2sIYX5Op0ReHyzvbpb2OO+c+/TjC2owGoo4NP339rdPXZ2VdjCjW9cAsCZaZcvN9/d5E5ZtAcGYzl1Bko4R4J+h6tqjus+O2VeI46dVafdaCp7CbCzn0zXXm1IxoTkOCajhgBgi2BSUDeQ0zVkX+iw5XPUEeztG0XARzpAqlxDatGZaFDGCOR3UjL4rLSQfK4h5XsciqcMi8C+edXEUBeyNUPAzqjwmphClg9LZtTh8hNnaz+2qWFGg3787h/OcUyQbsIBv57g7HOQrRJW2n9UTo7N0SBm1ofxyTcdpbU4rxiBSn2cZaxlDNjCrSFiYcXnL8LbTskGPvX3XRZBMOBMSVWTdNjyyUnT1n63dw2hZzjucAOEDeHuON6gH69Kd9txs+tw7Kx6Lcz6R7MtlL3Ot0ohdQtYL+sGyBa8qfjKgYEY/uXBTbhn9e5ssNgV2P7F9Wfi5qtPcLyvUitNi8BPhJqgXWCnMroao0FcedJsvO2UOdpiU5iuMDV+Uxibr6NBP5Y/vROAd5KCIrtSWa5FEPT7tBB5cU+fw58P2Pe38lw15snEUdaoQmUNdQ7EdcqxQt07qvsoYFcouzuFzm+O4tHPXKibH540t0Hfr0rxcVutyjJpzrPg0UTDgmASUDdQIp3R6aQRrcX4HW6eRCqD5pqgvlHqZYsE9WBHg7aGOxBLIiPdPIDtqw1bPsz2cOcA2YdSLfQdMtYtUBM5EelVuSKWd3dSr+Cq2z9+1hHNeLMsuvEiEvQ7UgMjll9PhqZFADgDxUoLNrVh9QCdsbAZP3jPKbj8RHtVKOViU4JxbmPEM/vCfYxu14ASmGpCVpPdS3t6kRFw5Iyb6YSOfRp/K/eAOoaB0aTWOj0tAjmpuK2MfDGClKv9tAoWt/eNSkvQmTUUCvjQUhtCXdhy1Ayo2IE6vz6yEyGUr7+9bxSW3xYMbzlpNm5972mO+ADgXOJVCceLjmnDis9fhOe+dDEWtGSLzy45fiYGYimcsbAJRxVosqa0f6cWnY0RqNTfdEboQk+Fz0e6ZcnSRfkXejGtAiUIDgzkWgSmK3C7rIG59Pj8972CiLSVolyZSkioY6mV7WrciRLlomyCgIjmE9EKItpMRJuI6NMe2xAR3UJE24loPRGdXq7xVBJ1A8WTWdeQmnBCli+nHUBDxNLab33EQq0sy1ffa4wGIQTw11e78L6frAIAnH9UKzZ98/KcVEuF0ppUd0QzwGVOQFecaAuC0UQ6x39O5J1u6eVjdU+o0aBftz+IWH6cfUSz/szykzbbVbaE+r55PEqL9rIIQpYP7zhtnvapqk6O+bKo9Pddx+PunQTY505djwXNUfh9hDVy1TKznYAZPDRRQj/gI92vp0H6qwdiKX39Qx7ntjGfayiSxyLIs9Lp3t5R7RpS4wwGfI4sN3MiV9lEqmhPabxqIu7oG0VDxMqZ/E0cFoEcPxHhiNYazGmM6HPfGLXw1pNta+26ZQvz7g/IZg2Zz4zpWjPPU0tNfrfKW07MbS2vf8O4XoOxFNr7RjEQS+UscNQYteNPtaEALjl+BgDgmjPmFxy/4iPnH4GffXCprnDXgkBbBAHc/dGzce1ZC0ra36FSzhhBCsDnhBAvElEdgLVE9JgQYrOxzVsAHCX/nQ3gdvn/YYXSch3BYuVG8PCH1kcsR4aMqVlFgn6c0mSn+X3oly/o90OB3CpbE6VpqPYAZh2BORmeOLceX7jsGFx2wkwk5OpMAZ+9Hmso4PN88C0/gch2HbhrExTRoL0+8qjUSgN+H46aUYttnUMgIh18VNaTElymxn3xsTNw0xuXaLeFOg7zGIIyYNgpLai6UGHT2q2Fhz1iHDXBrCAIBnyY3xTBmt12W2MzMKgEgHvSVqfskuOzrYsdFkEB11DWInDuc2FLDY6eWQvL78OmfQM6RmT2dVLnF7A1+La6kCN91D1OEzOt1LT4lFDbPxDPG6fQ21q28M8I799Sx9sYsfDm42bgd/9wDs4soKkD2etsCgKlwatrr3Bn75gcN7uQ1ZF1z9y1cjfuWrkbABypsQDw7jPmY+nCZkSCflx87Ezs+vaVBcduQkR403Ez8edN9sJTc6QgMF1+yxa3lLy/Q6VsFoEQokMI8aJ8PQhgC4C5rs2uBnCXsFkFoJGI8ovqacLy687AD99zqv5bmXdxV/ookGvyA/bk3xAJwkf2TWlqVtGgHyfPa8x5sLwCuybHzqpDbSigM0LsXkPOGAGQNVuXzKjLCSK7JyPzO2agzh6nkeIZ8Nnpsla2AAcAHrzpfPztSxcDyKbQui0C0zRurgni85cd4xB4aoymW6M+bEGIXH+vF8VcQ4DdH2aRo2K2TlfHtnpYBG732aZ2u5L4/Ya264gRFHANNelma06drSFi4S//9AYdsF8sV8R6+6lz8KZjZ+BjFxyBmy5eorcfjKXQNRiXMQJlueQ/N2bqsBk3UZPkgYEY6opUvBKRVkC8kgiU0lAbDoCIHEuE5kOdIzMtudaIEZjPRXNtrkXwiw+did98bFnB34mG/NJ16hyz2yKIBP05cYixosZ+7Ox6fONtxxe0VMrJpGQNEdEiAKcBWO36aC6A142/98r3HKuLE9ENAG4AgAULJsdUOhQuPWGWI/unuSYIy0+4a+Uu3X3QjBEoQgEf4qkM6iMWjppZiyNa7QIUs6hHLdG4dFETntmWXaO22IRXEwrgHafNxa9W2dpN2FiPIJ8QCWp3QAB9I0nP+EB27P6coLHiXafPw+yGMO5ZvQdAUo81EvRjbtD246occneMIJ+rS48xkJtxUx+x0DkYR03QX3xiUZORbMER9Uh//ekHz3RkhZw8rwGPb7EzXEz3g2qN7a6I/ecrj8OD6/bh3COzGp4KkL+0pxcPb+hwjMXkvCNb8J13nYTTF3hryuo8z2uK4FcfOdtueyx/XxWwKQZjdnGjFsgFLALncfm0T179Xs9wwtFuIR91YQsDsZSnIFDXLN9ylF4oReGaM+bp98w6AlNZafWwCN54zIyiv1ETtFcPDPqd19FtEUwEauw1oUBRt1g5KbsgIKJaAPcB+IwQYmA8+xBCLAewHACWLl2axws6tTDzzxuj9kpLfSNJXURiZg0p6sIBxIfshS1uuGAxPnzeEQDsvGOFehDPW9LqEATFLAIA+OC5i7QgqI9YukzfSxM131daS6HfCEmtPztOsxBrPk6e14j7XrQrcr0moF9/9Gw8srFD/5bKGioWLFOTpxnPUOZ1saIrwMiYkk35vFxD7kwss3e9mecdtnyex/aGo9s8++bUhy29YIo5FpOA34f3nJlf+VETSdjy57Q2UGNTCgZgu95Cfu/sJhNTgIYCfmSEtGQdVbzFpw9tEXj8Vj43YiHqwxZe/dZbHBagilsEA876FHc+f6moYk/LdT1m1nsnYhwKSuC2TsLiM4UoqyAgIgu2ELhbCPFHj03aAZjRlXnyvWmP2aEwYvlh6hYBH2lTWz1Ylp+MVgi2VheU25g+cfXQXH/uIpy5qAnvun0lAO9sHjdLZtRi7VffjI37BnDukS26k2YxQWCmUObDrY2ZD3fYFQ/xmhROnd+oVw8DxmIR5MY5lO86XyqtiWWk7XX0eweL3Zw8LztOU/sPBfxFLTOT+kjAUVSU7zoUQrsYPYS0yjw7fUETVkrr4Nqz5muLwKuVw5uOnZEzjmDAp7NnzOtaLEYAZN2K3oKAcvZZCu7xOdJHjfPvtcRsKcyoD6G5JpizPGY52kHPb47iDx8/x3HvV4JyZg0RgJ8B2CKE+H6ezR4E8AGZPbQMQL8QoiPPttOKoGzz3CQ7Gw4a6xKYLRhUE6tkWujUP3e3QUcVbjCrzZ2xMJt5EypxAmqpDeENR7fB8mdjBPmEiPq8ZhwWgdmLX/vO5Ri9tG430RIFgdq3l0VQLGMIMBuHBXQ74mLkm2BmNYQd7aKL4Z5IxyMIajwsS0VjNAiirAVz5qImuzpW57/nfudn15+J299/huM9MxHBPD+lWARqG69rrtbRKKUavRBmZbEpcApZPIX4wmXH4hcfOlPfU+cvacX3rjm5qJtxvCxd1KyzsipFOS2C8wBcB2ADEa2T7/0zgAUAIIS4A8DDAK4AsB3ACIAPlXE8k0405Hf0lPfCzJJQfvJCmpbbnxq2fHJ9gbHf9NkYQWGLQD3MxWIE5tgixutsl9LivmmFSq+c4bFqlNcYTUGgBGkpFkE264js1hclaqdfueK4nMVfvnj5sZ4rg+XDLfDzLbRTiGgoqxi4CQZ8uO29p+O0BY34yAVH6HvR5yOpPZd2rM5Otc4Gb8Uo5BpSiRM1eVqnl0ptnhjBeGmQWXsZWXRwzpEtePfS0tJCpytlEwRCiGcBFBShQggB4MZyjaHSRC1/0aZRJxn+Zt1T3SNH/KY3LsFtK7bnuC5aakJo7xstyTXkxquOwMQMFgOFNaz3nr3A0anTXEzGzK/3+yinPsGLd5w+DwtaosVdQypGEHBmDQFjswiCAR8+fP4ROGtRc5Fv2HzswsU570WC/pJabCjcFajjuYbq2uTr2Hnlyd5ZKKGAr2ArB5NgHosgX78jk7oCriF3ceV4UWMai3ArBbUW9mQ1fqsk3GuojLTVhXL6jrgxNXmlTXpZBJ+79Gh87tKjc8zThS1RtPeNOiotS6VOVi/ma2xFRGiKWlorLzRRuTMeIh4xArfpXoiGiIWLjy1epekVI9CuoRKyUcxais9ecnRJY5so3BNpKQLSTaE05EKELF/JE3AokO2lEwr4dG3AWCwCL5ebEi6H2ljNtAgmsne/Wq51shq/VRIWBGXk9vef4amhuG/We29Yhs7BOD75m5cAeK9IlM8/+d/XnobfrXm9YIFMPlpqQ3j4UxfgyLbcdQcU93/iPDTVBPGjFTtKjkMA9sSqCtHMNNWJ1NiAIq6hMVoEk40a54lz67GxfWDMQVPAyBoao2vwDUfPKNhmweTcI1t0gR4RoSZod8MtLWvIuyAOAG584xIEAz5HKuh4MNNHJxK1UBJbBMwhYVoDL3/90ryOsrNlBaEWBCU8YIq2upDuWzIezIwkL1QPdR+NbbIhsjtVxpMZnVkztymCeX2FLaSx4pU+qs5fKTECr+9PFsry++9rT0Mo4MspWCqFaDB/jKAQ//V3p5S87T+84UjH35GgvchRKUsoNuoV1nKvRU0ogM+8+dCtsGDAh9kNYf28/c91Z+DIttoi3yqOSrkdb/bRdIIFwSTRMAatYjLWKB0rwYBvTBYBoPzX2Wypz196TE5K3kSMC8itLAZKy2qppEXwhqPbsKNrCAubo+POGpnXFMXbT52Dc4xitXJTEwoAg/GSzu/bT5uLWQ3hsk+mj3/2Ddp1edkJsyZ03+waYipCKQ/YZGM3whvbA+Ferawck23IYyJXwfaxZA2NJ1B7qBw/px7/+e7SNXMvggEffnjtacU3nECUFVJKHUFDxJrwidmLfOtwTATsGmIqwnhSQcvNPR9bVjSDx0005PdcqnIi8YoRzKwPgyjb0bEQVgVdQ9OVsQiCw4Gp+DxONCwImJIYj881agUwGkgX3/AQ8Coom9cUxYrPXVRwgROFX1Z5jyeHv1pRtQSlBOOnM//2jhPx1NauSg9jUji8r+Q0Y8XnL9J99A8HGqLWJFoEzkj8otb8mVBurlu2EBeV0IyMsYkG/agJ+ic0VXMq8vdnL8Tfn125RnCTCQuCKcQRrTU5y/1NZ75yxXGIpcptEWSLicbLN646YaKGUxU01wQnbeUsZnJgQcCUjbFo5ePl9AVNuOHCxTgtT5tmZuL5p0uOxofPP6LSw2AmEBYEzLQmEvTjn684rtLDqCpaa0NjThxgpjYcIWMYhqlyWBAwDMNUOSwIGIZhqhwWBAzDMFUOCwKGYZgqhwUBwzBMlcOCgGEYpsphQcAwDFPlkBAT2x++3BBRF4Dd4/x6K4CDEzicSsLHMjXhY5ma8LEAC4UQbV4fTDtBcCgQ0RohxNJKj2Mi4GOZmvCxTE34WArDriGGYZgqhwUBwzBMlVNtgmB5pQcwgfCxTE34WKYmfCwFqKoYAcMwDJNLtVkEDMMwjAsWBAzDMFVO1QgCIrqciLYS0XYi+lKlxzNWiGgXEW0gonVEtEa+10xEjxHRNvn/lFymi4h+TkSdRLTReM9z7GRzi7xO64no9MqNPJc8x/INImqX12YdEV1hfPZleSxbieiyyow6FyKaT0QriGgzEW0iok/L96fddSlwLNPxuoSJ6Hkielkeyzfl+0cQ0Wo55nuJKCjfD8m/t8vPF43rh4UQh/0/AH4AOwAsBhAE8DKA4ys9rjEewy4Ara73vgvgS/L1lwB8p9LjzDP2CwGcDmBjsbEDuALAIwAIwDIAqys9/hKO5RsAPu+x7fHyXgsBOELeg/5KH4Mc22wAp8vXdQBeleOddtelwLFMx+tCAGrlawvAanm+fwfgWvn+HQD+Ub7+BIA75OtrAdw7nt+tFovgLADbhRA7hRAJAL8FcHWFxzQRXA3gTvn6TgBvr9xQ8iOEeBpAj+vtfGO/GsBdwmYVgEYimj0pAy2BPMeSj6sB/FYIERdCvAZgO+x7seIIITqEEC/K14MAtgCYi2l4XQocSz6m8nURQogh+acl/wkAFwP4g3zffV3U9foDgDcREY31d6tFEMwF8Lrx914UvlGmIgLAX4hoLRHdIN+bKYTokK/3A5hZmaGNi3xjn67X6ibpMvm54aKbFsci3QmnwdY+p/V1cR0LMA2vCxH5iWgdgE4Aj8G2WPqEECm5iTlefSzy834ALWP9zWoRBIcD5wshTgfwFgA3EtGF5ofCtg2nZS7wdB675HYARwI4FUAHgP+q6GjGABHVArgPwGeEEAPmZ9Ptungcy7S8LkKItBDiVADzYFsqx5b7N6tFELQDmG/8PU++N20QQrTL/zsB3A/7BjmgzHP5f2flRjhm8o192l0rIcQB+fBmAPwEWTfDlD4WIrJgT5x3CyH+KN+eltfF61im63VRCCH6AKwAcA5sV1xAfmSOVx+L/LwBQPdYf6taBMELAI6Skfcg7KDKgxUeU8kQUQ0R1anXAC4FsBH2MXxQbvZBAA9UZoTjIt/YHwTwAZmlsgxAv+GqmJK4fOXvgH1tAPtYrpWZHUcAOArA85M9Pi+kH/lnALYIIb5vfDTtrku+Y5mm16WNiBrl6wiAS2DHPFYAuEZu5r4u6npdA+BJacmNjUpHySfrH+ysh1dh+9u+UunxjHHsi2FnObwMYJMaP2xf4BMAtgF4HEBzpceaZ/y/gW2aJ2H7Nz+Sb+ywsyZ+JK/TBgBLKz3+Eo7lV3Ks6+WDOdvY/ivyWLYCeEulx2+M63zYbp/1ANbJf1dMx+tS4Fim43U5GcBLcswbAXxdvr8YtrDaDuD3AELy/bD8e7v8fPF4fpdbTDAMw1Q51eIaYhiGYfLAgoBhGKbKYUHAMAxT5bAgYBiGqXJYEDAMw1Q5LAgYZhIhoouI6KFKj4NhTFgQMAzDVDksCBjGAyJ6v+wLv46I/kc2Ahsioh/IPvFPEFGb3PZUIlolm5vdb/TwX0JEj8ve8i8S0ZFy97VE9AcieoWI7h5Pt0iGmUhYEDCMCyI6DsB7AJwn7OZfaQB/D6AGwBohxAkA/grgX+RX7gLwRSHEybArWdX7dwP4kRDiFADnwq5IBuzumJ+B3Rd/MYDzynxIDFOQQPFNGKbqeBOAMwC8IJX1COzmaxkA98ptfg3gj0TUAKBRCPFX+f6dAH4ve0PNFULcDwBCiBgAyP09L4TYK/9eB2ARgGfLflQMkwcWBAyTCwG4UwjxZcebRF9zbTfe/ixx43Ua/BwyFYZdQwyTyxMAriGiGYBex3ch7OdFdYB8H4BnhRD9AHqJ6AL5/nUA/irslbL2EtHb5T5CRBSdzINgmFJhTYRhXAghNhPRV2GvCOeD3Wn0RgDDAM6Sn3XCjiMAdhvgO+REvxPAh+T71wH4HyK6We7j3ZN4GAxTMtx9lGFKhIiGhBC1lR4Hw0w07BpiGIapctgiYBiGqXLYImAYhqlyWBAwDMNUOSwIGIZhqhwWBAzDMFUOCwKGYZgq5/8D72ve0Q4oXXcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.title('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20012bf13c8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Maximum of 4 colors\n",
    "\n",
    "Implement a network that will receive 4 colors and has to select one of them.\n",
    "\n",
    "This will require a change of the labels (y) that now take values of 0, 1, 2 or 3. However, networks do not use labels in that form directly for multi class classification, but use 1-hot encoded or categorical data instead.\n",
    "\n",
    "In keras there is a function `keras.utils.to_categorical` that can be used for that.\n",
    "\n",
    "The last layer in the network should then no longer be sigmoid, but the softmax function. And we need the multiclass form of the crossentropy function, which in keras is called `categorical_crossentropy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.39308184 0.90106928 0.89352332 0.79420884]\n",
      " [0.10509787 0.14938653 0.62006765 0.84303125]\n",
      " [0.00804328 0.70963369 0.10017055 0.45523265]\n",
      " [0.96931897 0.41272961 0.5735195  0.19918807]\n",
      " [0.33752051 0.40660834 0.06209964 0.27496058]]\n",
      "[1 3 1 0 1]\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# see: https://www.machinecurve.com/index.php/2019/10/22/how-to-use-binary-categorical-crossentropy-with-keras/#binary-crossentropy-for-binary-classification\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "loss_function_used = 'categorical_crossentropy'\n",
    "\n",
    "# 5000 quadtruple vectors\n",
    "x_train= np.random.random(size=(5,4))\n",
    "# print(x_train)\n",
    "# The colour with the maximum cards should be chosen as trump\n",
    "# The trumpvalue: the index of the column with the max value for each row (axis=0)\n",
    "y_train_label = np.argmax(x_train, axis=1)\n",
    "# The integernumbers of the category are hot encoded\n",
    "y_train_categorical= to_categorical(y_train_label)\n",
    "print(x_train)\n",
    "print(y_train_label)\n",
    "print(y_train_categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 1.3863 - accuracy: 0.0000e+00 - val_loss: 1.3589 - val_accuracy: 0.5000\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.3832 - accuracy: 0.6667 - val_loss: 1.3581 - val_accuracy: 0.5000\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.3802 - accuracy: 0.6667 - val_loss: 1.3573 - val_accuracy: 0.5000\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3772 - accuracy: 0.6667 - val_loss: 1.3566 - val_accuracy: 0.5000\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3742 - accuracy: 0.6667 - val_loss: 1.3558 - val_accuracy: 0.5000\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.3712 - accuracy: 0.6667 - val_loss: 1.3550 - val_accuracy: 0.5000\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.3682 - accuracy: 0.6667 - val_loss: 1.3543 - val_accuracy: 0.5000\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3653 - accuracy: 0.6667 - val_loss: 1.3536 - val_accuracy: 0.5000\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.3623 - accuracy: 0.6667 - val_loss: 1.3528 - val_accuracy: 0.5000\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.3594 - accuracy: 0.6667 - val_loss: 1.3521 - val_accuracy: 0.5000\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.3565 - accuracy: 0.6667 - val_loss: 1.3514 - val_accuracy: 0.5000\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.3536 - accuracy: 0.6667 - val_loss: 1.3507 - val_accuracy: 0.5000\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3507 - accuracy: 0.6667 - val_loss: 1.3500 - val_accuracy: 0.5000\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.3478 - accuracy: 0.6667 - val_loss: 1.3494 - val_accuracy: 0.5000\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.3449 - accuracy: 0.6667 - val_loss: 1.3487 - val_accuracy: 0.5000\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.3421 - accuracy: 0.6667 - val_loss: 1.3480 - val_accuracy: 0.5000\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.3393 - accuracy: 0.6667 - val_loss: 1.3474 - val_accuracy: 0.5000\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.3365 - accuracy: 0.6667 - val_loss: 1.3468 - val_accuracy: 0.5000\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.3337 - accuracy: 0.6667 - val_loss: 1.3461 - val_accuracy: 0.5000\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.3309 - accuracy: 0.6667 - val_loss: 1.3455 - val_accuracy: 0.5000\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3281 - accuracy: 0.6667 - val_loss: 1.3449 - val_accuracy: 0.5000\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.3253 - accuracy: 0.6667 - val_loss: 1.3443 - val_accuracy: 0.5000\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.3226 - accuracy: 0.6667 - val_loss: 1.3437 - val_accuracy: 0.5000\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3199 - accuracy: 0.6667 - val_loss: 1.3431 - val_accuracy: 0.5000\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3171 - accuracy: 0.6667 - val_loss: 1.3425 - val_accuracy: 0.5000\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3144 - accuracy: 0.6667 - val_loss: 1.3419 - val_accuracy: 0.5000\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.3118 - accuracy: 0.6667 - val_loss: 1.3414 - val_accuracy: 0.5000\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.3091 - accuracy: 0.6667 - val_loss: 1.3408 - val_accuracy: 0.5000\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.3064 - accuracy: 0.6667 - val_loss: 1.3403 - val_accuracy: 0.5000\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3038 - accuracy: 0.6667 - val_loss: 1.3397 - val_accuracy: 0.5000\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3011 - accuracy: 0.6667 - val_loss: 1.3392 - val_accuracy: 0.5000\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.2985 - accuracy: 0.6667 - val_loss: 1.3387 - val_accuracy: 0.5000\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2959 - accuracy: 0.6667 - val_loss: 1.3382 - val_accuracy: 0.5000\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.2933 - accuracy: 0.6667 - val_loss: 1.3377 - val_accuracy: 0.5000\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.2908 - accuracy: 0.6667 - val_loss: 1.3372 - val_accuracy: 0.5000\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.2882 - accuracy: 0.6667 - val_loss: 1.3367 - val_accuracy: 0.5000\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2856 - accuracy: 0.6667 - val_loss: 1.3362 - val_accuracy: 0.5000\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2831 - accuracy: 0.6667 - val_loss: 1.3358 - val_accuracy: 0.5000\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2806 - accuracy: 0.6667 - val_loss: 1.3353 - val_accuracy: 0.5000\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2781 - accuracy: 0.6667 - val_loss: 1.3348 - val_accuracy: 0.5000\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2756 - accuracy: 0.6667 - val_loss: 1.3344 - val_accuracy: 0.5000\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.2731 - accuracy: 0.6667 - val_loss: 1.3340 - val_accuracy: 0.5000\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2706 - accuracy: 0.6667 - val_loss: 1.3335 - val_accuracy: 0.5000\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2681 - accuracy: 0.6667 - val_loss: 1.3331 - val_accuracy: 0.5000\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2657 - accuracy: 0.6667 - val_loss: 1.3327 - val_accuracy: 0.5000\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2633 - accuracy: 0.6667 - val_loss: 1.3323 - val_accuracy: 0.5000\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2608 - accuracy: 0.6667 - val_loss: 1.3319 - val_accuracy: 0.5000\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2584 - accuracy: 0.6667 - val_loss: 1.3315 - val_accuracy: 0.5000\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2560 - accuracy: 0.6667 - val_loss: 1.3311 - val_accuracy: 0.5000\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2536 - accuracy: 0.6667 - val_loss: 1.3307 - val_accuracy: 0.5000\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.2513 - accuracy: 0.6667 - val_loss: 1.3304 - val_accuracy: 0.5000\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2489 - accuracy: 0.6667 - val_loss: 1.3300 - val_accuracy: 0.5000\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2466 - accuracy: 0.6667 - val_loss: 1.3297 - val_accuracy: 0.5000\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2442 - accuracy: 0.6667 - val_loss: 1.3293 - val_accuracy: 0.5000\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2419 - accuracy: 0.6667 - val_loss: 1.3290 - val_accuracy: 0.5000\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2396 - accuracy: 0.6667 - val_loss: 1.3287 - val_accuracy: 0.5000\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2373 - accuracy: 0.6667 - val_loss: 1.3283 - val_accuracy: 0.5000\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.2350 - accuracy: 0.6667 - val_loss: 1.3280 - val_accuracy: 0.5000\n",
      "Epoch 59/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2327 - accuracy: 0.6667 - val_loss: 1.3277 - val_accuracy: 0.5000\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2305 - accuracy: 0.6667 - val_loss: 1.3274 - val_accuracy: 0.5000\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2282 - accuracy: 0.6667 - val_loss: 1.3271 - val_accuracy: 0.5000\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2260 - accuracy: 0.6667 - val_loss: 1.3268 - val_accuracy: 0.5000\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2238 - accuracy: 0.6667 - val_loss: 1.3265 - val_accuracy: 0.5000\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2216 - accuracy: 0.6667 - val_loss: 1.3263 - val_accuracy: 0.5000\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2194 - accuracy: 0.6667 - val_loss: 1.3260 - val_accuracy: 0.5000\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.2172 - accuracy: 0.6667 - val_loss: 1.3257 - val_accuracy: 0.5000\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2150 - accuracy: 0.6667 - val_loss: 1.3255 - val_accuracy: 0.5000\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2128 - accuracy: 0.6667 - val_loss: 1.3252 - val_accuracy: 0.5000\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2107 - accuracy: 0.6667 - val_loss: 1.3250 - val_accuracy: 0.5000\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2085 - accuracy: 0.6667 - val_loss: 1.3248 - val_accuracy: 0.5000\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2064 - accuracy: 0.6667 - val_loss: 1.3245 - val_accuracy: 0.5000\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2043 - accuracy: 0.6667 - val_loss: 1.3243 - val_accuracy: 0.5000\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2021 - accuracy: 0.6667 - val_loss: 1.3241 - val_accuracy: 0.5000\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2001 - accuracy: 0.6667 - val_loss: 1.3239 - val_accuracy: 0.5000\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1980 - accuracy: 0.6667 - val_loss: 1.3237 - val_accuracy: 0.5000\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1959 - accuracy: 0.6667 - val_loss: 1.3235 - val_accuracy: 0.5000\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1938 - accuracy: 0.6667 - val_loss: 1.3233 - val_accuracy: 0.5000\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1918 - accuracy: 0.6667 - val_loss: 1.3232 - val_accuracy: 0.5000\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1897 - accuracy: 0.6667 - val_loss: 1.3230 - val_accuracy: 0.5000\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1877 - accuracy: 0.6667 - val_loss: 1.3228 - val_accuracy: 0.5000\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1857 - accuracy: 0.6667 - val_loss: 1.3227 - val_accuracy: 0.5000\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1836 - accuracy: 0.6667 - val_loss: 1.3225 - val_accuracy: 0.5000\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1816 - accuracy: 0.6667 - val_loss: 1.3224 - val_accuracy: 0.5000\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1797 - accuracy: 0.6667 - val_loss: 1.3222 - val_accuracy: 0.5000\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1777 - accuracy: 0.6667 - val_loss: 1.3221 - val_accuracy: 0.5000\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1757 - accuracy: 0.6667 - val_loss: 1.3219 - val_accuracy: 0.5000\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1737 - accuracy: 0.6667 - val_loss: 1.3218 - val_accuracy: 0.5000\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1718 - accuracy: 0.6667 - val_loss: 1.3217 - val_accuracy: 0.5000\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1699 - accuracy: 0.6667 - val_loss: 1.3216 - val_accuracy: 0.5000\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1679 - accuracy: 0.6667 - val_loss: 1.3215 - val_accuracy: 0.5000\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1660 - accuracy: 0.6667 - val_loss: 1.3214 - val_accuracy: 0.5000\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1641 - accuracy: 0.6667 - val_loss: 1.3213 - val_accuracy: 0.5000\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1622 - accuracy: 0.6667 - val_loss: 1.3212 - val_accuracy: 0.5000\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1603 - accuracy: 0.6667 - val_loss: 1.3211 - val_accuracy: 0.5000\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1584 - accuracy: 0.6667 - val_loss: 1.3210 - val_accuracy: 0.5000\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1566 - accuracy: 0.6667 - val_loss: 1.3210 - val_accuracy: 0.5000\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1547 - accuracy: 0.6667 - val_loss: 1.3209 - val_accuracy: 0.5000\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1529 - accuracy: 0.6667 - val_loss: 1.3209 - val_accuracy: 0.5000\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1510 - accuracy: 0.6667 - val_loss: 1.3208 - val_accuracy: 0.5000\n",
      "Epoch 100/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1492 - accuracy: 0.6667 - val_loss: 1.3208 - val_accuracy: 0.5000\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1474 - accuracy: 0.6667 - val_loss: 1.3207 - val_accuracy: 0.5000\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1456 - accuracy: 0.6667 - val_loss: 1.3207 - val_accuracy: 0.5000\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1438 - accuracy: 0.6667 - val_loss: 1.3206 - val_accuracy: 0.5000\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1420 - accuracy: 0.6667 - val_loss: 1.3206 - val_accuracy: 0.5000\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1402 - accuracy: 0.6667 - val_loss: 1.3206 - val_accuracy: 0.5000\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.1384 - accuracy: 0.6667 - val_loss: 1.3206 - val_accuracy: 0.5000\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1367 - accuracy: 0.6667 - val_loss: 1.3206 - val_accuracy: 0.5000\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1349 - accuracy: 0.6667 - val_loss: 1.3206 - val_accuracy: 0.5000\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1332 - accuracy: 0.6667 - val_loss: 1.3206 - val_accuracy: 0.5000\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1314 - accuracy: 0.6667 - val_loss: 1.3206 - val_accuracy: 0.5000\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1297 - accuracy: 0.6667 - val_loss: 1.3206 - val_accuracy: 0.5000\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1280 - accuracy: 0.6667 - val_loss: 1.3206 - val_accuracy: 0.5000\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1263 - accuracy: 0.6667 - val_loss: 1.3206 - val_accuracy: 0.5000\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1246 - accuracy: 0.6667 - val_loss: 1.3206 - val_accuracy: 0.5000\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1229 - accuracy: 0.6667 - val_loss: 1.3207 - val_accuracy: 0.5000\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1212 - accuracy: 0.6667 - val_loss: 1.3207 - val_accuracy: 0.5000\n",
      "Epoch 117/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1196 - accuracy: 0.6667 - val_loss: 1.3207 - val_accuracy: 0.5000\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1179 - accuracy: 0.6667 - val_loss: 1.3208 - val_accuracy: 0.5000\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1162 - accuracy: 0.6667 - val_loss: 1.3208 - val_accuracy: 0.5000\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1146 - accuracy: 0.6667 - val_loss: 1.3209 - val_accuracy: 0.5000\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1130 - accuracy: 0.6667 - val_loss: 1.3210 - val_accuracy: 0.5000\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1113 - accuracy: 0.6667 - val_loss: 1.3210 - val_accuracy: 0.5000\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1097 - accuracy: 0.6667 - val_loss: 1.3211 - val_accuracy: 0.5000\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.1081 - accuracy: 0.6667 - val_loss: 1.3212 - val_accuracy: 0.5000\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1065 - accuracy: 0.6667 - val_loss: 1.3212 - val_accuracy: 0.5000\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1049 - accuracy: 0.6667 - val_loss: 1.3213 - val_accuracy: 0.5000\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1033 - accuracy: 0.6667 - val_loss: 1.3214 - val_accuracy: 0.5000\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1018 - accuracy: 0.6667 - val_loss: 1.3215 - val_accuracy: 0.5000\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1002 - accuracy: 0.6667 - val_loss: 1.3216 - val_accuracy: 0.5000\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0986 - accuracy: 0.6667 - val_loss: 1.3217 - val_accuracy: 0.5000\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0971 - accuracy: 0.6667 - val_loss: 1.3218 - val_accuracy: 0.5000\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0955 - accuracy: 0.6667 - val_loss: 1.3219 - val_accuracy: 0.5000\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0940 - accuracy: 0.6667 - val_loss: 1.3220 - val_accuracy: 0.5000\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0925 - accuracy: 0.6667 - val_loss: 1.3221 - val_accuracy: 0.5000\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0910 - accuracy: 0.6667 - val_loss: 1.3223 - val_accuracy: 0.5000\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0895 - accuracy: 0.6667 - val_loss: 1.3224 - val_accuracy: 0.5000\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0880 - accuracy: 0.6667 - val_loss: 1.3225 - val_accuracy: 0.5000\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0865 - accuracy: 0.6667 - val_loss: 1.3227 - val_accuracy: 0.5000\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0850 - accuracy: 0.6667 - val_loss: 1.3228 - val_accuracy: 0.5000\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0835 - accuracy: 0.6667 - val_loss: 1.3230 - val_accuracy: 0.5000\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0820 - accuracy: 0.6667 - val_loss: 1.3231 - val_accuracy: 0.5000\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0806 - accuracy: 0.6667 - val_loss: 1.3233 - val_accuracy: 0.5000\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0791 - accuracy: 0.6667 - val_loss: 1.3234 - val_accuracy: 0.5000\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0777 - accuracy: 0.6667 - val_loss: 1.3236 - val_accuracy: 0.5000\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0762 - accuracy: 0.6667 - val_loss: 1.3237 - val_accuracy: 0.5000\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0748 - accuracy: 0.6667 - val_loss: 1.3239 - val_accuracy: 0.5000\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0734 - accuracy: 0.6667 - val_loss: 1.3241 - val_accuracy: 0.5000\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0720 - accuracy: 0.6667 - val_loss: 1.3242 - val_accuracy: 0.5000\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0705 - accuracy: 0.6667 - val_loss: 1.3244 - val_accuracy: 0.5000\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0691 - accuracy: 0.6667 - val_loss: 1.3246 - val_accuracy: 0.5000\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0677 - accuracy: 0.6667 - val_loss: 1.3248 - val_accuracy: 0.5000\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0664 - accuracy: 0.6667 - val_loss: 1.3250 - val_accuracy: 0.5000\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0650 - accuracy: 0.6667 - val_loss: 1.3252 - val_accuracy: 0.5000\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0636 - accuracy: 0.6667 - val_loss: 1.3254 - val_accuracy: 0.5000\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0622 - accuracy: 0.6667 - val_loss: 1.3256 - val_accuracy: 0.5000\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0609 - accuracy: 0.6667 - val_loss: 1.3258 - val_accuracy: 0.5000\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0595 - accuracy: 0.6667 - val_loss: 1.3260 - val_accuracy: 0.5000\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0582 - accuracy: 0.6667 - val_loss: 1.3262 - val_accuracy: 0.5000\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0568 - accuracy: 0.6667 - val_loss: 1.3264 - val_accuracy: 0.5000\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0555 - accuracy: 0.6667 - val_loss: 1.3266 - val_accuracy: 0.5000\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0542 - accuracy: 0.6667 - val_loss: 1.3269 - val_accuracy: 0.5000\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0529 - accuracy: 0.6667 - val_loss: 1.3271 - val_accuracy: 0.5000\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0516 - accuracy: 0.6667 - val_loss: 1.3273 - val_accuracy: 0.5000\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0503 - accuracy: 0.6667 - val_loss: 1.3276 - val_accuracy: 0.5000\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0490 - accuracy: 0.6667 - val_loss: 1.3278 - val_accuracy: 0.5000\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0477 - accuracy: 0.6667 - val_loss: 1.3280 - val_accuracy: 0.5000\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0464 - accuracy: 0.6667 - val_loss: 1.3283 - val_accuracy: 0.5000\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0451 - accuracy: 0.6667 - val_loss: 1.3285 - val_accuracy: 0.5000\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0438 - accuracy: 0.6667 - val_loss: 1.3288 - val_accuracy: 0.5000\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0426 - accuracy: 0.6667 - val_loss: 1.3290 - val_accuracy: 0.5000\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0413 - accuracy: 0.6667 - val_loss: 1.3293 - val_accuracy: 0.5000\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0401 - accuracy: 0.6667 - val_loss: 1.3296 - val_accuracy: 0.5000\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0388 - accuracy: 0.6667 - val_loss: 1.3298 - val_accuracy: 0.5000\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0376 - accuracy: 0.6667 - val_loss: 1.3301 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0364 - accuracy: 0.6667 - val_loss: 1.3304 - val_accuracy: 0.5000\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0351 - accuracy: 0.6667 - val_loss: 1.3306 - val_accuracy: 0.5000\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0339 - accuracy: 0.6667 - val_loss: 1.3309 - val_accuracy: 0.5000\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0327 - accuracy: 0.6667 - val_loss: 1.3312 - val_accuracy: 0.5000\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0315 - accuracy: 0.6667 - val_loss: 1.3315 - val_accuracy: 0.5000\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0303 - accuracy: 0.6667 - val_loss: 1.3317 - val_accuracy: 0.5000\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0291 - accuracy: 0.6667 - val_loss: 1.3320 - val_accuracy: 0.5000\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0279 - accuracy: 0.6667 - val_loss: 1.3323 - val_accuracy: 0.5000\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0267 - accuracy: 0.6667 - val_loss: 1.3326 - val_accuracy: 0.5000\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0256 - accuracy: 0.6667 - val_loss: 1.3329 - val_accuracy: 0.5000\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0244 - accuracy: 0.6667 - val_loss: 1.3332 - val_accuracy: 0.5000\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0232 - accuracy: 0.6667 - val_loss: 1.3335 - val_accuracy: 0.5000\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0221 - accuracy: 0.6667 - val_loss: 1.3338 - val_accuracy: 0.5000\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0209 - accuracy: 0.6667 - val_loss: 1.3341 - val_accuracy: 0.5000\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0198 - accuracy: 0.6667 - val_loss: 1.3344 - val_accuracy: 0.5000\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0186 - accuracy: 0.6667 - val_loss: 1.3348 - val_accuracy: 0.5000\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0175 - accuracy: 0.6667 - val_loss: 1.3351 - val_accuracy: 0.5000\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0164 - accuracy: 0.6667 - val_loss: 1.3354 - val_accuracy: 0.5000\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0153 - accuracy: 0.6667 - val_loss: 1.3357 - val_accuracy: 0.5000\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0141 - accuracy: 0.6667 - val_loss: 1.3360 - val_accuracy: 0.5000\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0130 - accuracy: 0.6667 - val_loss: 1.3364 - val_accuracy: 0.5000\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0119 - accuracy: 0.6667 - val_loss: 1.3367 - val_accuracy: 0.5000\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0108 - accuracy: 0.6667 - val_loss: 1.3370 - val_accuracy: 0.5000\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0097 - accuracy: 0.6667 - val_loss: 1.3374 - val_accuracy: 0.5000\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0086 - accuracy: 0.6667 - val_loss: 1.3377 - val_accuracy: 0.5000\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0076 - accuracy: 0.6667 - val_loss: 1.3380 - val_accuracy: 0.5000\n",
      "Epoch 201/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0065 - accuracy: 0.6667 - val_loss: 1.3384 - val_accuracy: 0.5000\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0054 - accuracy: 0.6667 - val_loss: 1.3387 - val_accuracy: 0.5000\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0043 - accuracy: 0.6667 - val_loss: 1.3391 - val_accuracy: 0.5000\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0033 - accuracy: 0.6667 - val_loss: 1.3394 - val_accuracy: 0.5000\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0022 - accuracy: 0.6667 - val_loss: 1.3398 - val_accuracy: 0.5000\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0012 - accuracy: 0.6667 - val_loss: 1.3401 - val_accuracy: 0.5000\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0001 - accuracy: 0.6667 - val_loss: 1.3405 - val_accuracy: 0.5000\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9991 - accuracy: 0.6667 - val_loss: 1.3408 - val_accuracy: 0.5000\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9980 - accuracy: 0.6667 - val_loss: 1.3412 - val_accuracy: 0.5000\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9970 - accuracy: 0.6667 - val_loss: 1.3416 - val_accuracy: 0.5000\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9960 - accuracy: 0.6667 - val_loss: 1.3419 - val_accuracy: 0.5000\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9950 - accuracy: 0.6667 - val_loss: 1.3423 - val_accuracy: 0.5000\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9940 - accuracy: 0.6667 - val_loss: 1.3427 - val_accuracy: 0.5000\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9929 - accuracy: 0.6667 - val_loss: 1.3430 - val_accuracy: 0.5000\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9919 - accuracy: 0.6667 - val_loss: 1.3434 - val_accuracy: 0.5000\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9909 - accuracy: 0.6667 - val_loss: 1.3438 - val_accuracy: 0.5000\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9899 - accuracy: 0.6667 - val_loss: 1.3442 - val_accuracy: 0.5000\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9890 - accuracy: 0.6667 - val_loss: 1.3445 - val_accuracy: 0.5000\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9880 - accuracy: 0.6667 - val_loss: 1.3449 - val_accuracy: 0.5000\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9870 - accuracy: 0.6667 - val_loss: 1.3453 - val_accuracy: 0.5000\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9860 - accuracy: 0.6667 - val_loss: 1.3457 - val_accuracy: 0.5000\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9850 - accuracy: 0.6667 - val_loss: 1.3461 - val_accuracy: 0.5000\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9841 - accuracy: 0.6667 - val_loss: 1.3465 - val_accuracy: 0.5000\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9831 - accuracy: 0.6667 - val_loss: 1.3469 - val_accuracy: 0.5000\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9822 - accuracy: 0.6667 - val_loss: 1.3473 - val_accuracy: 0.5000\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9812 - accuracy: 0.6667 - val_loss: 1.3477 - val_accuracy: 0.5000\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9803 - accuracy: 0.6667 - val_loss: 1.3481 - val_accuracy: 0.5000\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9793 - accuracy: 0.6667 - val_loss: 1.3485 - val_accuracy: 0.5000\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9784 - accuracy: 0.6667 - val_loss: 1.3489 - val_accuracy: 0.5000\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9775 - accuracy: 0.6667 - val_loss: 1.3493 - val_accuracy: 0.5000\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9765 - accuracy: 0.6667 - val_loss: 1.3497 - val_accuracy: 0.5000\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9756 - accuracy: 0.6667 - val_loss: 1.3501 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9747 - accuracy: 0.6667 - val_loss: 1.3505 - val_accuracy: 0.5000\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9738 - accuracy: 0.6667 - val_loss: 1.3509 - val_accuracy: 0.5000\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9729 - accuracy: 0.6667 - val_loss: 1.3513 - val_accuracy: 0.5000\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9720 - accuracy: 0.6667 - val_loss: 1.3517 - val_accuracy: 0.5000\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9710 - accuracy: 0.6667 - val_loss: 1.3522 - val_accuracy: 0.5000\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9702 - accuracy: 0.6667 - val_loss: 1.3526 - val_accuracy: 0.5000\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9693 - accuracy: 0.6667 - val_loss: 1.3530 - val_accuracy: 0.5000\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9684 - accuracy: 0.6667 - val_loss: 1.3534 - val_accuracy: 0.5000\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9675 - accuracy: 0.6667 - val_loss: 1.3538 - val_accuracy: 0.5000\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9666 - accuracy: 0.6667 - val_loss: 1.3543 - val_accuracy: 0.5000\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9657 - accuracy: 0.6667 - val_loss: 1.3547 - val_accuracy: 0.5000\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9649 - accuracy: 0.6667 - val_loss: 1.3551 - val_accuracy: 0.5000\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9640 - accuracy: 0.6667 - val_loss: 1.3556 - val_accuracy: 0.5000\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9631 - accuracy: 0.6667 - val_loss: 1.3560 - val_accuracy: 0.5000\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9623 - accuracy: 0.6667 - val_loss: 1.3564 - val_accuracy: 0.5000\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9614 - accuracy: 0.6667 - val_loss: 1.3569 - val_accuracy: 0.5000\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9606 - accuracy: 0.6667 - val_loss: 1.3573 - val_accuracy: 0.5000\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9597 - accuracy: 0.6667 - val_loss: 1.3577 - val_accuracy: 0.5000\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9589 - accuracy: 0.6667 - val_loss: 1.3582 - val_accuracy: 0.5000\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9580 - accuracy: 0.6667 - val_loss: 1.3586 - val_accuracy: 0.5000\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9572 - accuracy: 0.6667 - val_loss: 1.3591 - val_accuracy: 0.5000\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9564 - accuracy: 0.6667 - val_loss: 1.3595 - val_accuracy: 0.5000\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9555 - accuracy: 0.6667 - val_loss: 1.3599 - val_accuracy: 0.5000\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9547 - accuracy: 0.6667 - val_loss: 1.3604 - val_accuracy: 0.5000\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9539 - accuracy: 0.6667 - val_loss: 1.3608 - val_accuracy: 0.5000\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9531 - accuracy: 0.6667 - val_loss: 1.3613 - val_accuracy: 0.5000\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9523 - accuracy: 0.6667 - val_loss: 1.3617 - val_accuracy: 0.5000\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9515 - accuracy: 0.6667 - val_loss: 1.3622 - val_accuracy: 0.5000\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9507 - accuracy: 0.6667 - val_loss: 1.3627 - val_accuracy: 0.5000\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9499 - accuracy: 0.6667 - val_loss: 1.3631 - val_accuracy: 0.5000\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9491 - accuracy: 0.6667 - val_loss: 1.3636 - val_accuracy: 0.5000\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9483 - accuracy: 0.6667 - val_loss: 1.3640 - val_accuracy: 0.5000\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9475 - accuracy: 0.6667 - val_loss: 1.3645 - val_accuracy: 0.5000\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9467 - accuracy: 0.6667 - val_loss: 1.3649 - val_accuracy: 0.5000\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9459 - accuracy: 0.6667 - val_loss: 1.3654 - val_accuracy: 0.5000\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9451 - accuracy: 0.6667 - val_loss: 1.3659 - val_accuracy: 0.5000\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9444 - accuracy: 0.6667 - val_loss: 1.3663 - val_accuracy: 0.5000\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9436 - accuracy: 0.6667 - val_loss: 1.3668 - val_accuracy: 0.5000\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9428 - accuracy: 0.6667 - val_loss: 1.3673 - val_accuracy: 0.5000\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9421 - accuracy: 0.6667 - val_loss: 1.3677 - val_accuracy: 0.5000\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9413 - accuracy: 0.6667 - val_loss: 1.3682 - val_accuracy: 0.5000\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9405 - accuracy: 0.6667 - val_loss: 1.3687 - val_accuracy: 0.5000\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9398 - accuracy: 0.6667 - val_loss: 1.3692 - val_accuracy: 0.5000\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9390 - accuracy: 0.6667 - val_loss: 1.3696 - val_accuracy: 0.5000\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9383 - accuracy: 0.6667 - val_loss: 1.3701 - val_accuracy: 0.5000\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9375 - accuracy: 0.6667 - val_loss: 1.3706 - val_accuracy: 0.5000\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9368 - accuracy: 0.6667 - val_loss: 1.3711 - val_accuracy: 0.5000\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9361 - accuracy: 0.6667 - val_loss: 1.3715 - val_accuracy: 0.5000\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9353 - accuracy: 0.6667 - val_loss: 1.3720 - val_accuracy: 0.5000\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9346 - accuracy: 0.6667 - val_loss: 1.3725 - val_accuracy: 0.5000\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9339 - accuracy: 0.6667 - val_loss: 1.3730 - val_accuracy: 0.5000\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9332 - accuracy: 0.6667 - val_loss: 1.3735 - val_accuracy: 0.5000\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9324 - accuracy: 0.6667 - val_loss: 1.3739 - val_accuracy: 0.5000\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9317 - accuracy: 0.6667 - val_loss: 1.3744 - val_accuracy: 0.5000\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9310 - accuracy: 0.6667 - val_loss: 1.3749 - val_accuracy: 0.5000\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9303 - accuracy: 0.6667 - val_loss: 1.3754 - val_accuracy: 0.5000\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9296 - accuracy: 0.6667 - val_loss: 1.3759 - val_accuracy: 0.5000\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9289 - accuracy: 0.6667 - val_loss: 1.3764 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9282 - accuracy: 0.6667 - val_loss: 1.3769 - val_accuracy: 0.5000\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9275 - accuracy: 0.6667 - val_loss: 1.3774 - val_accuracy: 0.5000\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9268 - accuracy: 0.6667 - val_loss: 1.3778 - val_accuracy: 0.5000\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9261 - accuracy: 0.6667 - val_loss: 1.3783 - val_accuracy: 0.5000\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9254 - accuracy: 0.6667 - val_loss: 1.3788 - val_accuracy: 0.5000\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9247 - accuracy: 0.6667 - val_loss: 1.3793 - val_accuracy: 0.5000\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9241 - accuracy: 0.6667 - val_loss: 1.3798 - val_accuracy: 0.5000\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9234 - accuracy: 0.6667 - val_loss: 1.3803 - val_accuracy: 0.5000\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9227 - accuracy: 0.6667 - val_loss: 1.3808 - val_accuracy: 0.5000\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9220 - accuracy: 0.6667 - val_loss: 1.3813 - val_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "# The input shape is now 4 instead of 2\n",
    "model.add(keras.Input(shape=[4]))\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "model.add(keras.layers.Dense(4, activation='relu'))\n",
    "#Sigmoid cannot be used, since it provides only a classification between 0 and 1\n",
    "#--> softmax\n",
    "# The output has now 4 classifications instead of 1\n",
    "model.add(keras.layers.Dense(4, activation='softmax'))\n",
    "model.compile(loss= loss_function_used,\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "# Model is trained on only 0.75 of the training data. 0.25 will be used for validation after each epoch.\n",
    "# In each epoch a batch with size 100 is extracted from the 0.75 training data for training.\n",
    "history = model.fit(x_train, y_train_categorical, validation_split=0.25, epochs=300, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/157 [..............................] - ETA: 0s - loss: 1.6983 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_test_batch_begin` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_begin` time: 0.0010s). Check your callbacks.\n",
      "157/157 [==============================] - 0s 685us/step - loss: 1.5484 - accuracy: 0.2552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5484224557876587, 0.25519999861717224]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In order to test the accuracy, we create a test set:\n",
    "# 1000 quadtruple vectors\n",
    "x_test= np.random.random(size=(5000,4))\n",
    "# print(x_train)\n",
    "# The colour with the maximum cards should be chosen as trump\n",
    "# The trumpvalue: the index of the column with the max value for each row (axis=0)\n",
    "y_test_label = np.argmax(x_test, axis=1)\n",
    "# The integernumbers of the category are hot encoded\n",
    "y_test_categorical= to_categorical(y_test_label)\n",
    "\n",
    "\n",
    "model.evaluate(x_test,y_test_categorical)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Implement a ML Network to learn trump from features\n",
    "\n",
    "We would like to train a network to get the trump from some features. (We could use the cards directly, but this is deep learning and we will see more of that in next lesson :-) )\n",
    "\n",
    "As features we can use the number of cards of a color as before and some of the features from last lecture. For keras all input features should be floating point numbers. Also we need numpy arrays and not pandas. To get the array from a panda, the property `values` can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DA</th>\n",
       "      <th>DK</th>\n",
       "      <th>DQ</th>\n",
       "      <th>DJ</th>\n",
       "      <th>D10</th>\n",
       "      <th>D9</th>\n",
       "      <th>D8</th>\n",
       "      <th>D7</th>\n",
       "      <th>D6</th>\n",
       "      <th>HA</th>\n",
       "      <th>...</th>\n",
       "      <th>CK</th>\n",
       "      <th>CQ</th>\n",
       "      <th>CJ</th>\n",
       "      <th>C10</th>\n",
       "      <th>C9</th>\n",
       "      <th>C8</th>\n",
       "      <th>C7</th>\n",
       "      <th>C6</th>\n",
       "      <th>FH</th>\n",
       "      <th>trump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   DA  DK  DQ  DJ  D10  D9  D8  D7  D6  HA  ...  CK  CQ  CJ  C10  C9  C8  C7  \\\n",
       "0   0   0   0   1    1   0   1   1   0   0  ...   0   1   0    0   0   1   0   \n",
       "1   0   0   0   0    0   0   0   0   1   1  ...   0   0   1    0   0   0   1   \n",
       "2   1   0   0   1    0   0   0   0   0   0  ...   0   1   0    0   0   0   1   \n",
       "3   0   0   0   0    0   0   0   0   0   1  ...   0   0   0    1   1   0   0   \n",
       "4   0   1   0   0    0   0   0   0   1   1  ...   0   0   1    0   0   0   0   \n",
       "\n",
       "   C6  FH  trump  \n",
       "0   0   0      6  \n",
       "1   0   0      5  \n",
       "2   1   0      6  \n",
       "3   0   0      5  \n",
       "4   0   1      4  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "# The path to the data-folder\n",
    "path_to_data = Path('data')\n",
    "# Import only a fraction of data for efficient testing\n",
    "data = pd.read_csv(path_to_data / '2018_10_18_trump.csv', header=None, nrows=10000)\n",
    "cards = [\n",
    "# Diamonds\n",
    "'DA','DK','DQ','DJ','D10','D9','D8','D7','D6',\n",
    "# Hearts\n",
    "'HA','HK','HQ','HJ','H10','H9','H8','H7','H6',\n",
    "# Spades\n",
    "'SA','SK','SQ','SJ','S10','S9','S8','S7','S6',\n",
    "# Clubs\n",
    "'CA','CK','CQ','CJ','C10','C9','C8','C7','C6'\n",
    "]\n",
    "\n",
    "# Forehand (yes = 1, no = 0)\n",
    "forehand = ['FH']\n",
    "\n",
    "user  = ['user']\n",
    "trump = ['trump']\n",
    "\n",
    "data.columns = cards + forehand + user + trump\n",
    "feature_columns = cards + forehand\n",
    "data.drop('user', axis='columns', inplace=True)\n",
    "data.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue as follows:\n",
    "- Calculate features, \n",
    "- add them to the data set\n",
    "- drop the columns not used\n",
    "- convert to numpy array\n",
    "- build a network and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "featureColumns 7\n",
      "categoryColumns 7\n",
      "featureShape (10000, 7)\n",
      "categoryShape (10000, 7)\n",
      "categoryMatrix [[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# From the \"Trump prediction with ML\"-notebook\n",
    "\n",
    "# We recreate the feature columns, with less features\n",
    "\n",
    "for color in 'DHSC':\n",
    "    # We add a new column, which indicates, if a Jack+Nine combination exists\n",
    "    # Jack and nine combination\n",
    "    \n",
    "    # The index name\n",
    "    new_col = '{}_J9'.format(color)\n",
    "    # Add the combination condition to the index\n",
    "    data[new_col]  = data['{}J'.format(color)] & data['{}9'.format(color)]\n",
    "    \n",
    "    # Exercise: Add other features here such as the combination of Ace-King-Queen (Dreiblatt).\n",
    "    \n",
    "    # For Obe:\n",
    "    # The index name\n",
    "    new_col2 = '{}_HIGH'.format(color)\n",
    "    # Add the combination condition to the index\n",
    "    data[new_col2]  = data['{}A'.format(color)] & data['{}K'.format(color)] & data['{}K'.format(color)]\n",
    "             \n",
    "    # For Unde                                                 \n",
    "    # The index name\n",
    "    new_col3 = '{}_LOW'.format(color)\n",
    "    # Add the combination condition to the index\n",
    "    data[new_col3]  = data['{}6'.format(color)] & data['{}7'.format(color)] & data['{}8'.format(color)]   \n",
    "\n",
    "# Oben bereinigen\n",
    "new_colhigh = 'HIGH'\n",
    "data[new_colhigh]=data['D_HIGH']\n",
    "data.drop('D_HIGH', axis='columns', inplace=True)\n",
    "\n",
    "for color in 'HSC':\n",
    "    # Add the combination condition to the index\n",
    "    # data[new_colhigh]  = data['{}_HIGH'.format(color)]\n",
    "    data[new_colhigh]  = data[new_colhigh] | data['{}_HIGH'.format(color)]\n",
    "    data.drop('{}_HIGH'.format(color), axis='columns', inplace=True)\n",
    "\n",
    "data[new_colhigh] = data[new_colhigh] | (data['DA'.format(color)] & data['HA'.format(color)] & data['SA'.format(color)])\n",
    "data[new_colhigh] = data[new_colhigh] | (data['DA'.format(color)] & data['HA'.format(color)] & data['CA'.format(color)])\n",
    "data[new_colhigh] = data[new_colhigh] | (data['HA'.format(color)] & data['CA'.format(color)] & data['SA'.format(color)])\n",
    "\n",
    "for color in 'HSC':\n",
    "    # Add the combination condition to the index\n",
    "    data[new_colhigh]  = data[new_colhigh] | (data['DA'.format(color)] & data['DK'.format(color)] & data['{}A'.format(color)])\n",
    "    \n",
    "for color in 'DSC':\n",
    "    # Add the combination condition to the index\n",
    "    data[new_colhigh]  = data[new_colhigh] | (data['HA'.format(color)] & data['HK'.format(color)] & data['{}A'.format(color)])\n",
    "    \n",
    "for color in 'HDC':\n",
    "    # Add the combination condition to the index\n",
    "    data[new_colhigh]  = data[new_colhigh] | (data['SA'.format(color)] & data['SK'.format(color)] & data['{}A'.format(color)])\n",
    "    \n",
    "for color in 'HDS':\n",
    "    # Add the combination condition to the index\n",
    "    data[new_colhigh]  = data[new_colhigh] | (data['CA'.format(color)] & data['CK'.format(color)] & data['{}A'.format(color)])\n",
    "\n",
    "    \n",
    "# Unten bereinigen\n",
    "new_collow = 'LOW'\n",
    "data[new_collow]=data['D_LOW']\n",
    "data.drop('D_LOW', axis='columns', inplace=True)\n",
    "\n",
    "for color in 'HSC':\n",
    "    # Add the combination condition to the index\n",
    "    data[new_collow]  = data[new_collow] | data['{}_LOW'.format(color)]\n",
    "    data.drop('{}_LOW'.format(color), axis='columns', inplace=True)\n",
    "    \n",
    "data[new_collow] = data[new_collow] | (data['D6'.format(color)] & data['H6'.format(color)] & data['S6'.format(color)])\n",
    "data[new_collow] = data[new_collow] | (data['D6'.format(color)] & data['H6'.format(color)] & data['C6'.format(color)])\n",
    "data[new_collow] = data[new_collow] | (data['H6'.format(color)] & data['C6'.format(color)] & data['S6'.format(color)])\n",
    "\n",
    "for color in 'HSC':\n",
    "    # Add the combination condition to the index\n",
    "    data[new_collow]  = data[new_collow] | (data['D6'.format(color)] & data['D7'.format(color)] & data['{}6'.format(color)])\n",
    "    \n",
    "for color in 'DSC':\n",
    "    # Add the combination condition to the index\n",
    "    data[new_collow]  = data[new_collow] | (data['H6'.format(color)] & data['H7'.format(color)] & data['{}6'.format(color)])\n",
    "    \n",
    "for color in 'HDC':\n",
    "    # Add the combination condition to the index\n",
    "    data[new_collow]  = data[new_collow] | (data['S6'.format(color)] & data['S7'.format(color)] & data['{}6'.format(color)])\n",
    "    \n",
    "for color in 'HDS':\n",
    "    # Add the combination condition to the index\n",
    "    data[new_collow]  = data[new_collow] | (data['C6'.format(color)] & data['C7'.format(color)] & data['{}6'.format(color)])\n",
    "\n",
    "\n",
    "# data.tail(n=50)\n",
    "\n",
    "featuresFrame = data.copy()\n",
    "\n",
    "# Create output array\n",
    "outputNumPy = featuresFrame['trump'].to_numpy()\n",
    "outputNumPycategorical=to_categorical(outputNumPy)\n",
    "\n",
    "# Create input array\n",
    "featuresFrame.drop('trump', axis='columns', inplace=True)\n",
    "\n",
    "for name in cards:\n",
    "    featuresFrame.drop(name, axis='columns', inplace=True)\n",
    "\n",
    "featuresFrame.head()\n",
    "featuresNumPy= featuresFrame.to_numpy()\n",
    "featureColumns= np.size(featuresNumPy,1)\n",
    "categoryColumns= np.size(outputNumPycategorical,1)\n",
    "print('featureColumns ' + str(featureColumns) )\n",
    "print('categoryColumns ' + str(categoryColumns))\n",
    "print('featureShape ' + str(featuresNumPy.shape) )\n",
    "print('categoryShape ' + str(outputNumPycategorical.shape))\n",
    "# print('outputMatrix ' + str(outputNumPy))\n",
    "print('categoryMatrix ' + str(outputNumPycategorical))\n",
    "# Reduce feature columns to the size of the category columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 1.9218 - accuracy: 0.3719 - val_loss: 1.8927 - val_accuracy: 0.4316\n",
      "Epoch 2/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.8732 - accuracy: 0.4153 - val_loss: 1.8505 - val_accuracy: 0.3916\n",
      "Epoch 3/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.8371 - accuracy: 0.3880 - val_loss: 1.8189 - val_accuracy: 0.3904\n",
      "Epoch 4/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.8100 - accuracy: 0.3867 - val_loss: 1.7949 - val_accuracy: 0.3880\n",
      "Epoch 5/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.7895 - accuracy: 0.3851 - val_loss: 1.7764 - val_accuracy: 0.3880\n",
      "Epoch 6/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.7733 - accuracy: 0.3849 - val_loss: 1.7610 - val_accuracy: 0.3880\n",
      "Epoch 7/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.7599 - accuracy: 0.3851 - val_loss: 1.7481 - val_accuracy: 0.3880\n",
      "Epoch 8/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.7485 - accuracy: 0.3859 - val_loss: 1.7369 - val_accuracy: 0.3884\n",
      "Epoch 9/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.7387 - accuracy: 0.3968 - val_loss: 1.7270 - val_accuracy: 0.4300\n",
      "Epoch 10/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.7298 - accuracy: 0.4208 - val_loss: 1.7175 - val_accuracy: 0.4300\n",
      "Epoch 11/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.7210 - accuracy: 0.4208 - val_loss: 1.7081 - val_accuracy: 0.4300\n",
      "Epoch 12/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.7126 - accuracy: 0.4208 - val_loss: 1.6994 - val_accuracy: 0.4316\n",
      "Epoch 13/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.7048 - accuracy: 0.4219 - val_loss: 1.6914 - val_accuracy: 0.4316\n",
      "Epoch 14/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6976 - accuracy: 0.4224 - val_loss: 1.6842 - val_accuracy: 0.4316\n",
      "Epoch 15/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6911 - accuracy: 0.4224 - val_loss: 1.6776 - val_accuracy: 0.4316\n",
      "Epoch 16/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6850 - accuracy: 0.4224 - val_loss: 1.6713 - val_accuracy: 0.4316\n",
      "Epoch 17/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6789 - accuracy: 0.4224 - val_loss: 1.6652 - val_accuracy: 0.4316\n",
      "Epoch 18/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6733 - accuracy: 0.4227 - val_loss: 1.6596 - val_accuracy: 0.4316\n",
      "Epoch 19/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6681 - accuracy: 0.4227 - val_loss: 1.6549 - val_accuracy: 0.4316\n",
      "Epoch 20/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6636 - accuracy: 0.4227 - val_loss: 1.6505 - val_accuracy: 0.4316\n",
      "Epoch 21/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6594 - accuracy: 0.4227 - val_loss: 1.6465 - val_accuracy: 0.4316\n",
      "Epoch 22/300\n",
      "75/75 [==============================] - 0s 1000us/step - loss: 1.6554 - accuracy: 0.4227 - val_loss: 1.6427 - val_accuracy: 0.4316\n",
      "Epoch 23/300\n",
      "75/75 [==============================] - 0s 987us/step - loss: 1.6516 - accuracy: 0.4236 - val_loss: 1.6391 - val_accuracy: 0.4316\n",
      "Epoch 24/300\n",
      "75/75 [==============================] - 0s 1000us/step - loss: 1.6480 - accuracy: 0.4281 - val_loss: 1.6356 - val_accuracy: 0.4352\n",
      "Epoch 25/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6444 - accuracy: 0.4284 - val_loss: 1.6323 - val_accuracy: 0.4352\n",
      "Epoch 26/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6411 - accuracy: 0.4284 - val_loss: 1.6290 - val_accuracy: 0.4352\n",
      "Epoch 27/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6379 - accuracy: 0.4284 - val_loss: 1.6260 - val_accuracy: 0.4352\n",
      "Epoch 28/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6348 - accuracy: 0.4284 - val_loss: 1.6230 - val_accuracy: 0.4352\n",
      "Epoch 29/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6318 - accuracy: 0.4284 - val_loss: 1.6202 - val_accuracy: 0.4352\n",
      "Epoch 30/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6289 - accuracy: 0.4284 - val_loss: 1.6174 - val_accuracy: 0.4352\n",
      "Epoch 31/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6260 - accuracy: 0.4285 - val_loss: 1.6149 - val_accuracy: 0.4360\n",
      "Epoch 32/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6234 - accuracy: 0.4293 - val_loss: 1.6124 - val_accuracy: 0.4384\n",
      "Epoch 33/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6209 - accuracy: 0.4299 - val_loss: 1.6100 - val_accuracy: 0.4384\n",
      "Epoch 34/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6184 - accuracy: 0.4299 - val_loss: 1.6075 - val_accuracy: 0.4384\n",
      "Epoch 35/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6153 - accuracy: 0.4299 - val_loss: 1.6038 - val_accuracy: 0.4384\n",
      "Epoch 36/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6114 - accuracy: 0.4305 - val_loss: 1.6000 - val_accuracy: 0.4360\n",
      "Epoch 37/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.6074 - accuracy: 0.4319 - val_loss: 1.5959 - val_accuracy: 0.4360\n",
      "Epoch 38/300\n",
      "75/75 [==============================] - 0s 946us/step - loss: 1.6036 - accuracy: 0.4415 - val_loss: 1.5920 - val_accuracy: 0.4740\n",
      "Epoch 39/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5999 - accuracy: 0.4709 - val_loss: 1.5885 - val_accuracy: 0.4808\n",
      "Epoch 40/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5962 - accuracy: 0.4725 - val_loss: 1.5847 - val_accuracy: 0.4836\n",
      "Epoch 41/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5924 - accuracy: 0.4736 - val_loss: 1.5810 - val_accuracy: 0.4836\n",
      "Epoch 42/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5886 - accuracy: 0.4739 - val_loss: 1.5773 - val_accuracy: 0.4836\n",
      "Epoch 43/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5850 - accuracy: 0.4740 - val_loss: 1.5740 - val_accuracy: 0.4860\n",
      "Epoch 44/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5815 - accuracy: 0.4744 - val_loss: 1.5706 - val_accuracy: 0.4860\n",
      "Epoch 45/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5782 - accuracy: 0.4744 - val_loss: 1.5673 - val_accuracy: 0.4860\n",
      "Epoch 46/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5747 - accuracy: 0.4744 - val_loss: 1.5639 - val_accuracy: 0.4860\n",
      "Epoch 47/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5713 - accuracy: 0.4744 - val_loss: 1.5604 - val_accuracy: 0.4860\n",
      "Epoch 48/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5678 - accuracy: 0.4744 - val_loss: 1.5570 - val_accuracy: 0.4860\n",
      "Epoch 49/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5643 - accuracy: 0.4744 - val_loss: 1.5530 - val_accuracy: 0.4860\n",
      "Epoch 50/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5606 - accuracy: 0.4744 - val_loss: 1.5491 - val_accuracy: 0.4860\n",
      "Epoch 51/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5569 - accuracy: 0.4751 - val_loss: 1.5452 - val_accuracy: 0.4884\n",
      "Epoch 52/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5531 - accuracy: 0.4753 - val_loss: 1.5411 - val_accuracy: 0.4884\n",
      "Epoch 53/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5490 - accuracy: 0.4760 - val_loss: 1.5364 - val_accuracy: 0.4884\n",
      "Epoch 54/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5444 - accuracy: 0.4760 - val_loss: 1.5311 - val_accuracy: 0.4884\n",
      "Epoch 55/300\n",
      "75/75 [==============================] - 0s 959us/step - loss: 1.5398 - accuracy: 0.4760 - val_loss: 1.5263 - val_accuracy: 0.4884\n",
      "Epoch 56/300\n",
      "75/75 [==============================] - 0s 912us/step - loss: 1.5353 - accuracy: 0.4760 - val_loss: 1.5214 - val_accuracy: 0.4884\n",
      "Epoch 57/300\n",
      "75/75 [==============================] - 0s 978us/step - loss: 1.5309 - accuracy: 0.4760 - val_loss: 1.5168 - val_accuracy: 0.4892\n",
      "Epoch 58/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 915us/step - loss: 1.5271 - accuracy: 0.4765 - val_loss: 1.5127 - val_accuracy: 0.4892\n",
      "Epoch 59/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5234 - accuracy: 0.4764 - val_loss: 1.5088 - val_accuracy: 0.4892\n",
      "Epoch 60/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5201 - accuracy: 0.4765 - val_loss: 1.5053 - val_accuracy: 0.4892\n",
      "Epoch 61/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5168 - accuracy: 0.4765 - val_loss: 1.5016 - val_accuracy: 0.4892\n",
      "Epoch 62/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5135 - accuracy: 0.4765 - val_loss: 1.4983 - val_accuracy: 0.4892\n",
      "Epoch 63/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5104 - accuracy: 0.4765 - val_loss: 1.4948 - val_accuracy: 0.4892\n",
      "Epoch 64/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5075 - accuracy: 0.4765 - val_loss: 1.4917 - val_accuracy: 0.4892\n",
      "Epoch 65/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5047 - accuracy: 0.4765 - val_loss: 1.4890 - val_accuracy: 0.4892\n",
      "Epoch 66/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.5021 - accuracy: 0.4764 - val_loss: 1.4861 - val_accuracy: 0.4892\n",
      "Epoch 67/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4995 - accuracy: 0.4764 - val_loss: 1.4835 - val_accuracy: 0.4892\n",
      "Epoch 68/300\n",
      "75/75 [==============================] - 0s 973us/step - loss: 1.4972 - accuracy: 0.4764 - val_loss: 1.4810 - val_accuracy: 0.4892\n",
      "Epoch 69/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4949 - accuracy: 0.4764 - val_loss: 1.4785 - val_accuracy: 0.4892\n",
      "Epoch 70/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4927 - accuracy: 0.4764 - val_loss: 1.4766 - val_accuracy: 0.4892\n",
      "Epoch 71/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4907 - accuracy: 0.4764 - val_loss: 1.4746 - val_accuracy: 0.4892\n",
      "Epoch 72/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4889 - accuracy: 0.4764 - val_loss: 1.4723 - val_accuracy: 0.4892\n",
      "Epoch 73/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4872 - accuracy: 0.4764 - val_loss: 1.4705 - val_accuracy: 0.4892\n",
      "Epoch 74/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4855 - accuracy: 0.4763 - val_loss: 1.4687 - val_accuracy: 0.4892\n",
      "Epoch 75/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4840 - accuracy: 0.4771 - val_loss: 1.4673 - val_accuracy: 0.5020\n",
      "Epoch 76/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4825 - accuracy: 0.4812 - val_loss: 1.4653 - val_accuracy: 0.4892\n",
      "Epoch 77/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4812 - accuracy: 0.4807 - val_loss: 1.4639 - val_accuracy: 0.4892\n",
      "Epoch 78/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4799 - accuracy: 0.4805 - val_loss: 1.4625 - val_accuracy: 0.4892\n",
      "Epoch 79/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4788 - accuracy: 0.4840 - val_loss: 1.4611 - val_accuracy: 0.4892\n",
      "Epoch 80/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4776 - accuracy: 0.4869 - val_loss: 1.4598 - val_accuracy: 0.4892\n",
      "Epoch 81/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4766 - accuracy: 0.4847 - val_loss: 1.4587 - val_accuracy: 0.5020\n",
      "Epoch 82/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4756 - accuracy: 0.4891 - val_loss: 1.4577 - val_accuracy: 0.5028\n",
      "Epoch 83/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4747 - accuracy: 0.4889 - val_loss: 1.4565 - val_accuracy: 0.5028\n",
      "Epoch 84/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4736 - accuracy: 0.4901 - val_loss: 1.4556 - val_accuracy: 0.4888\n",
      "Epoch 85/300\n",
      "75/75 [==============================] - 0s 943us/step - loss: 1.4728 - accuracy: 0.4907 - val_loss: 1.4549 - val_accuracy: 0.5028\n",
      "Epoch 86/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4720 - accuracy: 0.4919 - val_loss: 1.4540 - val_accuracy: 0.5028\n",
      "Epoch 87/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4711 - accuracy: 0.4920 - val_loss: 1.4533 - val_accuracy: 0.5028\n",
      "Epoch 88/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4703 - accuracy: 0.4928 - val_loss: 1.4523 - val_accuracy: 0.5016\n",
      "Epoch 89/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4696 - accuracy: 0.4929 - val_loss: 1.4513 - val_accuracy: 0.5028\n",
      "Epoch 90/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4689 - accuracy: 0.4932 - val_loss: 1.4508 - val_accuracy: 0.5028\n",
      "Epoch 91/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4681 - accuracy: 0.4921 - val_loss: 1.4504 - val_accuracy: 0.5028\n",
      "Epoch 92/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4673 - accuracy: 0.4931 - val_loss: 1.4492 - val_accuracy: 0.5016\n",
      "Epoch 93/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4668 - accuracy: 0.4921 - val_loss: 1.4484 - val_accuracy: 0.5028\n",
      "Epoch 94/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4662 - accuracy: 0.4928 - val_loss: 1.4474 - val_accuracy: 0.5028\n",
      "Epoch 95/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4657 - accuracy: 0.4933 - val_loss: 1.4466 - val_accuracy: 0.5016\n",
      "Epoch 96/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4650 - accuracy: 0.4933 - val_loss: 1.4457 - val_accuracy: 0.5016\n",
      "Epoch 97/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4644 - accuracy: 0.4931 - val_loss: 1.4455 - val_accuracy: 0.5024\n",
      "Epoch 98/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4636 - accuracy: 0.4939 - val_loss: 1.4446 - val_accuracy: 0.5016\n",
      "Epoch 99/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4631 - accuracy: 0.4929 - val_loss: 1.4441 - val_accuracy: 0.5032\n",
      "Epoch 100/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4624 - accuracy: 0.4925 - val_loss: 1.4434 - val_accuracy: 0.5044\n",
      "Epoch 101/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4620 - accuracy: 0.4928 - val_loss: 1.4425 - val_accuracy: 0.5032\n",
      "Epoch 102/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4613 - accuracy: 0.4928 - val_loss: 1.4425 - val_accuracy: 0.5032\n",
      "Epoch 103/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4606 - accuracy: 0.4943 - val_loss: 1.4416 - val_accuracy: 0.5060\n",
      "Epoch 104/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4599 - accuracy: 0.4932 - val_loss: 1.4409 - val_accuracy: 0.5060\n",
      "Epoch 105/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4594 - accuracy: 0.4936 - val_loss: 1.4407 - val_accuracy: 0.5072\n",
      "Epoch 106/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4589 - accuracy: 0.4995 - val_loss: 1.4392 - val_accuracy: 0.5060\n",
      "Epoch 107/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4582 - accuracy: 0.4935 - val_loss: 1.4388 - val_accuracy: 0.5060\n",
      "Epoch 108/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4574 - accuracy: 0.4976 - val_loss: 1.4383 - val_accuracy: 0.5060\n",
      "Epoch 109/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4571 - accuracy: 0.5005 - val_loss: 1.4372 - val_accuracy: 0.5196\n",
      "Epoch 110/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4563 - accuracy: 0.5069 - val_loss: 1.4362 - val_accuracy: 0.5196\n",
      "Epoch 111/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4555 - accuracy: 0.5052 - val_loss: 1.4359 - val_accuracy: 0.5192\n",
      "Epoch 112/300\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.4548 - accuracy: 0.5073 - val_loss: 1.4350 - val_accuracy: 0.5196\n",
      "Epoch 113/300\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.4542 - accuracy: 0.5071 - val_loss: 1.4341 - val_accuracy: 0.5192\n",
      "Epoch 114/300\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.4535 - accuracy: 0.5072 - val_loss: 1.4332 - val_accuracy: 0.5192\n",
      "Epoch 115/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4525 - accuracy: 0.5073 - val_loss: 1.4324 - val_accuracy: 0.5196\n",
      "Epoch 116/300\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.4520 - accuracy: 0.5073 - val_loss: 1.4312 - val_accuracy: 0.5192\n",
      "Epoch 117/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4512 - accuracy: 0.5075 - val_loss: 1.4304 - val_accuracy: 0.5192\n",
      "Epoch 118/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4503 - accuracy: 0.5075 - val_loss: 1.4298 - val_accuracy: 0.5192\n",
      "Epoch 119/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4495 - accuracy: 0.5075 - val_loss: 1.4287 - val_accuracy: 0.5192\n",
      "Epoch 120/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4488 - accuracy: 0.5083 - val_loss: 1.4277 - val_accuracy: 0.5200\n",
      "Epoch 121/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4481 - accuracy: 0.5083 - val_loss: 1.4270 - val_accuracy: 0.5200\n",
      "Epoch 122/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4470 - accuracy: 0.5083 - val_loss: 1.4260 - val_accuracy: 0.5200\n",
      "Epoch 123/300\n",
      "75/75 [==============================] - 0s 990us/step - loss: 1.4461 - accuracy: 0.5085 - val_loss: 1.4249 - val_accuracy: 0.5200\n",
      "Epoch 124/300\n",
      "75/75 [==============================] - 0s 983us/step - loss: 1.4450 - accuracy: 0.5087 - val_loss: 1.4241 - val_accuracy: 0.5200\n",
      "Epoch 125/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4443 - accuracy: 0.5085 - val_loss: 1.4236 - val_accuracy: 0.5204\n",
      "Epoch 126/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4431 - accuracy: 0.5089 - val_loss: 1.4221 - val_accuracy: 0.5204\n",
      "Epoch 127/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4420 - accuracy: 0.5089 - val_loss: 1.4209 - val_accuracy: 0.5204\n",
      "Epoch 128/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4407 - accuracy: 0.5089 - val_loss: 1.4191 - val_accuracy: 0.5216\n",
      "Epoch 129/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4393 - accuracy: 0.5093 - val_loss: 1.4172 - val_accuracy: 0.5204\n",
      "Epoch 130/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4378 - accuracy: 0.5089 - val_loss: 1.4157 - val_accuracy: 0.5204\n",
      "Epoch 131/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4365 - accuracy: 0.5089 - val_loss: 1.4144 - val_accuracy: 0.5216\n",
      "Epoch 132/300\n",
      "75/75 [==============================] - 0s 959us/step - loss: 1.4347 - accuracy: 0.5091 - val_loss: 1.4124 - val_accuracy: 0.5204\n",
      "Epoch 133/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4333 - accuracy: 0.5092 - val_loss: 1.4110 - val_accuracy: 0.5204\n",
      "Epoch 134/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4316 - accuracy: 0.5091 - val_loss: 1.4094 - val_accuracy: 0.5204\n",
      "Epoch 135/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4300 - accuracy: 0.5091 - val_loss: 1.4076 - val_accuracy: 0.5216\n",
      "Epoch 136/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4282 - accuracy: 0.5092 - val_loss: 1.4058 - val_accuracy: 0.5216\n",
      "Epoch 137/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4265 - accuracy: 0.5093 - val_loss: 1.4040 - val_accuracy: 0.5216\n",
      "Epoch 138/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4250 - accuracy: 0.5093 - val_loss: 1.4021 - val_accuracy: 0.5204\n",
      "Epoch 139/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4231 - accuracy: 0.5100 - val_loss: 1.3988 - val_accuracy: 0.5204\n",
      "Epoch 140/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4212 - accuracy: 0.5095 - val_loss: 1.3979 - val_accuracy: 0.5216\n",
      "Epoch 141/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4195 - accuracy: 0.5099 - val_loss: 1.3959 - val_accuracy: 0.5216\n",
      "Epoch 142/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4179 - accuracy: 0.5104 - val_loss: 1.3940 - val_accuracy: 0.5216\n",
      "Epoch 143/300\n",
      "75/75 [==============================] - 0s 998us/step - loss: 1.4159 - accuracy: 0.5112 - val_loss: 1.3931 - val_accuracy: 0.5232\n",
      "Epoch 144/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4143 - accuracy: 0.5120 - val_loss: 1.3907 - val_accuracy: 0.5244\n",
      "Epoch 145/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4126 - accuracy: 0.5125 - val_loss: 1.3890 - val_accuracy: 0.5244\n",
      "Epoch 146/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4110 - accuracy: 0.5124 - val_loss: 1.3868 - val_accuracy: 0.5236\n",
      "Epoch 147/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4095 - accuracy: 0.5149 - val_loss: 1.3858 - val_accuracy: 0.5292\n",
      "Epoch 148/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4078 - accuracy: 0.5161 - val_loss: 1.3847 - val_accuracy: 0.5292\n",
      "Epoch 149/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4062 - accuracy: 0.5195 - val_loss: 1.3829 - val_accuracy: 0.5368\n",
      "Epoch 150/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4051 - accuracy: 0.5225 - val_loss: 1.3807 - val_accuracy: 0.5368\n",
      "Epoch 151/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4034 - accuracy: 0.5235 - val_loss: 1.3796 - val_accuracy: 0.5368\n",
      "Epoch 152/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4022 - accuracy: 0.5240 - val_loss: 1.3784 - val_accuracy: 0.5368\n",
      "Epoch 153/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.4004 - accuracy: 0.5240 - val_loss: 1.3783 - val_accuracy: 0.5368\n",
      "Epoch 154/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3993 - accuracy: 0.5241 - val_loss: 1.3754 - val_accuracy: 0.5368\n",
      "Epoch 155/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3977 - accuracy: 0.5243 - val_loss: 1.3737 - val_accuracy: 0.5368\n",
      "Epoch 156/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3966 - accuracy: 0.5240 - val_loss: 1.3731 - val_accuracy: 0.5368\n",
      "Epoch 157/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3953 - accuracy: 0.5249 - val_loss: 1.3725 - val_accuracy: 0.5380\n",
      "Epoch 158/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3941 - accuracy: 0.5259 - val_loss: 1.3706 - val_accuracy: 0.5368\n",
      "Epoch 159/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3930 - accuracy: 0.5229 - val_loss: 1.3703 - val_accuracy: 0.5368\n",
      "Epoch 160/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3918 - accuracy: 0.5249 - val_loss: 1.3685 - val_accuracy: 0.5368\n",
      "Epoch 161/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3908 - accuracy: 0.5259 - val_loss: 1.3687 - val_accuracy: 0.5368\n",
      "Epoch 162/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3898 - accuracy: 0.5236 - val_loss: 1.3673 - val_accuracy: 0.5380\n",
      "Epoch 163/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3889 - accuracy: 0.5248 - val_loss: 1.3674 - val_accuracy: 0.5368\n",
      "Epoch 164/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3882 - accuracy: 0.5249 - val_loss: 1.3651 - val_accuracy: 0.5368\n",
      "Epoch 165/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3872 - accuracy: 0.5241 - val_loss: 1.3651 - val_accuracy: 0.5396\n",
      "Epoch 166/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3862 - accuracy: 0.5261 - val_loss: 1.3642 - val_accuracy: 0.5384\n",
      "Epoch 167/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3856 - accuracy: 0.5239 - val_loss: 1.3624 - val_accuracy: 0.5384\n",
      "Epoch 168/300\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.3843 - accuracy: 0.5279 - val_loss: 1.3624 - val_accuracy: 0.5368\n",
      "Epoch 169/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3837 - accuracy: 0.5268 - val_loss: 1.3619 - val_accuracy: 0.5392\n",
      "Epoch 170/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3826 - accuracy: 0.5269 - val_loss: 1.3623 - val_accuracy: 0.5388\n",
      "Epoch 171/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3824 - accuracy: 0.5276 - val_loss: 1.3609 - val_accuracy: 0.5384\n",
      "Epoch 172/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3817 - accuracy: 0.5256 - val_loss: 1.3593 - val_accuracy: 0.5376\n",
      "Epoch 173/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3811 - accuracy: 0.5259 - val_loss: 1.3589 - val_accuracy: 0.5364\n",
      "Epoch 174/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3800 - accuracy: 0.5267 - val_loss: 1.3594 - val_accuracy: 0.5384\n",
      "Epoch 175/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3798 - accuracy: 0.5261 - val_loss: 1.3590 - val_accuracy: 0.5372\n",
      "Epoch 176/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3793 - accuracy: 0.5255 - val_loss: 1.3565 - val_accuracy: 0.5376\n",
      "Epoch 177/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3786 - accuracy: 0.5264 - val_loss: 1.3565 - val_accuracy: 0.5364\n",
      "Epoch 178/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3778 - accuracy: 0.5271 - val_loss: 1.3585 - val_accuracy: 0.5384\n",
      "Epoch 179/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3777 - accuracy: 0.5277 - val_loss: 1.3559 - val_accuracy: 0.5364\n",
      "Epoch 180/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3770 - accuracy: 0.5260 - val_loss: 1.3549 - val_accuracy: 0.5364\n",
      "Epoch 181/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3766 - accuracy: 0.5245 - val_loss: 1.3537 - val_accuracy: 0.5364\n",
      "Epoch 182/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3757 - accuracy: 0.5276 - val_loss: 1.3538 - val_accuracy: 0.5364\n",
      "Epoch 183/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3753 - accuracy: 0.5271 - val_loss: 1.3543 - val_accuracy: 0.5372\n",
      "Epoch 184/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3747 - accuracy: 0.5256 - val_loss: 1.3521 - val_accuracy: 0.5364\n",
      "Epoch 185/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3742 - accuracy: 0.5280 - val_loss: 1.3514 - val_accuracy: 0.5376\n",
      "Epoch 186/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3740 - accuracy: 0.5267 - val_loss: 1.3522 - val_accuracy: 0.5376\n",
      "Epoch 187/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3734 - accuracy: 0.5261 - val_loss: 1.3505 - val_accuracy: 0.5376\n",
      "Epoch 188/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3728 - accuracy: 0.5263 - val_loss: 1.3501 - val_accuracy: 0.5376\n",
      "Epoch 189/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3722 - accuracy: 0.5255 - val_loss: 1.3508 - val_accuracy: 0.5364\n",
      "Epoch 190/300\n",
      "75/75 [==============================] - 0s 980us/step - loss: 1.3720 - accuracy: 0.5244 - val_loss: 1.3517 - val_accuracy: 0.5376\n",
      "Epoch 191/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3716 - accuracy: 0.5252 - val_loss: 1.3492 - val_accuracy: 0.5364\n",
      "Epoch 192/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3711 - accuracy: 0.5256 - val_loss: 1.3494 - val_accuracy: 0.5364\n",
      "Epoch 193/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3705 - accuracy: 0.5277 - val_loss: 1.3502 - val_accuracy: 0.5376\n",
      "Epoch 194/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3706 - accuracy: 0.5277 - val_loss: 1.3493 - val_accuracy: 0.5364\n",
      "Epoch 195/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3701 - accuracy: 0.5260 - val_loss: 1.3482 - val_accuracy: 0.5364\n",
      "Epoch 196/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3699 - accuracy: 0.5269 - val_loss: 1.3471 - val_accuracy: 0.5364\n",
      "Epoch 197/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3692 - accuracy: 0.5265 - val_loss: 1.3473 - val_accuracy: 0.5364\n",
      "Epoch 198/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3688 - accuracy: 0.5252 - val_loss: 1.3476 - val_accuracy: 0.5376\n",
      "Epoch 199/300\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 1.3685 - accuracy: 0.5281 - val_loss: 1.3466 - val_accuracy: 0.5376\n",
      "Epoch 200/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3683 - accuracy: 0.5285 - val_loss: 1.3473 - val_accuracy: 0.5380\n",
      "Epoch 201/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3680 - accuracy: 0.5269 - val_loss: 1.3461 - val_accuracy: 0.5368\n",
      "Epoch 202/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3676 - accuracy: 0.5255 - val_loss: 1.3454 - val_accuracy: 0.5368\n",
      "Epoch 203/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3672 - accuracy: 0.5289 - val_loss: 1.3453 - val_accuracy: 0.5340\n",
      "Epoch 204/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3669 - accuracy: 0.5261 - val_loss: 1.3453 - val_accuracy: 0.5368\n",
      "Epoch 205/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3670 - accuracy: 0.5260 - val_loss: 1.3454 - val_accuracy: 0.5368\n",
      "Epoch 206/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3667 - accuracy: 0.5265 - val_loss: 1.3452 - val_accuracy: 0.5368\n",
      "Epoch 207/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3665 - accuracy: 0.5277 - val_loss: 1.3444 - val_accuracy: 0.5368\n",
      "Epoch 208/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3660 - accuracy: 0.5267 - val_loss: 1.3442 - val_accuracy: 0.5368\n",
      "Epoch 209/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3661 - accuracy: 0.5244 - val_loss: 1.3438 - val_accuracy: 0.5368\n",
      "Epoch 210/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3655 - accuracy: 0.5263 - val_loss: 1.3438 - val_accuracy: 0.5368\n",
      "Epoch 211/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3654 - accuracy: 0.5289 - val_loss: 1.3428 - val_accuracy: 0.5368\n",
      "Epoch 212/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3653 - accuracy: 0.5280 - val_loss: 1.3428 - val_accuracy: 0.5368\n",
      "Epoch 213/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3651 - accuracy: 0.5267 - val_loss: 1.3426 - val_accuracy: 0.5360\n",
      "Epoch 214/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3650 - accuracy: 0.5292 - val_loss: 1.3425 - val_accuracy: 0.5448\n",
      "Epoch 215/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3646 - accuracy: 0.5297 - val_loss: 1.3428 - val_accuracy: 0.5360\n",
      "Epoch 216/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3642 - accuracy: 0.5273 - val_loss: 1.3429 - val_accuracy: 0.5368\n",
      "Epoch 217/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3639 - accuracy: 0.5280 - val_loss: 1.3409 - val_accuracy: 0.5444\n",
      "Epoch 218/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3636 - accuracy: 0.5291 - val_loss: 1.3415 - val_accuracy: 0.5360\n",
      "Epoch 219/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3633 - accuracy: 0.5296 - val_loss: 1.3409 - val_accuracy: 0.5408\n",
      "Epoch 220/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3633 - accuracy: 0.5337 - val_loss: 1.3423 - val_accuracy: 0.5360\n",
      "Epoch 221/300\n",
      "75/75 [==============================] - 0s 980us/step - loss: 1.3630 - accuracy: 0.5309 - val_loss: 1.3401 - val_accuracy: 0.5428\n",
      "Epoch 222/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3624 - accuracy: 0.5311 - val_loss: 1.3414 - val_accuracy: 0.5360\n",
      "Epoch 223/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3629 - accuracy: 0.5301 - val_loss: 1.3400 - val_accuracy: 0.5340\n",
      "Epoch 224/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3622 - accuracy: 0.5268 - val_loss: 1.3410 - val_accuracy: 0.5340\n",
      "Epoch 225/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3620 - accuracy: 0.5293 - val_loss: 1.3399 - val_accuracy: 0.5428\n",
      "Epoch 226/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3620 - accuracy: 0.5300 - val_loss: 1.3391 - val_accuracy: 0.5448\n",
      "Epoch 227/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3614 - accuracy: 0.5321 - val_loss: 1.3395 - val_accuracy: 0.5428\n",
      "Epoch 228/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3614 - accuracy: 0.5297 - val_loss: 1.3399 - val_accuracy: 0.5448\n",
      "Epoch 229/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3610 - accuracy: 0.5324 - val_loss: 1.3389 - val_accuracy: 0.5408\n",
      "Epoch 230/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3611 - accuracy: 0.5329 - val_loss: 1.3395 - val_accuracy: 0.5340\n",
      "Epoch 231/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3605 - accuracy: 0.5335 - val_loss: 1.3400 - val_accuracy: 0.5448\n",
      "Epoch 232/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3604 - accuracy: 0.5333 - val_loss: 1.3380 - val_accuracy: 0.5428\n",
      "Epoch 233/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3603 - accuracy: 0.5316 - val_loss: 1.3376 - val_accuracy: 0.5448\n",
      "Epoch 234/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3601 - accuracy: 0.5348 - val_loss: 1.3371 - val_accuracy: 0.5428\n",
      "Epoch 235/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3597 - accuracy: 0.5349 - val_loss: 1.3381 - val_accuracy: 0.5456\n",
      "Epoch 236/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3594 - accuracy: 0.5323 - val_loss: 1.3373 - val_accuracy: 0.5456\n",
      "Epoch 237/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3595 - accuracy: 0.5353 - val_loss: 1.3371 - val_accuracy: 0.5448\n",
      "Epoch 238/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3587 - accuracy: 0.5344 - val_loss: 1.3360 - val_accuracy: 0.5456\n",
      "Epoch 239/300\n",
      "75/75 [==============================] - 0s 998us/step - loss: 1.3589 - accuracy: 0.5353 - val_loss: 1.3364 - val_accuracy: 0.5400\n",
      "Epoch 240/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3587 - accuracy: 0.5333 - val_loss: 1.3382 - val_accuracy: 0.5448\n",
      "Epoch 241/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3583 - accuracy: 0.5344 - val_loss: 1.3373 - val_accuracy: 0.5456\n",
      "Epoch 242/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3583 - accuracy: 0.5341 - val_loss: 1.3357 - val_accuracy: 0.5448\n",
      "Epoch 243/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3579 - accuracy: 0.5341 - val_loss: 1.3365 - val_accuracy: 0.5448\n",
      "Epoch 244/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3575 - accuracy: 0.5335 - val_loss: 1.3355 - val_accuracy: 0.5420\n",
      "Epoch 245/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3571 - accuracy: 0.5336 - val_loss: 1.3394 - val_accuracy: 0.5484\n",
      "Epoch 246/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3577 - accuracy: 0.5333 - val_loss: 1.3365 - val_accuracy: 0.5436\n",
      "Epoch 247/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3572 - accuracy: 0.5353 - val_loss: 1.3359 - val_accuracy: 0.5448\n",
      "Epoch 248/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3567 - accuracy: 0.5348 - val_loss: 1.3342 - val_accuracy: 0.5448\n",
      "Epoch 249/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3569 - accuracy: 0.5351 - val_loss: 1.3352 - val_accuracy: 0.5456\n",
      "Epoch 250/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3566 - accuracy: 0.5335 - val_loss: 1.3354 - val_accuracy: 0.5444\n",
      "Epoch 251/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3567 - accuracy: 0.5357 - val_loss: 1.3348 - val_accuracy: 0.5444\n",
      "Epoch 252/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3559 - accuracy: 0.5349 - val_loss: 1.3358 - val_accuracy: 0.5448\n",
      "Epoch 253/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3562 - accuracy: 0.5348 - val_loss: 1.3340 - val_accuracy: 0.5448\n",
      "Epoch 254/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3560 - accuracy: 0.5332 - val_loss: 1.3339 - val_accuracy: 0.5448\n",
      "Epoch 255/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3562 - accuracy: 0.5331 - val_loss: 1.3343 - val_accuracy: 0.5464\n",
      "Epoch 256/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3552 - accuracy: 0.5343 - val_loss: 1.3339 - val_accuracy: 0.5456\n",
      "Epoch 257/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3552 - accuracy: 0.5337 - val_loss: 1.3331 - val_accuracy: 0.5452\n",
      "Epoch 258/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3555 - accuracy: 0.5331 - val_loss: 1.3334 - val_accuracy: 0.5420\n",
      "Epoch 259/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3551 - accuracy: 0.5340 - val_loss: 1.3332 - val_accuracy: 0.5428\n",
      "Epoch 260/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3551 - accuracy: 0.5344 - val_loss: 1.3331 - val_accuracy: 0.5468\n",
      "Epoch 261/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3547 - accuracy: 0.5365 - val_loss: 1.3352 - val_accuracy: 0.5472\n",
      "Epoch 262/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3546 - accuracy: 0.5345 - val_loss: 1.3337 - val_accuracy: 0.5444\n",
      "Epoch 263/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3544 - accuracy: 0.5355 - val_loss: 1.3333 - val_accuracy: 0.5452\n",
      "Epoch 264/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3542 - accuracy: 0.5335 - val_loss: 1.3341 - val_accuracy: 0.5472\n",
      "Epoch 265/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3539 - accuracy: 0.5361 - val_loss: 1.3339 - val_accuracy: 0.5452\n",
      "Epoch 266/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3538 - accuracy: 0.5339 - val_loss: 1.3326 - val_accuracy: 0.5468\n",
      "Epoch 267/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3540 - accuracy: 0.5313 - val_loss: 1.3328 - val_accuracy: 0.5480\n",
      "Epoch 268/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3528 - accuracy: 0.5321 - val_loss: 1.3346 - val_accuracy: 0.5440\n",
      "Epoch 269/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3539 - accuracy: 0.5348 - val_loss: 1.3319 - val_accuracy: 0.5448\n",
      "Epoch 270/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3530 - accuracy: 0.5341 - val_loss: 1.3352 - val_accuracy: 0.5472\n",
      "Epoch 271/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3531 - accuracy: 0.5317 - val_loss: 1.3319 - val_accuracy: 0.5448\n",
      "Epoch 272/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3531 - accuracy: 0.5328 - val_loss: 1.3313 - val_accuracy: 0.5476\n",
      "Epoch 273/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3527 - accuracy: 0.5352 - val_loss: 1.3318 - val_accuracy: 0.5428\n",
      "Epoch 274/300\n",
      "75/75 [==============================] - 0s 977us/step - loss: 1.3524 - accuracy: 0.5355 - val_loss: 1.3324 - val_accuracy: 0.5480\n",
      "Epoch 275/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3524 - accuracy: 0.5341 - val_loss: 1.3313 - val_accuracy: 0.5472\n",
      "Epoch 276/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3522 - accuracy: 0.5327 - val_loss: 1.3328 - val_accuracy: 0.5472\n",
      "Epoch 277/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3519 - accuracy: 0.5340 - val_loss: 1.3327 - val_accuracy: 0.5480\n",
      "Epoch 278/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3523 - accuracy: 0.5332 - val_loss: 1.3310 - val_accuracy: 0.5480\n",
      "Epoch 279/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3516 - accuracy: 0.5367 - val_loss: 1.3319 - val_accuracy: 0.5456\n",
      "Epoch 280/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3512 - accuracy: 0.5359 - val_loss: 1.3315 - val_accuracy: 0.5484\n",
      "Epoch 281/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3515 - accuracy: 0.5369 - val_loss: 1.3320 - val_accuracy: 0.5440\n",
      "Epoch 282/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3514 - accuracy: 0.5335 - val_loss: 1.3311 - val_accuracy: 0.5448\n",
      "Epoch 283/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3507 - accuracy: 0.5351 - val_loss: 1.3310 - val_accuracy: 0.5472\n",
      "Epoch 284/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3512 - accuracy: 0.5336 - val_loss: 1.3316 - val_accuracy: 0.5472\n",
      "Epoch 285/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3505 - accuracy: 0.5344 - val_loss: 1.3308 - val_accuracy: 0.5472\n",
      "Epoch 286/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3503 - accuracy: 0.5348 - val_loss: 1.3301 - val_accuracy: 0.5472\n",
      "Epoch 287/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3501 - accuracy: 0.5344 - val_loss: 1.3332 - val_accuracy: 0.5472\n",
      "Epoch 288/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3502 - accuracy: 0.5351 - val_loss: 1.3317 - val_accuracy: 0.5444\n",
      "Epoch 289/300\n",
      "75/75 [==============================] - 0s 972us/step - loss: 1.3499 - accuracy: 0.5331 - val_loss: 1.3309 - val_accuracy: 0.5480\n",
      "Epoch 290/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3500 - accuracy: 0.5309 - val_loss: 1.3313 - val_accuracy: 0.5444\n",
      "Epoch 291/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3496 - accuracy: 0.5317 - val_loss: 1.3299 - val_accuracy: 0.5448\n",
      "Epoch 292/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3495 - accuracy: 0.5333 - val_loss: 1.3302 - val_accuracy: 0.5472\n",
      "Epoch 293/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3495 - accuracy: 0.5329 - val_loss: 1.3303 - val_accuracy: 0.5452\n",
      "Epoch 294/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3489 - accuracy: 0.5325 - val_loss: 1.3304 - val_accuracy: 0.5452\n",
      "Epoch 295/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3493 - accuracy: 0.5317 - val_loss: 1.3304 - val_accuracy: 0.5444\n",
      "Epoch 296/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3489 - accuracy: 0.5351 - val_loss: 1.3295 - val_accuracy: 0.5452\n",
      "Epoch 297/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3483 - accuracy: 0.5304 - val_loss: 1.3300 - val_accuracy: 0.5436\n",
      "Epoch 298/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3488 - accuracy: 0.5333 - val_loss: 1.3300 - val_accuracy: 0.5448\n",
      "Epoch 299/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3483 - accuracy: 0.5343 - val_loss: 1.3310 - val_accuracy: 0.5448\n",
      "Epoch 300/300\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 1.3481 - accuracy: 0.5343 - val_loss: 1.3292 - val_accuracy: 0.5444\n"
     ]
    }
   ],
   "source": [
    "modelCard = keras.Sequential()\n",
    "\n",
    "# It seems that the number of features has to be equal to the number of categories.\n",
    "# The input shape is now featureColumns instead of 2\n",
    "modelCard.add(keras.Input(shape=[featureColumns]))\n",
    "modelCard.add(keras.layers.Dense(featureColumns, activation='relu'))\n",
    "modelCard.add(keras.layers.Dense(featureColumns, activation='relu'))\n",
    "modelCard.add(keras.layers.Dense(featureColumns, activation='relu'))\n",
    "modelCard.add(keras.layers.Dense(featureColumns, activation='relu'))\n",
    "modelCard.add(keras.layers.Dense(categoryColumns, activation='softmax'))\n",
    "#Sigmoid cannot be used, since it provides only a classification between 0 and 1\n",
    "#--> softmax\n",
    "# The output has now 7 (4 colours obe, unde and push) classifications instead of 1\n",
    "modelCard.compile(loss= loss_function_used, optimizer='sgd',metrics=['accuracy'])\n",
    "# Model is trained on only 0.75 of the training data. 0.25 will be used for validation after each epoch.\n",
    "# In each epoch a batch with size 100 is extracted from the 0.75 training data for training.\n",
    "history = modelCard.fit(featuresNumPy, outputNumPycategorical, validation_split=0.25, epochs=300, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20014283908>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
